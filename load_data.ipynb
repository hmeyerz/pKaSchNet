{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note this doesnt work when first letter isnt the element name as is true for cofactors metals and oddballl atoms, so just grab it from the pdb next time you edit this.!!!!!####################################\n",
    "import time\n",
    "\n",
    "# get the current time in seconds since the epoch\n",
    "\n",
    "import pdbfixer\n",
    "from openmm.app import PDBFile\n",
    "import sys\n",
    "from egnn_pytorch import EGNN_Network\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "from collections import defaultdict, OrderedDict\n",
    "import dask.dataframe as dd\n",
    "import math\n",
    "anions = {\"HIS\":(\"ND1\", \"ND2\"),\n",
    "        \"ASP\":(\"OD1\",\"OD2\"),\n",
    "        \"LYS\":(\"NZ\"),\n",
    "        \"TYR\":(\"OH\"),\n",
    "        \"GLU\":(\"OE1\", \"OE2\"),\n",
    "        \"CYS\":(\"SG\")}\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "elements = {\n",
    "    \"H\": 1, \"He\": 2, \"Li\": 3, \"Be\": 4, \"B\": 5, \"C\": 6, \"N\": 7, \"O\": 8, \"F\": 9, \"Ne\": 10,\n",
    "    \"Na\": 11, \"Mg\": 12, \"Al\": 13, \"Si\": 14, \"P\": 15, \"S\": 16, \"Cl\": 17, \"Ar\": 18, \"K\": 19,\n",
    "    \"Ca\": 20, \"Sc\": 21, \"Ti\": 22, \"V\": 23, \"Cr\": 24, \"Mn\": 25, \"Fe\": 26, \"Co\": 27, \"Ni\": 28,\n",
    "    \"Cu\": 29, \"Zn\": 30, \"Ga\": 31, \"Ge\": 32, \"As\": 33, \"Se\": 34, \"Br\": 35, \"Kr\": 36, \"Rb\": 37,\n",
    "    \"Sr\": 38, \"Y\": 39, \"Zr\": 40, \"Nb\": 41, \"Mo\": 42, \"Tc\": 43, \"Ru\": 44, \"Rh\": 45, \"Pd\": 46,\n",
    "    \"Ag\": 47, \"Cd\": 48, \"In\": 49, \"Sn\": 50, \"Sb\": 51, \"I\": 53, \"Xe\": 54, \"Cs\": 55, \"Ba\": 56,\n",
    "    \"La\": 57, \"Ce\": 58, \"Pr\": 59, \"Nd\": 60, \"Pm\": 61, \"Sm\": 62, \"Eu\": 63, \"Gd\": 64, \"Tb\": 65,\n",
    "    \"Dy\": 66, \"Ho\": 67, \"Er\": 68, \"Tm\": 69, \"Yb\": 70, \"Lu\": 71, \"Hf\": 72, \"Ta\": 73, \"W\": 74,\n",
    "    \"Re\": 75, \"Os\": 76, \"Ir\": 77, \"Pt\": 78, \"Au\": 79, \"Hg\": 80, \"Tl\": 81, \"Pb\": 82, \"Bi\": 83,\n",
    "    \"Po\": 84, \"At\": 85, \"Rn\": 86, \"Fr\": 87, \"Ra\": 88, \"Ac\": 89, \"Th\": 90, \"Pa\": 91, \"U\": 92,\n",
    "    \"Np\": 93, \"Pu\": 94, \"Am\": 95, \"Cm\": 96, \"Bk\": 97, \"Cf\": 98, \"Es\": 99, \"Fm\": 100, \"Md\": 101,\n",
    "    \"No\": 102, \"Lr\": 103, \"Rf\": 104, \"Db\": 105, \"Sg\": 106, \"Bh\": 107, \"Hs\": 108, \"Mt\": 109,\n",
    "    \"Ds\": 110, \"Rg\": 111, \"Cn\": 112, \"Nh\": 113, \"Fl\": 114, \"Mc\": 115, \"Lv\": 116, \"Ts\": 117,\n",
    "    \"Og\": 118\n",
    "}\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) *\n",
    "                             (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if embed_dim % 2 == 1:\n",
    "            # Handle odd dimensions by filling the remaining column with cos()\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:, :pe[:, 1::2].shape[1]]\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, embed_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor with shape (seq_length, batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class SimpleMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(SimpleMultiheadAttention, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (seq_length, batch_size, embed_dim)\n",
    "        \"\"\"\n",
    "        attn_outputs, attn_weights = self.multihead_attn(x, x, x)\n",
    "        return attn_outputs\n",
    "\n",
    "\n",
    "def EGNN1():\n",
    "    \"\"\"This layer creates an embedding as well as hopefully navigates dimerrors produced by putting molecules with\n",
    "    varying node size.\n",
    "    in\n",
    "    feats = 1 x n_atoms\n",
    "\n",
    "    input can be minimum one atom, this accomdates for rogue solo cofactors and unhydrogonated water.\"\"\"\n",
    "    net = EGNN_Network(\n",
    "        num_positions = 1000, # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "        dim = 1,\n",
    "        depth=1,\n",
    "        num_tokens=118,\n",
    "        num_nearest_neighbors=1,\n",
    "        dropout=0.05)\n",
    "    return net\n",
    "\n",
    "\n",
    "net = EGNN1()\n",
    "\n",
    "#A = PositionalEncoding(1)\n",
    "#mha = SimpleMultiheadAttention(1,1)\n",
    "optimizer2 = torch.optim.Adam(list(net.parameters()), lr=.009, weight_decay=0.02)\n",
    "criterion = nn.HuberLoss()\n",
    "losses=[]\n",
    "preds=[]\n",
    "Ts=[]\n",
    "counter=0\n",
    "#print(net, (time.time() - to)/60)\n",
    "\n",
    "\n",
    "\n",
    "class prepper():\n",
    "    \"\"\"TODO\n",
    "    init: put pdbs with read csv file from polars\"\"\"\n",
    "    def __init__(self):\n",
    "        self.db = \"/Users/jessihoernschemeyer/pKaSchNet/pkas.csv\" #('/content/drive/MyDrive/pkas.csv') #3directory\n",
    "        #self.pdbs = self.read_pdbs_from_pkpdb()\n",
    "\n",
    "    def get_targets(self, calculate=False):\n",
    "        \"\"\"TODO: remove, put get_pdbs in init.\n",
    "        pdbs is the list which i have a data onject for\"\"\"\n",
    "        all_targets=OrderedDict()\n",
    "        df = dd.read_csv(self.db,delimiter=\";\").compute().sort_values([\"idcode\"])\n",
    "        _pdbs = list(list(df[[\"idcode\"]].to_dict().values())[0].values())\n",
    "        _pdbs=list(set(_pdbs))\n",
    "        _pdbs.sort()\n",
    "        self.pdbs=_pdbs[:1499]\n",
    "\n",
    "    def fix_pdb(self,pdb):\n",
    "      \"\"\"puts amber atom names and resis (needed for any parsing), fixes typesetter errors (need), and messes up the numbering\"\"\"\n",
    "      fixer =pdbfixer.PDBFixer(f\"{pdb}.pdb\")\n",
    "      fixer.findNonstandardResidues()\n",
    "      fixer.replaceNonstandardResidues()\n",
    "      PDBFile.writeFile(fixer.topology, fixer.positions, f\"{pdb}f.pdb\")\n",
    "\n",
    "    def load_pdb(self, pdb):\n",
    "        \"\"\"loads everything from pdb name\"\"\" \n",
    "        try:\n",
    "            #!wget https://files.rcsb.org/download/{pdb}.pdb\n",
    "            #self.fix_pdb(pdb)\n",
    "            counter = 0\n",
    "            with open(pdb + \"f.pdb\", \"r\") as f:\n",
    "                resis, counter = OrderedDict(), 0\n",
    "                for line in f:\n",
    "                    L = line.split()\n",
    "                    if line.startswith(\"ATOM\"):\n",
    "                        resname, atomname = L[3], L[2]\n",
    "                        if resname in anions.keys():\n",
    "                            resname = resname + str(L[5]) + L[4]\n",
    "                            if resname in resis:\n",
    "                                resis[resname][f\"{atomname} {counter}\"] = (float(L[6]), float(L[7]), float(L[8]))\n",
    "                                counter += 1\n",
    "                            else:\n",
    "                                resis[resname] = {f\"{atomname} {counter}\": (float(L[6]), float(L[7]), float(L[8]))}\n",
    "                                counter += 1\n",
    "        except:\n",
    "            print(pdb,\"fail\")\n",
    "            \n",
    "\n",
    "        return resis\n",
    "\n",
    "    def get_pdbs_list(self):\n",
    "        data ={}\n",
    "        for pdb in self.pdbs:\n",
    "            data[pdb] = self.load_pdb(pdb)\n",
    "        self.residues = data\n",
    "        return\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"returns dictionary of data with pdbids as keys and residueid indexed positions, atomic node and species information, input features (hydrogen indices), and the number of atoms in each sample (residue).\n",
    "        all_species: {199l: {ASP1A : {CA:1} , ..., {TYR100 : {OXT:1032} } \"\"\"\n",
    "        all_pos, all_species = OrderedDict(),OrderedDict()\n",
    "        counter=0\n",
    "        data = self.residues\n",
    "        for pdb in self.pdbs:\n",
    "            resis = data[pdb]\n",
    "        #####residues##########\n",
    "            species, R =OrderedDict(), OrderedDict()\n",
    "            for resid, vals in resis.items():\n",
    "                if resid[:3] in anions.keys():\n",
    "                    counter, resi_pos, resi_species = 0,OrderedDict(), OrderedDict()\n",
    "                    for atom, pos in vals.items():\n",
    "                        node = atom.split()\n",
    "                        resi_pos[counter] = pos\n",
    "                        resi_species[node[0]] = counter\n",
    "                        counter +=1\n",
    "                    species[resid] = resi_species\n",
    "                    R[resid] = resi_pos\n",
    "                else:\n",
    "                    print(32323)\n",
    "            all_pos[pdb]= R\n",
    "            all_species[pdb] = species\n",
    "        all_rdata = defaultdict(dict)\n",
    "        for title, d in zip([\"Z\", \"R\"], (all_species, all_pos)):\n",
    "            for key, value in d.items():\n",
    "                all_rdata[key][title] = value\n",
    "        self.residues = all_rdata\n",
    "\n",
    "    def run(self):\n",
    "        \"run data prep\"\n",
    "        self.get_targets(calculate=False)\n",
    "        self.get_pdbs_list()\n",
    "        self.get_data()\n",
    "        \n",
    "        return self\n",
    "        \n",
    "def get_Zs(data):\n",
    "    \"\"\".\"\"\"\n",
    "    rdata = data.residues\n",
    "    all_atomic_numbers = OrderedDict()\n",
    "    for pdb in data.pdbs:\n",
    "        zs=OrderedDict()\n",
    "        for res in list(rdata[pdb][\"Z\"].keys()):\n",
    "            try:\n",
    "                zs[res] = [torch.tensor(elements[a[0]]) for a in list(rdata[pdb][\"Z\"][res].keys())]\n",
    "            except:\n",
    "                print(pdb,\"zs\")\n",
    "                continue\n",
    "        all_atomic_numbers[pdb] = zs\n",
    "    return all_atomic_numbers\n",
    "\n",
    "\n",
    "def tar():\n",
    "    all_targets={}\n",
    "    with open(\"/Users/jessihoernschemeyer/pKaSchNet/targets_100k\",\"r\") as f:\n",
    "        rdata={}\n",
    "        for line in f:\n",
    "            h=line.split()\n",
    "            if not h:\n",
    "                all_targets[pdb]=rdata\n",
    "                continue\n",
    "            elif len(h) == 1:\n",
    "                rdata={}\n",
    "                pdb=h[0]\n",
    "                continue\n",
    "            rdata[h[0]] = torch.tensor(float(h[1]))\n",
    "    return all_targets\n",
    "\n",
    "to=time.time()\n",
    "data = prepper().run()\n",
    "seconds = time.time() - to \n",
    "print(seconds/60)\n",
    "\n",
    "\n",
    "#all_targets=tar() \n",
    "print(\"data prep done\", ((time.time() - to)/60))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def EGNN1():\n",
    "    \"\"\"\"\"\"\n",
    "    net = EGNN_Network(\n",
    "        num_positions = 1000, # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "        dim = 1,\n",
    "        depth=1,\n",
    "        num_tokens=118,\n",
    "        num_nearest_neighbors=1,\n",
    "        dropout=0.05)\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "refpk = {\n",
    "    \"ASP\": 3.79,\n",
    "    \"CYS\": 8.67,\n",
    "    \"GLU\": 4.20,\n",
    "    \"HIS\": 6.74,\n",
    "    \"LYS\": 10.46,\n",
    "    \"TYR\": 9.59,\n",
    "}\n",
    "\n",
    "\n",
    "print((time.time() - to )/60)\n",
    "  \n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "pdbs,zs=data.pdbs,get_Zs(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize output dictionaries\n",
    "hoodos = {}\n",
    "all_hoods = {}\n",
    "all_nodes,all_conn = {},{}\n",
    "\n",
    "cutoff_radius = 2\n",
    "\n",
    "cutoff_radius=4\n",
    "all_hoods={}\n",
    "for pdb in pdbs:\n",
    "    if not zs[pdb]:\n",
    "        print(pdb)\n",
    "        continue\n",
    "\n",
    "    residues_data = data.residues[pdb]\n",
    "    residue_keys = list(zs[pdb].keys())\n",
    "\n",
    "    # --- Build lists of tensors and record slice ranges ---\n",
    "    # Each residue may have a different number of nodes.\n",
    "    # We'll build two lists: one for coordinates (\"R\") and one for zs features.\n",
    "    res_R_list = {}  # list of tensors of shape [n_i, 3]\n",
    "    res_z_list = {} # list of tensors for zs features (shape may vary)\n",
    "    # This dict maps each residue key to the slice (start, end) in the concatenated tensors.\n",
    "    #residue_idx_ranges = {}\n",
    "    global_start = 0\n",
    "    keys={}\n",
    "    for i,key in enumerate(residue_keys):\n",
    "        # Convert coordinates for this residue to a tensor.\n",
    "        # Note: using tuple(...) ensures a 2D tensor if the values are already arranged properly.\n",
    "        #R_tensor = torch.tensor(tuple(residues_data[\"R\"][key].values()))\n",
    "\n",
    "        res_R_list[i] = torch.tensor(tuple(residues_data[\"R\"][key].values()))#.append(R_tensor)\n",
    "\n",
    "        # Convert zs data for this residue to a tensor.\n",
    "        z_tensor = torch.tensor(zs[pdb][key])\n",
    "        res_z_list[i]=z_tensor\n",
    "        keys[i] = key\n",
    "\n",
    "        # Record the index range for this residue in the concatenated tensors.\n",
    "        n_nodes =  z_tensor.shape[0]\n",
    "        #residue_idx_ranges[i] = (global_start, global_start + z_tensor.shape[0])\n",
    "        global_start += z_tensor.shape[0]#n_nodes\n",
    "\n",
    "    # Concatenate all residues together.\n",
    "    all_coords = torch.cat(tuple(res_R_list.values()), dim=0).detach().numpy()#torch.cat(res_R_list, dim=0)  # shape: [total_nodes, 3]\n",
    "    node_features = torch.cat(tuple(res_z_list.values()), dim=0) # shape: [total_nodes, feature_dim] (if feature_dim exists)\n",
    "\n",
    "    # Optionally, store the node features for the pdb (if needed later).\n",
    "    #all_nodes[pdb] = node_features\n",
    "\n",
    "    # --- Build query points for nearest neighbors ---\n",
    "    # For example, using the last coordinate of each residue.\n",
    "    #query_points = torch.stack([R[-1] for R in res_R_list])\n",
    "\n",
    "    # Build a NearestNeighbors model using all_coords.\n",
    "    nbrs = NearestNeighbors(radius=cutoff_radius).fit(all_coords)\n",
    "    neigh_indices_list = nbrs.radius_neighbors(torch.stack([R[-1] for R in res_R_list.values()]).detach().numpy(), return_distance=False)\n",
    "\n",
    "    # --- Build neighborhoods while preserving parent (residue) id ---\n",
    "    # Instead of using an integer index, we use the residue key as the dictionary key.\n",
    "    #neighborhoods = {}\n",
    "    last=0\n",
    "    Rs,neighbors={},{}\n",
    "    nmap={}\n",
    "    # Here, each query corresponds to a residue in the order of residue_keys.\n",
    "    for i,n in enumerate(neigh_indices_list):\n",
    "    #contacts = n[1] #connectivity\n",
    "# for i, neighbor_idx in enumerate(neigh_indices_list):\n",
    "    # Extract features from the zs tensor for these neighbor indices.\n",
    "    #hf,hpos = node_features[torch.tensor(contacts)], all_coords[torch.tensor(r[1])]\n",
    "    # Use the corresponding residue key as the neighborhood key.\n",
    "    #key = residue_keys[i]\n",
    "    #neighborhoods[i] = (hf, hpos)\n",
    "        l= res_z_list[i].shape[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        #for nnodes in n[1:]:\n",
    "        next_last=last+l\n",
    "        for j in range(last,next_last):\n",
    "            nmap[j] = i #n[0] #id map\n",
    "        \n",
    "        #Rs[i] = r\n",
    "        #conn={i:{contacts[0]:r}} #who hes connected to\n",
    "        last=next_last\n",
    "        \n",
    "    #print(nmap)\n",
    "\n",
    "\n",
    "    hoods={}\n",
    "    for i in range(len(neigh_indices_list)):\n",
    "        hood = neigh_indices_list[i]\n",
    "        \n",
    "        myi=nmap[hood[0]]\n",
    "        hoodo = [(res_z_list[nmap[myi]].unsqueeze(1), res_R_list[nmap[myi]])]\n",
    "        fertig=[myi]\n",
    "            #print(idx)\n",
    "        for k in hood[1:]:\n",
    "            if nmap[k] not in fertig:\n",
    "                #print(nmap[k])\n",
    "                \n",
    "                fertig.append(nmap[k])\n",
    "                #Set([res_z_list[nmap[k]] for k in hood])\n",
    "                #print(len(res_z_list[nmap[k]]),len(res_R_list[nmap[k]]))\n",
    "                \n",
    "                hoodo.append((res_z_list[nmap[k]].unsqueeze(1), res_R_list[nmap[k]]))\n",
    "            \n",
    "            \n",
    "                #print(\"else\")\n",
    "\n",
    "        \n",
    "        #hf, hpos = torch.cat([res_z_list[nmap[k]] for k in hood]), torch.vstack([res_R_list[nmap[k]] for k in hood])#in order\n",
    "        hoods[i]=torch.vstack([torch.hstack(h) for h in hoodo])\n",
    "        #print(hf)\n",
    "        #break\n",
    "\n",
    "        #hf,hpos = node_features[torch.tensor(hood[1:])], all_coords[torch.tensor(hood[1:])]\n",
    "        #print(hf,hpos)\n",
    "        \n",
    "\n",
    "    #for i,hood in enumerate(conn.values()):\n",
    "\n",
    "    #r=res_R_list[i]\n",
    "\n",
    "\n",
    "    all_hoods[pdb] = hoods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_radius=10\n",
    "all_hoods={}\n",
    "for pdb in pdbs:\n",
    "    if not zs[pdb]:\n",
    "        print(pdb)\n",
    "        continue\n",
    "\n",
    "    residues_data = data.residues[pdb]\n",
    "    residue_keys = list(zs[pdb].keys())\n",
    "\n",
    "    # --- Build lists of tensors and record slice ranges ---\n",
    "    # Each residue may have a different number of nodes.\n",
    "    # We'll build two lists: one for coordinates (\"R\") and one for zs features.\n",
    "    res_R_list = {}  # list of tensors of shape [n_i, 3]\n",
    "    res_z_list = {} # list of tensors for zs features (shape may vary)\n",
    "    # This dict maps each residue key to the slice (start, end) in the concatenated tensors.\n",
    "    #residue_idx_ranges = {}\n",
    "    global_start = 0\n",
    "    keys={}\n",
    "    for i,key in enumerate(residue_keys):\n",
    "        # Convert coordinates for this residue to a tensor.\n",
    "        # Note: using tuple(...) ensures a 2D tensor if the values are already arranged properly.\n",
    "        #R_tensor = torch.tensor(tuple(residues_data[\"R\"][key].values()))\n",
    "\n",
    "        res_R_list[i] = torch.tensor(tuple(residues_data[\"R\"][key].values()))#.append(R_tensor)\n",
    "\n",
    "        # Convert zs data for this residue to a tensor.\n",
    "        z_tensor = torch.tensor(zs[pdb][key])\n",
    "        res_z_list[i]=z_tensor\n",
    "        keys[i] = key\n",
    "\n",
    "        # Record the index range for this residue in the concatenated tensors.\n",
    "        n_nodes =  z_tensor.shape[0]\n",
    "        #residue_idx_ranges[i] = (global_start, global_start + z_tensor.shape[0])\n",
    "        global_start += z_tensor.shape[0]#n_nodes\n",
    "\n",
    "    # Concatenate all residues together.\n",
    "    all_coords = torch.cat(tuple(res_R_list.values()), dim=0).detach().numpy()#torch.cat(res_R_list, dim=0)  # shape: [total_nodes, 3]\n",
    "    node_features = torch.cat(tuple(res_z_list.values()), dim=0) # shape: [total_nodes, feature_dim] (if feature_dim exists)\n",
    "\n",
    "    # Optionally, store the node features for the pdb (if needed later).\n",
    "    #all_nodes[pdb] = node_features\n",
    "\n",
    "    # --- Build query points for nearest neighbors ---\n",
    "    # For example, using the last coordinate of each residue.\n",
    "    #query_points = torch.stack([R[-1] for R in res_R_list])\n",
    "\n",
    "    # Build a NearestNeighbors model using all_coords.\n",
    "    nbrs = NearestNeighbors(radius=cutoff_radius).fit(all_coords)\n",
    "    neigh_indices_list = nbrs.radius_neighbors(torch.stack([R[-1] for R in res_R_list.values()]).detach().numpy(), return_distance=False)\n",
    "\n",
    "    # --- Build neighborhoods while preserving parent (residue) id ---\n",
    "    # Instead of using an integer index, we use the residue key as the dictionary key.\n",
    "    #neighborhoods = {}\n",
    "    last=0\n",
    "    Rs,neighbors={},{}\n",
    "    nmap={}\n",
    "    # Here, each query corresponds to a residue in the order of residue_keys.\n",
    "    for i,n in enumerate(neigh_indices_list):\n",
    "    #contacts = n[1] #connectivity\n",
    "# for i, neighbor_idx in enumerate(neigh_indices_list):\n",
    "    # Extract features from the zs tensor for these neighbor indices.\n",
    "    #hf,hpos = node_features[torch.tensor(contacts)], all_coords[torch.tensor(r[1])]\n",
    "    # Use the corresponding residue key as the neighborhood key.\n",
    "    #key = residue_keys[i]\n",
    "    #neighborhoods[i] = (hf, hpos)\n",
    "        l= res_z_list[i].shape[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        #for nnodes in n[1:]:\n",
    "        next_last=last+l\n",
    "        for j in range(last,next_last):\n",
    "            nmap[j] = i #n[0] #id map\n",
    "        \n",
    "        #Rs[i] = r\n",
    "        #conn={i:{contacts[0]:r}} #who hes connected to\n",
    "        last=next_last\n",
    "        \n",
    "    #print(nmap)\n",
    "\n",
    "\n",
    "    hoods={}\n",
    "    for i in range(len(neigh_indices_list)):\n",
    "        hood = neigh_indices_list[i]\n",
    "        \n",
    "        myi=nmap[hood[0]]\n",
    "        hoodo = {myi:(res_z_list[nmap[myi]].unsqueeze(1), res_R_list[nmap[myi]])}\n",
    "        fertig={myi:myi}\n",
    "            #print(idx)\n",
    "        for k in hood[1:]:\n",
    "            if k not in fertig:\n",
    "                #print(nmap[k])\n",
    "                \n",
    "                fertig[k]=nmap[k]\n",
    "                #Set([res_z_list[nmap[k]] for k in hood])\n",
    "                #print(len(res_z_list[nmap[k]]),len(res_R_list[nmap[k]]))\n",
    "                \n",
    "                hoodo[k]=(res_z_list[nmap[k]].unsqueeze(1), res_R_list[nmap[k]])\n",
    "            \n",
    "            \n",
    "                #print(\"else\")\n",
    "\n",
    "        \n",
    "        #hf, hpos = torch.cat([res_z_list[nmap[k]] for k in hood]), torch.vstack([res_R_list[nmap[k]] for k in hood])#in order\n",
    "        hoods[i]=torch.vstack([torch.hstack(h) for h in hoodo.values()])\n",
    "        #print(hf)\n",
    "        #break\n",
    "\n",
    "        #hf,hpos = node_features[torch.tensor(hood[1:])], all_coords[torch.tensor(hood[1:])]\n",
    "        #print(hf,hpos)\n",
    "        \n",
    "\n",
    "    #for i,hood in enumerate(conn.values()):\n",
    "\n",
    "    #r=res_R_list[i]\n",
    "\n",
    "\n",
    "    all_hoods[pdb] = hoods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,hood in enumerate(all_hoods.values()):\n",
    "    for j,h in enumerate(hood.values()):\n",
    "        \n",
    "        \n",
    "    \n",
    "    #input = torch.hstack((h[0][0].unsqueeze(1),h[0][1]))\n",
    "        torch.save(h,f\"./4A/{i}_{j}\")\n",
    "\n",
    "torch.load(\"./4A/7_3\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
