{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import glob\n",
    "import edlib\n",
    "import os\n",
    "from utils import constants\n",
    "\n",
    "\n",
    "#data_dir = os.getcwd()\n",
    "constants = constants()\n",
    "fullcode = constants[\"titratables_full\"]\n",
    "code = constants[\"titratables_short\"]\n",
    "three_to_one = constants[\"aa_code\"]\n",
    "elements = constants[\"elements\"]\n",
    "cof = constants[\"cofactors\"]\n",
    "ligands = constants[\"ligands\"]\n",
    "#cnbrs = constants[\"disulfide_nn\"]\n",
    "amber=constants[\"amber_sites\"]\n",
    "\n",
    "\n",
    "\n",
    "class parser():\n",
    "    def __init__(self, gzipped_pdb, ref_pdb):\n",
    "        self.path    = gzipped_pdb\n",
    "        self.pdb     = gzipped_pdb[-7:-3]\n",
    "        self.targets =f\"../../data/targets/{self.pdb}.npz\"\n",
    "        self.save_dir = \"../inputs/\"\n",
    "        self.ligands = ligands\n",
    "        self.ref_pdb = ref_pdb\n",
    "        \n",
    "    def residue_map(self):\n",
    "            \"\"\"This function uses the RCSB reference pdb as the reference sequence\n",
    "            for sequence alignment through edlib. This allows for the incoming residue \n",
    "            numbers to be arbitrary and allows for custom PDDs and to be fixed by modeling\n",
    "            software (does not support files with non-standardized naming conventions, where\n",
    "            standard is considered to be Amber, even if that isn't correct.)\n",
    "\n",
    "            **does not support files without:\n",
    "                 TER records \n",
    "                 Chain records (\"A\", \"X\")\n",
    "                 gzipped\n",
    "\n",
    "            cur = modeled and parsed | ref = rcsb (assumed pkPDB numbering)\n",
    "\n",
    "            three_to_one is a dict takes as input key the residue name, which if it is standard as it should be, converts it to its one letter nickname e.g. GLU --> E. \n",
    "            If it is not recognized, e.g. a typesetter error in original PDB making GLX for GLU, the nickname will be \"X\". Currently, these will still be paired as\n",
    "            two X's rather than excluded on the basis of being an X. However, if this code is used as intended with a PDBFixed PDB or analogous (find nonstandard residue\n",
    "            names and replace with standard names built in), this should ideally not occur where two Xs are paired. In any case, it will not affect our results if two\n",
    "            Xs are paired, because the Xs will not ever be our five titratable residues which needed the standard names \"HIS/ASP/etc.\" to be parsed. The fact that \n",
    "            every residue is used in this function is only because this is needed for full integrity alignment. \n",
    "\n",
    "            The output is matched pairwise such that insertions and deletions trigger \n",
    "            the increase of their respective counter, which retrieves their keys made by\n",
    "            residue number, chain, and residue name e.g. 12A0 = residue 12, chain A, HIS \n",
    "            (where HIS = 0 is an internal dictionary code defined in utils.py.)\n",
    "\n",
    "            These matched residues form the mapping which are the only IDs which get through\n",
    "            the parser to be matched with the pkPDB targets.\n",
    "\n",
    "            ##New##\n",
    "            Manual parsing to retrieve sequence: Atom-ine by Atom-line (exclude ligands) the code retrieves the protein sequence.\n",
    "            Residue by residue, the IDs are formed (12A0 = res12,chA,HIS).\n",
    "            Chain by chain (triggered by \"TER\"), the chain-sequences are made (b\"EEH\" = GLU GLU HIS) and appended to protein-wide chain list \"seqs\",\n",
    "            as well as their corresponding IDs ([b\"1A4\",b\"2A4\",b\"3A0\"] == Glu Glu His) \n",
    "            During parse, the last residue is remembered and inaction taken upon its subsequent parsing.\n",
    "\n",
    "            Once the sequences and key lists are formed, they are input into edlib. \n",
    "            The keys will be parsed by index and thus doesnt include the deletions which will appear as \"-\"'s in edlibs nice alignment.\n",
    "\n",
    "            After the alignment is made, the matches are paired such that the index value slicing the og/model IDs list         #TODO: rename for clarity?\n",
    "            increases when there is a match and when there isnt. When there is, a key value pair is made between the             #TODO: save mapping? woulda been good..\n",
    "            residue IDs (\"1A4\" <-->\"100A4\") between the OG residue and the modeled, thereby mapping any two residue numbering schemes.\n",
    "            When deletion in the modeled (should not occur, ever, for this project since at the baseline \"modeled\" PDBs are the RCSB itself, but \n",
    "            121k PDBs = mysteries), it should mean the next residue in the references matches with the current frame of the nidx, which wouldnt have\n",
    "            received a number during enumeration as it enumerated on the unaligned sequences and thus no \"-\"'s. This is why i, despite it retrieves\n",
    "            from reference seq, is incremented upon deletion in the modeled sequence.\n",
    "\n",
    "            The opposite logic is true for additions in the modeled structure with missing residues in the original strucute inducing an increase in\n",
    "            the index slicing the modeled pdb's residue ID keys. This is assumed to be the baseline behavior for this project\n",
    "\n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            # helper: extract per-chain sequences & idx-lists into parallel lists\n",
    "            def extract(path):\n",
    "                seqs,seq,idxs,keys = [],[],[],[]   # list of lists of idx‐bytes\n",
    "                lastkey=None\n",
    "                with gzip.open(path, \"rb\") as fh:\n",
    "                    for raw in fh:\n",
    "                        if not raw.startswith(b\"ATOM\"): continue\n",
    "                        elif raw.startswith(b\"TER\"):\n",
    "                            seqs.append(\"\".join(seq))\n",
    "                            idxs.append(keys)\n",
    "                            keys,seq=[],[]\n",
    "                            continue\n",
    "\n",
    "                        ch,resi = raw[21:22], raw[17:20]\n",
    "                        key = raw[22:26].strip() + ch + fullcode.get(resi,b\"\") #code is {\"HIS\":0..}\n",
    "                        \n",
    "                        if lastkey==key:\n",
    "                            lastkey=key\n",
    "                            continue\n",
    "\n",
    "                        keys.append(key)\n",
    "                        aa1 = three_to_one.get(resi, \"X\") \n",
    "                        seq.append(aa1)\n",
    "                        lastkey=key\n",
    "                        \n",
    "                seqs.append(\"\".join(seq))\n",
    "                idxs.append(keys)\n",
    "\n",
    "                # finalize: join seqs, convert idxs to np.char.array\n",
    "                seq_strs = seqs#[\"\".join(s) for s in seqs]\n",
    "                idx_arrs  = [np.char.array(i) for i in idxs]\n",
    "                return seq_strs, idx_arrs\n",
    "\n",
    "            # read ref & current\n",
    "            ref_seqs, ref_idxs = extract(self.ref_pdb)\n",
    "            cur_seqs, cur_idxs = extract(self.path)\n",
    "\n",
    "            mapping = {}\n",
    "            # align each chain by index\n",
    "            for c, oseq in enumerate(ref_seqs):\n",
    "                if c >= len(cur_seqs):\n",
    "                    break\n",
    "                nseq  = cur_seqs[c]\n",
    "                nidx = cur_idxs[c]\n",
    "                oidx = ref_idxs[c]\n",
    "\n",
    "                # 1) compute the alignment path\n",
    "                res = edlib.align(oseq, nseq, mode=\"NW\", task=\"path\")\n",
    "                # skip if no CIGAR produced\n",
    "                if not res.get(\"cigar\"):\n",
    "                    # you could log a warning here if you like:\n",
    "                    print(f\"Warning: no alignment path for chain {c}, skipping\")\n",
    "                    self.mapping=None\n",
    "                    return\n",
    "\n",
    "                nice = edlib.getNiceAlignment(res, oseq, nseq)\n",
    "                ref_aln, qry_aln = nice[\"target_aligned\"], nice[\"query_aligned\"]\n",
    "                i = j = 0\n",
    "                for r_c, q_c in zip(ref_aln, qry_aln):\n",
    "                    if r_c != \"-\":\n",
    "                        if q_c != \"-\":\n",
    "                            if r_c == q_c:\n",
    "                                mapping[nidx[j]] = oidx[i]\n",
    "                                i += 1; j += 1\n",
    "                            else: \n",
    "                                i += 1; j += 1\n",
    "                        else: j += 1 #insertion\n",
    "                    else: i += 1 #deletion\n",
    "\n",
    "            self.mapping = mapping\n",
    "\n",
    "    def parse_titratable_lines(self, lines):\n",
    "        \"\"\"get info from asp,glu,his,cys,tyr. removes hydrogens. \n",
    "        \n",
    "        It strips residue numbers of their insertions, which have been fixed by a mender which never has two residue numbers the same. The chance of \n",
    "        failure is probably small but still real if this is used not as intended e.g. PARSING A PDB WHICH HAS INSERTION CODES THAT DONT BY\n",
    "        A GOOD CHANCE HAVE A UNIQUE RES NUMBER EXCLUDING INSERTION CODE (A LETTER) FOR EACH RESIDUE IN A CHAIN.\n",
    "\n",
    "        \n",
    "        If there is a titratable heavy atom that is the hydrogen pair doner/acceptor defined by Amber and thus in amber_set, then the residue-wise information is \n",
    "        retained in the species/coors long term memory. If the parser never encountered a titratable site (e.g. parsing a raw rcsb pdb with unresolved Lysine side chain\n",
    "        and thus a missing NZ atom), then the residue information will instead be sent to \"others\", which is the atoms from non-titratable including\n",
    "        ligands, residues, and non-labeled titratable residues.\"\"\"\n",
    "            \n",
    "        amber_set = amber.values()      # byte-strings of titratable names\n",
    "        self.species,self.coors, self.sites  = [],[],[]\n",
    "        last_resnum                                   = None\n",
    "        cur_species, cur_coords     =[],[]\n",
    "        amber_flag=False\n",
    "        cur_species, cur_coords, others_c,others_s      =[],[],[],[]\n",
    "\n",
    "        # 1) Single-pass parse & group by residue\n",
    "        for line in lines:\n",
    "            if line != b\"X\": #chain\n",
    "                try:\n",
    "                    #skip hydrogens\n",
    "                    if line.lstrip().startswith(b\"H\"): continue\n",
    "                    #strip insertions \n",
    "                    resnum = line[10:15].strip(b\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "                \n",
    "\n",
    "                    # new residue?  put previous residue in long term memory of potential sites (self.species), or other resis (self.others) depending on flag status\n",
    "                    if resnum != last_resnum and cur_species:\n",
    "                        if amber_flag: \n",
    "                            self.species.append(cur_species)\n",
    "                            self.coors.append(cur_coords)\n",
    "                        else: \n",
    "                            others_s.append(cur_species)\n",
    "                            others_c.append((cur_coords))\n",
    "                        cur_species, cur_coords = [], []\n",
    "                        amber_flag=False\n",
    "                    \n",
    "                    #per-residue short term memory accumulates Z + pos info\n",
    "                    cur_species.append(elements[line[-9:].split()[0][0:]])\n",
    "                    cur_coords.append((float(line[18:26]), float(line[26:34]), float(line[34:42])))\n",
    "\n",
    "                    #only retain resis with resolved/modeled titratable sites. these lines generate the IDs to be paired with pkPDB targets downstream\n",
    "                    if line[:5].strip() in amber_set:\n",
    "                        amber_flag=True\n",
    "                        self.sites.append(line)\n",
    "\n",
    "                    last_resnum = resnum\n",
    "                except Exception as e:\n",
    "                    print(self.pdb)\n",
    "                    print(e)\n",
    "                    print(line)\n",
    "                    return False\n",
    "            else: #CHAIN\n",
    "                #if resnum != last_resnum and cur_species:\n",
    "                if amber_flag: \n",
    "                    self.species.append(cur_species)\n",
    "                    self.coors.append(cur_coords)\n",
    "                else: \n",
    "                    others_s.append(cur_species)\n",
    "                    others_c.append((cur_coords))\n",
    "                cur_species, cur_coords = [], []\n",
    "                amber_flag=False\n",
    "                    \n",
    "                   \n",
    "                \n",
    "\n",
    "                    \n",
    "\n",
    "            \n",
    "        #final long term memory capture\n",
    "        if amber_flag: \n",
    "            self.species.append(cur_species)\n",
    "            self.coors.append(cur_coords)\n",
    "        else: \n",
    "            others_s.append(cur_species)\n",
    "            others_c.append((cur_coords))\n",
    "        if others_s: self.others.append((np.concatenate(others_s),np.vstack(others_c)))\n",
    "\n",
    "        #save as arrays for downstream mask operations\n",
    "        self.species,self.coors=np.array(self.species,dtype=object),np.array(self.coors,dtype=object)\n",
    "        return True \n",
    "        \n",
    "    \n",
    "    def aggregate_others(self):\n",
    "        \"\"\"#TODO\"\"\"\n",
    "        #def stack_columns(data):\n",
    "        \"\"\"\n",
    "\n",
    "        It simply reshapes all the information ive thrown into self.others until it is called in run().\n",
    "\n",
    "        ###############\n",
    "        data: list of (int_arr, coord_arr) tuples, where\n",
    "        - int_arr is either a 1D np.ndarray of ints or an object array of int sub-arrays\n",
    "        - coord_arr is either a 2D np.ndarray of shape (M,3) or an object array of 2D sub-arrays\n",
    "        Returns:\n",
    "        - all_ints: 1D np.ndarray of all ints concatenated\n",
    "        - all_coords: 2D np.ndarray of shape (total_rows, 3)\n",
    "        \"\"\"\n",
    "        int_chunks, coord_chunks = [], []\n",
    "        for int_arr, coord_arr in self.others:\n",
    "            int_chunks.append(int_arr)\n",
    "            coord_chunks.append(coord_arr)\n",
    "        all_ints   = np.concatenate(int_chunks, axis=0)\n",
    "        all_coords = np.vstack(coord_chunks)\n",
    "        return all_ints, all_coords\n",
    "\n",
    "    \n",
    "    def parse_others(self, lines):\n",
    "        \"\"\"parse all other ligand HETATM and non-ligand ATOM lines using entire periodic table\"\"\"\n",
    "        species, coors = [], []\n",
    "        for line in lines:\n",
    "            if line != b\"X\": #CHAIN\n",
    "            # skip hydrogens\n",
    "                if not line[0:2].strip().startswith(b\"H\"):\n",
    "                    #here I use cofactors as my Z number dict --> entire periodic table\n",
    "                    species.append(cof[line[-9:].split()[0][0:]])\n",
    "                    coors.append((float(line[18:26]), float(line[26:34]), float(line[34:42])))\n",
    "\n",
    "\n",
    "    def parse_pdb(self):\n",
    "        \"\"\"to do try except re: encoding/gzipped\n",
    "        #hi b'HG23 ILE A 492      65.222 102.163  26.506  1.00  0.00           H   std\\n'\n",
    "        # #encode everything if user didnt gzip their filess? #TODO\n",
    "        # TODO: here is where we can encode user input.\n",
    "\n",
    "        Ligands excludes solvent molecules (exlclusion by lack of membership in ligands).\n",
    "        Should a solvent molecule have a residue name of a metal of another element,\n",
    "        they will be parsed!\n",
    "    \n",
    "\n",
    "        This code opens the gzipped pdb and line by line extracts ATOM and HETATM records.\n",
    "        \n",
    "        For atom records, if they are titratable, they are sent to parse_titratable_lines.\n",
    "        This uses a Z table dict which is only 6,7,8,16 (C,N,O,S).\n",
    "\n",
    "        Otherwise, they are sent to parse_others, which parses nontitratable residues (ATOM),\n",
    "        metal (or non-metal pure elements, but goal is metal) cofactors (HETATM records),\n",
    "        and ligands (HETATM) with the full periodic table.*\n",
    "\n",
    "        Others is continuously appended even outside (after) this function so nobody gets left behind.\n",
    "        \n",
    "        *Despite there is H in the cofactors dict, only lines which dont have Hydrogens \n",
    "        (see parse_others) enter the dictionary.\"\"\"\n",
    "\n",
    "        with gzip.open(self.path, \"rb\") as f: #TODO?\n",
    "            lines=f.readlines()\n",
    "\n",
    "        titratables, others  = [], []\n",
    "\n",
    "        #get ligands\n",
    "        for line in lines:\n",
    "            if line.startswith(b\"HETATM\"):\n",
    "                if line[16:20].strip() in self.ligands or cof: others.append(line[12:])\n",
    "\n",
    "            elif line.startswith(b\"ATOM\"):\n",
    "                if line[16:20].strip() in fullcode: titratables.append(line[12:])\n",
    "                else: others.append(line[12:])\n",
    "            elif line.startswith(b\"TER\"):\n",
    "                titratables.append(b\"X\")\n",
    "                others.append(b\"X\")\n",
    "\n",
    "        self.parse_others(others)\n",
    "        flag = self.parse_titratable_lines(titratables) #skips lines without an element in my dictionaries. e.g. Deterium, ionized O and N.\n",
    "        return flag\n",
    "        \n",
    "    def get_disulfides(self,lines):\n",
    "        \"\"\"in: the SITE of the titratable lines and thus Sulfur when Cys\n",
    "        intend to minic pdb2pqr 2.05 cutoff was my understanding\n",
    "    \n",
    "        for cys sulfur lines the coords of the sulfurs are entered into a brute force KNN with radius 2.1 (room from 2.05 for error).\n",
    "        If they have other sulfur neighbors, who are the neighbors are retained and they are sent to others and their IDs get removed\n",
    "         with masks.\n",
    "          \n",
    "           Note it doesnt need to work (=correctly identify) with our labeled data thus was for development and to not forget until unlabeled disaster\n",
    "           \n",
    "           somebodz could test if it worked by running same code run on jesses computer with it hashed out and seeing that the lengths of the pk arrays\n",
    "           is the same and if not, by how much.\"\"\"\n",
    "        cys_lines = [(i,line) for i,line in enumerate(lines) if line[5:6] == b\"C\"]\n",
    "        if cys_lines:   \n",
    "            cys_coors = [(float(line[1][18:26]),float(line[1][26:34]),float(line[1][34:42])) for line in cys_lines] #TODO confirm its S?\n",
    "            cnbrs.fit(cys_coors)\n",
    "            nbrs = cnbrs.radius_neighbors(cys_coors,radius=2.1, return_distance=False) #TODO\n",
    "            bridges=[]\n",
    "            for a,cys in zip(nbrs,cys_lines):\n",
    "                if len(a) == 2:\n",
    "                    bridges.append(cys[0])\n",
    "            self.disulfides=bridges\n",
    "        else: self.disulfides=None\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"it gets the residue map between the RCSB and assumed pkPDB resi numbering scheme. using edlib seq alignment and parsing 2 pdbs.\n",
    "        \n",
    "        Then, the pdb is parsed line by line and hydrogens stripped. Here, the sites corresponding to S, NZ, etc., are gathered. If no\n",
    "        site, it is sent to others instead of species. that is flag based\n",
    "\n",
    "        The ids for the modeled and thus parsed PDb are gotten directly from the titratable site ATOM lines e.g. S, NZ..\n",
    "        if they exist also in the RCSB pdb, they are assumed a potential candidate for having a pypka label and retained\n",
    "        in species/coors and made an ID. Here I form the sitecoors, too, are the centers for forming neighborhoods \n",
    "        downstream.\n",
    "\n",
    "        I append retained sites (being resolved in OG PDB) for bookkeeping \n",
    "\n",
    "        Then, the cys bridges determined with KNN brute force are actually never removed from species. \n",
    "        I suppose I did this on purpose as I didnt wanna\n",
    "        miss no labels developing, but would fail unlabeled data #TODO #ALERTA\n",
    "\n",
    "        TODO: remove cysteines from SPECIES/COORDS/ (or? ) IDS, or remove disulfides because it is redundant sending\n",
    "        unmatched labels which include cys  bridges, to others, after already sending them there after disulfides\n",
    "\n",
    "\n",
    "        final mask:\n",
    "        Only the structure info from modeled residues is retained. We return the indices corresponding to the intersection\n",
    "        of the IDs which for the modeled pdb, should be the same length as the species/coords by design but not default #TODO easy integrity check\n",
    "\n",
    "        and most of the time is the same as pkPDB targets/sites but far from always, especially with modeled missing residued\n",
    "        in modeled parsed PDB.\n",
    "\n",
    "        The official IDs are those which are shared by both, and thus not my PDBs. lol. forgot to save map until past execute. good thing code is reproducible\n",
    "    \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\n",
    "        \n",
    "        \"\"\n",
    "        #if not os.path.exists(self.targets): #skip target files with only ntr and ctr, that I alreadz deleted\n",
    "            \n",
    "        #    return\n",
    "            \n",
    "        \n",
    "        #get species/pos info of titratable residues\n",
    "        self.residue_map()\n",
    "        if not self.mapping:\n",
    "            with gzip.open(\"../badparse.gz\", \"ab\") as f:\n",
    "                f.write(self.pdb.encode())\n",
    "                f.write(b\"\\n\")\n",
    "                return\n",
    "        else:\n",
    "            #print(self.mapping.items())\n",
    "            #with gzip.open(\"../res_maps.gz\", \"ab\") as f:\n",
    "            #    f.write(b\"\\n\")\n",
    "            #    f.write(self.pdb.encode())\n",
    "            #mapping=tuple(self.mapping.items())\n",
    "            #print(mapping)\n",
    "            #new, old = list(mapping[0]),list(mapping[1])\n",
    "            #print(new,old)\n",
    "            #np.save(np.array)\n",
    "            map=list(self.mapping.items())\n",
    "            np.savez_compressed(f\"../res_maps/{self.pdb}.npz\", map=map)\n",
    "\n",
    "                \n",
    "\n",
    "          \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        self.others=[]\n",
    "        sitecoors,ids,sites=[],[],[]\n",
    "        \n",
    "        flag=self.parse_pdb()\n",
    "        if not flag: #if there was an issue in parsing the elements. should produce errors for Deterium, N1+, O-, and EP.\n",
    "            with gzip.open(\"../badparse.gz\", \"ab\") as f:\n",
    "                f.write(self.pdb.encode())\n",
    "                f.write(b\"\\n\")\n",
    "                return\n",
    "    \n",
    "        \n",
    "        for line in self.sites:\n",
    "            id=line[10:18].strip() + line[9:10] + code[line[5:6]] #get ids frrom sites\n",
    "            id=self.mapping.get(id) #from map\n",
    "            if id:\n",
    "                ids.append(id)\n",
    "                sitecoors.append((float(line[18:26]),float(line[26:34]),float(line[34:42])))\n",
    "                sites.append(line)\n",
    "        sitecoors=np.array(sitecoors).astype(np.float32)\n",
    "       #that fixes when there is not a titratable site in the titratable residue. now onto targets\n",
    "        \n",
    "        #self.get_disulfides(sites) \n",
    "        #if self.disulfides: \n",
    "        #    sulf=np.array(self.disulfides)\n",
    "        #    self.others.append((np.concatenate(self.species[sulf]), np.vstack(self.coors[sulf]))) #ALERTA\n",
    "\n",
    "        pkpdb=np.load(self.targets)\n",
    "        common, pidx, midx = np.intersect1d(pkpdb[\"ids\"], ids,\n",
    "                                            return_indices=True)\n",
    "        site_mask = np.ones(len(self.species), dtype=bool)\n",
    "        site_mask[midx] = False                            # False → keep\n",
    "\n",
    "        if site_mask.any():                                # spill invalid sites\n",
    "            self.others.append((np.concatenate(self.species[site_mask]),\n",
    "                                np.vstack(self.coors[site_mask])))\n",
    "            \n",
    "        self.coors,self.species = self.coors[midx], self.species[midx]\n",
    "        others=self.aggregate_others()\n",
    "        #self.hoods(self.coors[midx], self.species[midx])#, self.sitecoors[midx])\n",
    "        if others[1]:\n",
    "\n",
    "            atom_coords = np.concatenate([*self.coors, np.vstack(others[1])], axis=0).astype(np.float32)\n",
    "            atom_elems  = np.concatenate([*self.species,\n",
    "                                        others[0]]).astype(np.uint8)\n",
    "        else:\n",
    "            atom_coords = np.concatenate([*self.coors]).astype(np.float32)\n",
    "            atom_elems  = np.concatenate([*self.species]).astype(np.uint8)\n",
    "        print(self.species)\n",
    "        np.savez_compressed(\n",
    "    self.save_dir + f\"{self.pdb}.npz\",\n",
    "    z=atom_elems,\n",
    "    pos=atom_coords,\n",
    "    pks=pkpdb[\"pks\"][pidx],\n",
    "    ids=common,\n",
    "    sites=sitecoors[midx]\n",
    "\n",
    ")\n",
    "        return self\n",
    "\n",
    "#to do rerun with right cysteins and keep map this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 8, 8])\n",
      " list([7, 6, 6, 8, 6, 6, 6, 8, 8]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])\n",
      " list([7, 6, 6, 8, 6, 6, 7, 6, 6, 7]) list([7, 6, 6, 8, 6, 6, 6, 6, 7])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:456: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.parser at 0x7f26b0f2eba8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser(\"../../data/pdbs/fixed/101m.gz\",\"../../data/pdbs/rcsb/101m.pdb.gz\").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501 501 54 54 54\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #after debug\n",
    "dat=np.load(\"../inputs/101m.npz\",allow_pickle=True)\n",
    "z       = dat[\"z\"]           # (N,)\n",
    "pos     = dat[\"pos\"]         # (N,3)\n",
    "#seglen  = dat[\"seglen\"]      # (S,)\n",
    "anchor  = dat[\"sites\"]      # (S,)\n",
    "ids     = dat[\"ids\"]         # (S,)\n",
    "pks     = dat[\"pks\"]         # (S,)\n",
    "#oz      = dat[\"others\"]\n",
    "pks.dtype\n",
    "print(len(z),len(pos),len(anchor),len(ids),len(pks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 8, 8, 7, 6, 6, 8, 6, 6, 6, 6, 6,\n",
       "       6, 6, 8, 8], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'7A3', b'7B3'], dtype='|S3')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
