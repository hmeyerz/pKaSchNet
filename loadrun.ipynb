{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "from egnn_pytorch import EGNN_Network\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "charges = {'HIS': 1, 'LYS': 1,\n",
    "           'ASP': -1, 'GLU': -1,  # carboxylate\n",
    "           'CYS': 0, 'CYS': 0,\n",
    "           'TYR': 0}   # thiol and phenol prot\n",
    "\n",
    "cations = {'HIS': (\"HD1\", \"HD2\", \"HE1\", \"HE2\"),\n",
    "        'ASP':None,\n",
    "        \"LYS\":(\"HZ1\", \"HZ2\",\"HZ3\"),\n",
    "        \"TYR\":\"HH\",\n",
    "        \"GLU\":None,\n",
    "        \"CYS\":'HG',\n",
    "        \"ARG\": (\"HE11\",\"HE12\", \"HE21\", \"HE22\"),\n",
    "        \"THR\":\"HG1\",\n",
    "        \"SER\":\"HG\",\n",
    "        \"TRP\":\"HE1\"}\n",
    "\n",
    "\n",
    "anions = {\"HIS\":(\"ND1\", \"ND2\"),\n",
    "          \"ASP\":(\"OD1\",\"OD2\"),\n",
    "          \"LYS\":\"NZ\",\n",
    "          \"TYR\":\"OH\",\n",
    "          \"GLU\":(\"OE1\", \"OE2\"),\n",
    "          \"CYS\":\"SG\",\n",
    "          \"ARG\": (\"NE1\",\"NE2\"),\n",
    "          \"THR\":\"OG1\",\n",
    "          \"SER\":\"OG\",\n",
    "          \"TRP\":\"NE1\"}\n",
    "\n",
    "labels = {\"HIS\":6.08,\n",
    "          \"ASP\":3.9,\n",
    "          \"LYS\":10.5,\n",
    "          \"TYR\":10.1,\n",
    "          \"GLU\":4.3,\n",
    "          \"CYS\":8.28}\n",
    "\n",
    "\n",
    "\n",
    "prot = charges.keys()\n",
    "\n",
    "Rs, solvents = {}, {}\n",
    "counter = 0\n",
    "all_ppos, all_pspecies = {}, {}\n",
    "positions = []\n",
    "s={}\n",
    "all_ions={}\n",
    "\n",
    "###############################\n",
    "resi_keys=tuple(resis.keys())\n",
    "\n",
    "pspecies={}\n",
    "for res in resi_keys:\n",
    "    pspecies, Rs = {},{}\n",
    "    residict = resis[res]\n",
    "    for atom, pos in residict.items():\n",
    "        node = atom.split()  # name, pos\n",
    "        #rint(node)\n",
    "        nodei = int(node[1])\n",
    "        pspecies.update({node[0] : nodei})\n",
    "        #rint(nodei)\n",
    "        #print(i)\n",
    "        #i += 1\n",
    "        #rint({node[0] : nodei}, pos)\n",
    "        Rs[nodei] = pos\n",
    "\n",
    "\n",
    "    all_ppos[res] = Rs  # ({res: {node[1]: pos}})\n",
    "    all_pspecies[res] = pspecies\n",
    "\n",
    "\n",
    "\n",
    "for key in resi_keys:\n",
    "    \n",
    "    resname = key[:3]\n",
    "    species = all_pspecies[key]\n",
    "    ion = torch.zeros(len(species))\n",
    "\n",
    "\n",
    "    counter1, counter2 =0, 0\n",
    "    species_map={}\n",
    "    for k, v in species.items():\n",
    "        \n",
    "        element = k[0]\n",
    "        #print(element)\n",
    "        if element not in species_map:\n",
    "            species_map[element] = counter1\n",
    "            counter1 +=1\n",
    "        species[k] = counter2\n",
    "        counter2 += 1\n",
    " \n",
    "    cats =cations[resname]\n",
    "    if cats:\n",
    "        cs = [c for c in cats if c is not None]\n",
    "        cats =[species.get(a) for a in cs]\n",
    "        cs = [c for c in cats if c is not None]\n",
    "        ion[cs] = 1\n",
    "\n",
    "\n",
    "    ans = [species.get(a) for a in anions[resname]]\n",
    "    ion[ans]  =-1\n",
    "    \n",
    "    all_ions[key] = ion\n",
    "\n",
    "    feats = np.where(np.array(list(species.keys())) == species_map['H'], torch.tensor(1), torch.tensor(0))\n",
    "    #feats = torch.tensor(feats).unsqueeze(0)\n",
    "\n",
    "\n",
    "#turn species into numbers\n",
    "#get pos tensor\n",
    "\n",
    "els = list(species.keys())\n",
    "es = [a[0] for a in els]\n",
    "\n",
    "final=[]\n",
    "\n",
    "fs = [a for a in es if a == \"H\"]\n",
    "for a in es:\n",
    "    if a==\"H\":\n",
    "        final.append(1)\n",
    "    else:\n",
    "        final.append(0)\n",
    "\n",
    "Hs = torch.tensor(final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzmax(x, temp):\n",
    "    \"\"\"\n",
    "    Computes Boltzmann probabilities (Euclidean Softmax) for a tensor of distances.\n",
    "    \n",
    "    Parameters:\n",
    "        distances (torch.Tensor): feature tensor\n",
    "        temperature (float): The temperature parameter to control the sharpness.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of notprobabilities \n",
    "    \"\"\"\n",
    "    # Scale the distances by temperature\n",
    "    scaled_distances = x / temp\n",
    "    \n",
    "    # Exponentiate the scaled distances\n",
    "    x = torch.exp(scaled_distances)\n",
    "    p = x / torch.max(x) #doing max instead of \n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "def contrastive_loss(latent_features, species):\n",
    "    \"\"\"seperates latent space by enforcing dissimilarity between negative and positive ions\"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    positive_pairs = [(i, j) for i, j in itertools.combinations(range(len(species)), 2) if species[i] == species[j]]\n",
    "    negative_pairs = [(i, j) for i, j in itertools.combinations(range(3), 2) if species[i] != species[j]]\n",
    "\n",
    "\n",
    "    # Positive pairs\n",
    "    for i, j in positive_pairs:\n",
    "        z_i, z_j = latent_features[i], latent_features[j]\n",
    "        dist = torch.norm(z_i - z_j, p=2)  # Euclidean distance\n",
    "        loss += dist ** 2  # Minimize distance for positive pairs\n",
    "\n",
    "    # Negative pairs\n",
    "    for i, j in negative_pairs:\n",
    "        z_i, z_j = latent_features[i], latent_features[j]\n",
    "        dist = torch.norm(z_i - z_j, p=2) #seperation\n",
    "        loss += torch.clamp(dist, min=0) ** 2 #enforce sepertion\n",
    "\n",
    "\n",
    "\n",
    "    return loss / (len(positive_pairs) + len(negative_pairs))\n",
    "\n",
    "\n",
    "\n",
    "def model(num_nodes, dim, depth, lr, weight_decay):\n",
    "    net = EGNN_Network(\n",
    "        num_tokens = 100, #vocabulary siye, number of unique species\n",
    "        num_positions = num_nodes,  #number of nodes         # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "        dim = dim,# #internal rep size. c has square dependence. richer rep but overfitting for small d.s.\n",
    "        depth = depth, #number of layers #deeper need more memort to store intermediate reps\n",
    "        num_nearest_neighbors = 1, #number of nearest neighbors to consider #make this the max hood size\n",
    "        coor_weights_clamp_value = 2   # absolute clamped value for the coordinate weights, needed if you increase the num neareest neighbors\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return net, optimizer\n",
    "\n",
    "\n",
    "\n",
    "def loop(nepochs, coors, species, ion_labels, model, optimizer, negative_slope):\n",
    "\n",
    "    es = [a[0] for a in species]\n",
    "    final=[]\n",
    "    for a in es:\n",
    "        if a==\"H\":\n",
    "            final.append(1)\n",
    "        else:\n",
    "            final.append(0)\n",
    "    Hs = torch.tensor(final)\n",
    "\n",
    "    for epoch in range(200):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x, _ = model(Hs.unsqueeze(0), coors.unsqueeze(0)) #can turn off return po\n",
    "        rep = boltzmax((nn.LeakyReLU(negative_slope=negative_slope)(x)), 15)[0] #apply activation (output raw numbers), and softmax (output nonnormalized probabilities)  \n",
    "        L = contrastive_loss(rep, ion_labels)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(L)    \n",
    "    return rep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "maxes, Ns, scs=[],[],[]\n",
    "\n",
    "\n",
    "\n",
    "#for i in range(1):\n",
    "for sample in all_ppos.keys():\n",
    "    if sample[:3] != \"ARG\":\n",
    "        coords = torch.tensor(list(all_ppos[sample].values()))\n",
    "        species = (list(all_pspecies[sample]))\n",
    "        net, optimizer = model(len(species), 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "\n",
    "\n",
    "        x = loop(10, coords, species, all_ions[sample], net, optimizer, 1) #margin, negative slope\n",
    "        out = torch.mean(x, dim=1)\n",
    "        f = out.unsqueeze(0).detach().numpy()*coords.detach().numpy().T\n",
    "        combined = np.column_stack(f).flatten()\n",
    "        \n",
    "        D=np.gradient(np.gradient(combined)).reshape(-1,3)\n",
    "        #plot(x[0], D)\n",
    "        print(sample)\n",
    "        #plt.show()\n",
    "        \n",
    "        def loop2(D, sample):\n",
    "\n",
    "            r = sample[:3]\n",
    "            #out = torch.mean(x, dim=1)\n",
    "            pnet = EGNN_Network(\n",
    "            num_tokens = 6, #vocabulary siye, number of unique species\n",
    "            num_positions = len(D),  #number of nodes         # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "            dim = 1,# #internal rep size. c has square dependence. richer rep but overfitting for small d.s.\n",
    "            depth = 2, #number of layers #deeper need more memort to store intermediate reps\n",
    "            num_nearest_neighbors = 2, #number of nearest neighbors to consider #make this the max hood size\n",
    "            dropout=.03,\n",
    "            m_pool_method='mean')\n",
    "\n",
    "\n",
    "            loss = torch.nn.HuberLoss()\n",
    "            \n",
    "            optimizer3 = torch.optim.Adam(pnet.parameters(), lr=.1, weight_decay=.1)\n",
    "            \n",
    "\n",
    "            ins = torch.ones(len(D))\n",
    "            ins[0]=0\n",
    "            ins[-1] = 2#,10,10\n",
    "\n",
    "            \n",
    "\n",
    "            for epoch in range(200):\n",
    "                optimizer3.zero_grad()\n",
    "                y=pnet(torch.tensor(ins, dtype=int).unsqueeze(0), torch.tensor(D).unsqueeze(0))[0]\n",
    "                label=labels[r]\n",
    "                loss2=loss(max(y[0][1::]), torch.tensor(label))\n",
    "                loss2.backward()\n",
    "                optimizer3.step()\n",
    "\n",
    "            a=y[0].detach().numpy()\n",
    "            maxes.append((np.where(a == max(a[1::]))[0], max(a[1::])))\n",
    "            Ns.append(a[0])\n",
    "            scs.append(a[-1])\n",
    "\n",
    "\n",
    "        return maxes, Ns, scs\n",
    "    m, n, scs = loop2(D, sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = torch.tensor(list(list(all_ppos.values())[0].values()), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdb(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        resis = {}\n",
    "        xs = []\n",
    "        counter=0\n",
    "        for line in f:\n",
    "            L = line.split()\n",
    "\n",
    "            if line.startswith(\"ATOM\"):\n",
    "                resname, atomname = L[3], L[2]\n",
    "                if resname in prot:\n",
    "                    resname = resname + str(L[5])\n",
    "\n",
    "                    if resname in resis:\n",
    "                        resis[resname][f\"{atomname} {counter}\"] = (float(L[6]), float(L[7]), float(L[8]))\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        resis[resname] = {f\"{atomname} {counter}\": (float(L[6]), float(L[7]), float(L[8]))}\n",
    "                        counter += 1\n",
    "\n",
    "                else:  # resname in noprot\n",
    "                    resname, atomname, resnum = L[3], L[2], L[5]\n",
    "                    xs.append((float(L[6]), float(L[7]), float(L[8])))\n",
    "                    counter += 1\n",
    "\n",
    "            elif line.startswith(\"HETATM\"):\n",
    "                resname = L[3] + str(L[5])\n",
    "                atomname = L[2]\n",
    "                if resname in solvents:\n",
    "                    solvents[resname][f\"{atomname} {str(counter)}\"] = (float(L[6]), float(L[7]), float(L[8]))\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    solvents[resname] = {f\"{atomname} {str(counter)}\": (float(L[6]), float(L[7]), float(L[8]))}\n",
    "                counter += 1\n",
    "    #print(resis)\n",
    "    return resis\n",
    "\n",
    "#load_pdb()\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Specify the directory and pattern\n",
    "directory_path = '/home/jrhoernschemeyer/Desktop/thesis/PDB/*red.pdb'  # Replace with your directory path\n",
    "\n",
    "# Retrieve all .txt file paths\n",
    "files = glob.glob(directory_path)\n",
    "data = [load_pdb(file) for file in files]\n",
    "pdbs = [file[-12:-8] for file in files]\n",
    "#set(list(data.keys()))\n",
    "dic = {}\n",
    "datas=[]\n",
    "for p,d in zip(pdbs, data):\n",
    "    #entry = \n",
    "    dic[p] = d \n",
    "    \n",
    "    #datas.append(list(d))\n",
    "#print(datas)\n",
    "print(dic)\n",
    "    \n",
    "\n",
    "\n",
    "def read_database(path):\n",
    "    \"\"\"csv --> dask df\"\"\"\n",
    "    #make the dask data frame from the PYPKA csv\n",
    "    dk=dd.read_csv(path, delimiter=';')\n",
    "    print(path)                                                \n",
    "    dk=dk.rename(columns={'idcode': 'PDB ID', 'residue_number': 'Res ID', 'residue_name': 'Res Name', 'residue_number': 'Res ID', 'pk': 'pKa', 'chain' : 'Chain'}) #rename columns to match df from pkad \n",
    "    #dk=dk.sort_values(['PDB ID', 'Res ID'], ascending=[True, True]) \n",
    "    #dk=dk.compute() \n",
    "    #dff = dk.reset_index() \n",
    "\n",
    "    return dk#.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "df = read_database(\"/home/jrhoernschemeyer/Desktop/thesis/pkas.csv\")\n",
    "#def (pdbs, df):\n",
    "for pdb in pdbs:\n",
    "    #df = df[df.iloc[:, 1] == pdb].drop(columns = [\"PDB ID\"])\n",
    "    #targets=torch.tensor(df[\"pKa\"])\n",
    "    print(2)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final model, predict pkas on batched split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# scikit-learn split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    full_dataset, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_tensor = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "val_tensor = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
    "\n",
    "# PyTorch DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_tensor,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_tensor,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "#val_loader.dataset.tensors\n",
    "#train_loader.dataset.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c54792aff91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#maxes, is in the backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#len(resis[\"GLU5\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "targets=[3.49973,\n",
    "1.13269,\n",
    "4.32236,\n",
    "10.366,\n",
    "11.1131,\n",
    "11.2963,\n",
    "2.78064,\n",
    "2.79104,\n",
    "10.3907,\n",
    "9.83911,\n",
    "7.78241,\n",
    "10.9913,\n",
    "9.36903,\n",
    "3.27824,\n",
    "2.62777,\n",
    "11.4944,\n",
    "10.5661,\n",
    "3.60359,\n",
    "2.22548,\n",
    "4.09207,\n",
    "10.8503,\n",
    "1.76499,\n",
    "3.03049,\n",
    "10.377,\n",
    "11.2809,\n",
    "10.1192,\n",
    "2.5044,\n",
    "0.977022,\n",
    "3.33801,\n",
    "10.0178,\n",
    "3.46841,\n",
    "3.48914,\n",
    "10.2195,\n",
    "9.58874,\n",
    "11.1147,\n",
    "2.95551,\n",
    "14.4481,\n",
    "10.9523]\n",
    "\n",
    "scindex=[]\n",
    "for id in resis.keys():\n",
    "    scindex.append(len(resis[id]))\n",
    "\n",
    "N = [np.array((0, v.item())) for v in n]\n",
    "S = [np.array((i, v.item())) for i,v in zip(scindex, scs)]\n",
    "\n",
    "    \n",
    "B = [np.concatenate((i[0], i[1])) for i in m] #maxes, is in the backbone\n",
    "#len(resis[\"GLU5\"])\n",
    "\n",
    "#np.vstack([N,S,B]).reshape(-1,2)\n",
    "\n",
    "triangles=[]\n",
    "for i in range(len(N)):\n",
    "    triangles.append(np.array([N[i], S[i], B[i]]))\n",
    "    \n",
    "\n",
    "\n",
    "for k, v in all_pspecies.items():\n",
    "    resi, val = k[0], k[3:]\n",
    "    #ens[k[0]]  len(v)\n",
    "    print(resi,val)\n",
    "\n",
    "\n",
    "\n",
    "def create_residue_graph(sequence, coordinates, distance_threshold=None):\n",
    "    \"\"\"\n",
    "    Creates a DGL graph for a single residue with 3 nodes.\n",
    "\n",
    "    Parameters:\n",
    "    - sequence (list): List of residues (single-letter codes). Should correspond to 3 nodes.\n",
    "    - coordinates (np.array or torch.Tensor): Shape (3, 3), 3D coordinates of nodes.\n",
    "    - distance_threshold (float): Distance to consider creating an edge.\n",
    "\n",
    "    Returns:\n",
    "    - DGLGraph\n",
    "    \"\"\"\n",
    "    num_nodes = 3  # Each graph has 3 nodes\n",
    "    g = dgl.graph(([], []), num_nodes=num_nodes)\n",
    "\n",
    "    # Add edges based on sequential adjacency\n",
    "    src = [0, 1, 2]\n",
    "    dst = [1, 2, 0]\n",
    "    g.add_edges(src, dst)\n",
    "    g.add_edges(dst, src)  # Make it bidirectional\n",
    "\n",
    "    # Optionally add edges based on spatial proximity\n",
    "    if distance_threshold is not None and coordinates is not None:\n",
    "        if isinstance(coordinates, torch.Tensor):\n",
    "            coordinates = coordinates.numpy()\n",
    "        \n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 1, num_nodes):\n",
    "                dist = np.linalg.norm(coordinates[i] - coordinates[j])\n",
    "                if dist <= distance_threshold:\n",
    "                    g.add_edges([i], [j])\n",
    "                    g.add_edges([j], [i])\n",
    "\n",
    "    return g\n",
    "\n",
    "graphs = []\n",
    "features = []\n",
    "all_coords = []\n",
    "\n",
    "for residue_index in range(38):  # Assuming 48 residues\n",
    "    # Example: Get coordinates for 3 nodes of the current residue\n",
    "    coords = tpos[residue_index]  # Replace with your actual data source\n",
    "    #print(coords.shape)\n",
    "    residue = sequence[residue_index]  # Residue identifier (single-letter code)\n",
    "    #print(residue)\n",
    "    # Create a graph for this residue\n",
    "    g = create_residue_graph([residue] * 3, coords, distance_threshold=5.0)\n",
    "    print(g)\n",
    "    #g = dgl.add_self_loop(g)  # Add self-loops\n",
    "    \n",
    "    graphs.append(g)\n",
    "\n",
    "    # Use coordinates as features for the 3 nodes\n",
    "    #node_features = torch.tensor(coords, dtype=torch.float32)  # Shape: (3, 3)\n",
    "    features.append(torch.tensor(triangles[residue_index],dtype=torch.float32))\n",
    "    #print(len(features[0]))\n",
    "    # Save coordinates for debugging\n",
    "    all_coords.append(coords)\n",
    "\n",
    "\n",
    "def create_protein_graph(residue_graphs, residue_positions, distance_threshold):\n",
    "    \"\"\"\n",
    "    Creates a protein-level graph with residue-level information.\n",
    "\n",
    "    Parameters:\n",
    "    - residue_graphs (list): List of 48 residue-level graphs (DGLGraphs).\n",
    "    - residue_positions (torch.Tensor): Shape (48, 3), 3D positions for each residue.\n",
    "    - distance_threshold (float): Distance threshold for creating edges.\n",
    "\n",
    "    Returns:\n",
    "    - protein_graph (DGLGraph): Protein-level graph with 48 nodes.\n",
    "    \"\"\"\n",
    "    num_residues = len(residue_graphs)\n",
    "    g = dgl.graph(([], []), num_nodes=num_residues)\n",
    "\n",
    "    # Add edges based on pairwise distances\n",
    "    for i in range(num_residues):\n",
    "        for j in range(i + 1, num_residues):\n",
    "            dist = torch.norm(residue_positions[i] - residue_positions[j])\n",
    "            if dist <= distance_threshold:\n",
    "                g.add_edges(i, j)\n",
    "                g.add_edges(j, i)  # Make it bidirectional\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "features_list = [f[-1][1] for f in features]\n",
    "#features_list\n",
    "\n",
    "poslist=[]\n",
    "tpos=[]\n",
    "for a in all_ppos.keys():\n",
    "    poslist.append(list(all_ppos[a].values()))\n",
    "\n",
    "idxs=[]  \n",
    "for p,b in zip(poslist, triangles):\n",
    "    idxs.append([y[0] for y in b])\n",
    "for i, j in zip(idxs, poslist):\n",
    "\n",
    "    idxs = ([int(k) for k in i])\n",
    "    m=idxs[-1]\n",
    "    tpos.append(np.array([j[0], j[-1], j[m]]))\n",
    "print(torch.tensor(tpos))\n",
    "\n",
    "residue_positions = [p[-1] for p in tpos]\n",
    "#residue_positions\n",
    "\n",
    "# Create the protein-level graph\n",
    "protein_graph = create_protein_graph(graphs, torch.tensor(residue_positions), distance_threshold=20.0)\n",
    "\n",
    "# Add features to the protein-level graph\n",
    "protein_features = [features.flatten() for features in features_list]  # Flatten (3, 2) -> (6,)\n",
    "protein_features = torch.stack(protein_features)  # Shape: (48, 6)\n",
    "protein_graph.ndata['features'] = protein_features\n",
    "\n",
    "class MultiresolutionModel(nn.Module):\n",
    "    def __init__(self, residue_in_feats, residue_hidden_feats, protein_hidden_feats, out_feats):\n",
    "        super(MultiresolutionModel, self).__init__()\n",
    "        \n",
    "\n",
    "        # Final output layer\n",
    "        self.fc = nn.Linear(38,38)\n",
    "\n",
    "    def forward(self, residue_graphs, residue_features, protein_graph):\n",
    "        # Step 1: Process residue-level graphs\n",
    "        residue_embeddings = []\n",
    "        for g, features in zip(residue_graphs, residue_features):\n",
    "            residue_embeddings.append(features[2][1])\n",
    "        residue_embeddings = torch.stack(residue_embeddings)  # Shape: (48, residue_hidden_feats)\n",
    "        protein_graph.ndata['features'] = residue_embeddings.unsqueeze(1)\n",
    "        \n",
    "        outputs = self.fc(residue_embeddings)  # Shape: (48, out_feats)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "model = MultiresolutionModel(2, 1, 1,38)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=.01)\n",
    "criterion = nn.HuberLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(graphs, features, protein_graph)  # Shape: (48, out_feats)\n",
    "    #print(outputs[:3])\n",
    "    #print(outputs)\n",
    "    # Compute loss (per residue)\n",
    "    print(outputs)\n",
    "    loss = criterion(outputs, torch.tensor(targets))  # Ensure residue_targets is shape (48, out_feats)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "#plt.plot(outputs.detach().numpy())\n",
    "\n",
    "#plt.plot(outputs.detach().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
