{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import *\n",
    "from Bio import PDB\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from ase import Atoms, Atom\n",
    "import dask.dataframe as dd\n",
    "from ordered_set import OrderedSet\n",
    "import os\n",
    "\n",
    "\n",
    "local_folder=\"/Users/jessihoernschemeyer/pKaSchNet\"\n",
    "pkPDB_CSV = f\"{local_folder}/pkas.csv\"\n",
    "def read_database(path):\n",
    "    \"\"\"csv --> dask df\"\"\"\n",
    "    #make the dask data frame from the PYPKA csv\n",
    "    dk=dd.read_csv(path, delimiter=';', na_filter=False, dtype={'idcode':'category', \n",
    "                                                                    'residue_number':'uint8',\n",
    "                                                                    'pk': 'float32',\n",
    "                                                                    'residue_name':'category',\n",
    "                                                                    'chain': 'category',\n",
    "                                                                    'residue_name': 'category'\n",
    "                                                                    })\n",
    "                                                            \n",
    "    dk=dk.rename(columns={'idcode': 'PDB ID', 'residue_number': 'Res ID', 'residue_name': 'Res Name', 'residue_number': 'Res ID', 'pk': 'pKa', 'chain' : 'Chain'}) #rename columns to match df from pkad \n",
    "    dk=dk.sort_values(['PDB ID', 'Res ID'], ascending=[True, True]) \n",
    "    dk=dk.compute() \n",
    "    dff = dk.reset_index() \n",
    "\n",
    "    return dff\n",
    "\n",
    "def check_atoms_protein(structure, struc_atoms): \n",
    "    \"\"\"internal function. checks every atom in the entire protein for metals, undesirables\"\"\"\n",
    "    pdb_residues=[]\n",
    "    for atom in struc_atoms: \n",
    "        resname, atomid=atom.get_parent().get_resname(), atom.get_full_id()[2:]\n",
    "        element=atomid[2][0]\n",
    "\n",
    "        if element in [\"MG\", \"MN\", \"FE\", \"CO\", \"NI\", \"CU\", \"ZN\"]:\n",
    "            return 0#,0#print(f\"{element} present, pdb skipped\")\n",
    "        \n",
    "        else:\n",
    "            #atomid=atom.get_full_id() #('', 0, 'B', (' ', 177, ' '), ('OH', ' '))\n",
    "            if atomid[1][0] not in [' ']:\n",
    "                if element == 'S': #check 4 hetero sulfur, exclude.\n",
    "                    print(f\"{atomid}, hetero sulfur. pdb skipped \")\n",
    "                    return 0#,0\n",
    "                \n",
    "                if element in ['CA', 'CL', 'K', 'NA']: #other salt\n",
    "                    for res in structure.get_residues():\n",
    "                        if resname in [\"GLU\", \"HIS\", \"ASP\", \"ARG\", \"TYR\", \"CYS\", \"LYS\"]: #if the other salt is part of the residue (<3Ã¥ from geometric center), delete atom from residue\n",
    "                            if np.linalg.norm(res.center_of_mass(geometric=True) - atom.get_coord()) < 3:\n",
    "                                atom.get_parent().detach_child(atom.get_id()) #print(f\"salt {atom} deleted, {d} from {res}\")\n",
    "    \n",
    "    return structure#, set(pdb_residues) #('', 0, 'B', ('W', 371, ' '), ('O', ' '))\n",
    "\n",
    "def atoms_to_structure(cutout, filename): \n",
    "    \"\"\"Internal function (or not), cutout --> save to harddrive\n",
    "    input: cutout: list of biopython atom objects (NOT ASE)\"\"\"\n",
    "    chain_dict = {}\n",
    "\n",
    "    structure = Structure.Structure(filename)\n",
    "    model = Model.Model(0)\n",
    "    structure.add(model)\n",
    "\n",
    "    for atom in cutout:\n",
    "        res = atom.get_parent()\n",
    "        res_id, resname, chain_id = res.get_id(), res.get_resname(), res.get_full_id()[2]\n",
    "\n",
    "        #make acidic GLH and ASH straight here. so change their name before saving \n",
    "        if resname == \"GLU\":\n",
    "            resname=\"GLH\"\n",
    "            \n",
    "        if resname==\"ASP\":\n",
    "            resname=\"ASH\"\n",
    "\n",
    "        if resname==\"HIS\":\n",
    "            resname=\"HIP\"\n",
    "            \n",
    "        \n",
    "        if chain_id not in chain_dict:\n",
    "            chain = Chain.Chain(chain_id) #make new chain\n",
    "            chain_dict[chain_id] = chain\n",
    "            model.add(chain) #add it\n",
    "\n",
    "        else:\n",
    "            chain = chain_dict[chain_id]\n",
    "\n",
    "        if res_id in [res.get_id() for res in chain.get_residues()]:\n",
    "            residue = [res for res in chain.get_residues() if res.get_id() == res_id][0] \n",
    "        else:\n",
    "            residue = Residue.Residue(res_id, resname, '') #make new res\n",
    "            chain.add(residue)\n",
    "\n",
    "        residue.add(atom)\n",
    "    # save the pdb\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    io.save(f\"cuts/{filename}.pdb\")\n",
    "\n",
    "#dask_df = read_database(local_folder + pkPDB_CSV)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:(\n",
    "    make sure changing index in merge or not is correct, as well as the modified fnames.\n",
    "\n",
    "recut, deprotonate\n",
    "\n",
    "to do: chek about those stupid indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdb_parser, pdbs = PDB.PDBParser(), list(OrderedSet(list(dask_df[\"PDB ID\"])))\n",
    "pdb_parser = PDB.PDBParser()\n",
    "def generate_cutout_around_protonatable_site(residue, distance_cutoff, ns, counter, resname):\n",
    "    \"\"\"Residue wise resolurion. ns is neighbor search set up for the entire protein, residue is the single data point / 1 of several residues in a pdb & in pypka.\n",
    "    input is one residue. output is the cutout around its titratable site, both of which can be plural e.g. his, mb asp and glu.\n",
    "    residue (biopython Residue object): a single protonable residue \"\"\"\n",
    "    protonatable_sites = {\"G\":(\"OE1\",\"OE2\"), \"A\":(\"OD1\",\"OD2\"), \"C\":\"SG\", \"L\":\"NZ\", \"H\":(\"NE2\", \"ND1\"), \"T\":\"OH\"}\n",
    "    cuts = []\n",
    "    if resname==0:\n",
    "        #first atom is N and NTR\n",
    "        #atoms=residue.\n",
    "        center = residue['N'].get_coord()\n",
    "        cut = ns.search(center, distance_cutoff, \"A\")\n",
    "        cuts.append((counter, center, 'NTR', cut)) #counter is id!\n",
    "        return cuts\n",
    "    \n",
    "    elif resname==1: #CTR\n",
    "        try:\n",
    "            center = residue['OXT'].get_coord()\n",
    "            cut = ns.search(center, distance_cutoff, \"A\")\n",
    "            cuts.append((counter, center, 'OX', cut))\n",
    "        except:\n",
    "            center = residue['C'].get_coord()\n",
    "        cut = ns.search(center, distance_cutoff, \"A\")\n",
    "        cuts.append((counter, center, 'X', cut)) #counter is id!\n",
    "        return cuts\n",
    " \n",
    "    else:\n",
    "        if resname==\"G\": \n",
    "            sites=protonatable_sites[resname]\n",
    "            atom1,atom2=residue[sites[0]],residue[sites[1]]\n",
    "            if atom1.is_disordered(): \n",
    "                center, resname = atom1.get_coord(), resname + \"D\"\n",
    "            elif atom2.is_disordered():\n",
    "                center, resname = atom2.get_coord(), resname + \"D\"\n",
    "            else:\n",
    "                center=(atom1.get_coord() + atom2.get_coord()) / 2.0\n",
    "            cut = ns.search(center, distance_cutoff, \"A\") #put ns search i n below? todo\n",
    "            cuts.append((counter, center, resname, cut)) #counter is id!\n",
    "            return cuts\n",
    "        if resname==\"A\": \n",
    "            sites=protonatable_sites[resname]\n",
    "            atom1,atom2=residue[sites[0]],residue[sites[1]]\n",
    "            if atom1.is_disordered(): \n",
    "                center, resname = atom1.get_coord(), resname + \"D\"\n",
    "                print(1)\n",
    "            elif atom2.is_disordered():\n",
    "                center, resname = atom2.get_coord(), resname + \"D\"\n",
    "                print(2)\n",
    "            else:\n",
    "                center=(atom1.get_coord() + atom2.get_coord()) / 2.0\n",
    "                #print(3)\n",
    "            \n",
    "            cut = ns.search(center, distance_cutoff, \"A\")\n",
    "            #print(\"cut\", cut)\n",
    "            cuts.append((counter, center, resname, cut)) #counter is id!\n",
    "            return cuts\n",
    "        if resname==\"C\": \n",
    "            site=residue[protonatable_sites[resname]]\n",
    "            if site.is_disordered(): \n",
    "                resname = resname + \"D\"\n",
    "            center =site.get_coord()\n",
    "            cut = ns.search(center, distance_cutoff, \"A\")\n",
    "            cuts.append((counter, center, resname, cut)) #counter is id!\n",
    "            return cuts\n",
    "        if resname==\"L\": \n",
    "            site=residue[protonatable_sites[resname]]\n",
    "            if site.is_disordered(): \n",
    "                resname = resname + \"D\"\n",
    "            center =site.get_coord()\n",
    "            cut = ns.search(center, distance_cutoff, \"A\")\n",
    "            cuts.append((counter, center, resname, cut)) #counter is id!\n",
    "            return cuts\n",
    "        if resname==\"T\":\n",
    "            site=residue[protonatable_sites[resname]]\n",
    "            if site.is_disordered(): \n",
    "                resname = resname + \"D\"\n",
    "            center =site.get_coord()\n",
    "            cut = ns.search(center, distance_cutoff, \"A\")\n",
    "            cuts.append((counter, center, resname, cut)) #counter is id!\n",
    "            return cuts\n",
    "              \n",
    "        if resname==\"H\":\n",
    "            sites=protonatable_sites[resname]\n",
    "            atom1,atom2=residue[sites[0]],residue[sites[1]]            \n",
    "            if atom1.is_disordered(): \n",
    "                resname = resname + \"D\"\n",
    "            if atom2.is_disordered():\n",
    "                resname = resname + \"D\"\n",
    "            center1,center2=atom1.get_coord(), atom2.get_coord()\n",
    "            cut1 = ns.search(center1, distance_cutoff, \"A\")\n",
    "            cut2= ns.search(center2, distance_cutoff, \"A\")\n",
    "            cuts.append([(counter+.1, center1, resname, cut1),(counter+.2, center2, resname, cut2)])\n",
    "            #cuts.append((counter+.2, center2, resname, cut2))\n",
    "\n",
    "\n",
    "    return cuts #plural because of sites with multiple sites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "def merge_or_not_cutouts(cutouts_apdb, distance_cutoff): #TODO: reduce dtypes #PDB WISE!\n",
    "    \"\"\"\n",
    "    in: all of the cutouts from the pdb. returns the merged or solo cutout for each input residue of cutouts_apdb. len in = len out\"\"\"\n",
    "    #protein wise ..\n",
    "    dp_ids,centers,cuts, Ds_lite, cutouts, resnames,redunant_merged_is, done_pairs =[],[],[],[], [],[],[],[]\n",
    "\n",
    "    for site in cutouts_apdb:\n",
    "        if type(site)==tuple:\n",
    "            #print(\"a site\", site[0])\n",
    "            dp_ids.append(site[0]) #1,2,3f,4,5,6,7,8,9.1, 9.2....\n",
    "            centers.append(site[1]) \n",
    "            resnames.append(site[2])\n",
    "            cuts.append(site[3])\n",
    "        else:\n",
    "            print(\"a site\", site[0][0], site[1][0])\n",
    "            #site1,site2=site[0],site[1]\n",
    "            dp_ids.append(site[0][0]) #1,2,3,4,5,6,7,8,9.1, 9.2....\n",
    "            dp_ids.append(site[1][0])\n",
    "\n",
    "            centers.append(site[0][1]) \n",
    "            centers.append(site[1][1]) \n",
    "\n",
    "            resnames.append(site[0][2])\n",
    "            resnames.append(site[1][2]) \n",
    "\n",
    "            cuts.append(site[0][3])\n",
    "            cuts.append(site[1][3])\n",
    "    print(resnames)\n",
    "    num_residues=len(centers)\n",
    "    distances = np.zeros((num_residues, num_residues))\n",
    "    \n",
    "    for i in range(num_residues):\n",
    "        for j in range(i + 1, num_residues):\n",
    "            distance = np.linalg.norm(centers[i] - centers[j]).astype(np.float32)\n",
    "\n",
    "            if distance < distance_cutoff:\n",
    "                distances[i, j] = distance.astype(np.float32)\n",
    "                distances[j, i] = distance.astype(np.float32)\n",
    "\n",
    "    #Ds lite correctly gets the nonzero entries from column? row? i of distances.\n",
    "    Ds_lite = [distances[i][distances[i] != 0] for i in range(num_residues)] #nonzero entries for easier searching\n",
    "    #print(len(Ds_lite))\n",
    "#residuewise...\n",
    "    for i in range(len(Ds_lite)): #=len IDs \n",
    "        a_residues_distance_array=Ds_lite[i]\n",
    "        #print(f\"residue {i}'s distance array, Ds_lite[i]\", a_residues_distance_array)\n",
    "        if a_residues_distance_array.any(): #if not empty\n",
    "            index=i\n",
    "            #tolerance = 1e-1  # Define an appropriate tolerance level\n",
    "            #print((np.abs(distances[:, int(index)] - np.min(a_residues_distance_array)) < tolerance)[0])\n",
    "            #closest_cutout_i = int(np.where(np.abs(distances[:, int(index)] - np.min(a_residues_distance_array)) < tolerance)[0])\n",
    "            #print(\"where does the row of the residue's distances, equal the min of its D's lite?\",distances[:,int(index)] == np.min(a_residues_distance_array))\n",
    "            #print(\"distances column\", distances[:,int(index)])\n",
    "            closest_cutout_i = int(np.where((distances[int(index), :])==np.min(a_residues_distance_array))[0]) #int is unneccessary TODO\n",
    "            \n",
    "            print(resnames[index])\n",
    "            print(resnames[closest_cutout_i])\n",
    "            print(closest_cutout_i)\n",
    "            print(\"\")\n",
    "            #print(\"closest cutout i\", closest_cutout_i)\n",
    "            #print(\"min of d array\", np.min(a_residues_distance_array))\n",
    "            pair_i = frozenset((index,dp_ids[closest_cutout_i]))#key #frozen set is immutable thus can be used as a dict key #also order doesnt matter, 2-1=1-2\n",
    "\n",
    "            if not done_pairs: #if there are any yet merged\n",
    "                cutout = (list(set(cuts[i] + cuts[closest_cutout_i])),resnames[i] + resnames[closest_cutout_i], (centers[i], centers[closest_cutout_i]))\n",
    "                done_pairs.append(pair_i)\n",
    "                redunant_merged_is.append(closest_cutout_i)\n",
    "                \n",
    "            else: #if there are already some generated\n",
    "                if pair_i not in done_pairs: #if that mergedcut hasnt yet been made\n",
    "                    cutout = (list(set(cuts[i] + cuts[closest_cutout_i])),resnames[i] + resnames[closest_cutout_i], (centers[i], centers[closest_cutout_i]))\n",
    "                    done_pairs.append(pair_i)\n",
    "                    redunant_merged_is.append(closest_cutout_i)\n",
    "\n",
    "                else: #null\n",
    "                    cutout = None\n",
    "\n",
    "        else: #solo cutout\n",
    "            cutout = (cuts[i], centers[i])\n",
    "\n",
    "        cutouts.append(cutout)\n",
    "\n",
    "    if len(cutouts) != len(dp_ids): #delete?\n",
    "        return #this will make an exception if something went wrong\n",
    "    \n",
    "    return cutouts,redunant_merged_is #merged or solo\n",
    "\n",
    "def get_cutout(dask_df, distance_cutoff): #\"PARENT\" FUNCTION\n",
    "    \"\"\"for each protein in dask_df (the entire PYPKA database), it iterates residue wise through the 121,294 proteins in PYPKA database and downloads\n",
    "    the structure from RCSB with biopython. Then, it checks and skips the structure if metals & hetero sulfurs are present, and deletes non-sulfur\n",
    "    salts from titratable residues.\n",
    "    Then, for each structure residue represented in PYPKA, generates a cutout for each residue, appends the structure to cutouts_apdb\"\"\"\n",
    "    #pdbname=\"11as\"  #for now #delete\n",
    "    all_fnames, all_cuts, all_centers = [],[],[]\n",
    "    for i in range(19,20): #will equal len of set of pdbs in pypka, == 121294 \n",
    "        cutouts_apdb, fnames, cutouts_1_datapoint, counter, pdbname, newfnames, centers_apdb = [],[], [],0, pdbs[i],[],[]\n",
    "        Structure = pdb_parser.get_structure(\"\",  PDBList().retrieve_pdb_file(str.lower(pdbname),obsolete=False, pdir='PDB',file_format = 'pdb'))\n",
    "        structure= check_atoms_protein(Structure, Structure.get_atoms())\n",
    "        if not structure: #skip entire pdb and all its entries in pypka db if there are undesirables in pdb\n",
    "            continue\n",
    "            \n",
    "        ns = PDB.NeighborSearch(list(structure.get_atoms())) #set up ns , entire protein\n",
    "        pdb_df = dask_df[dask_df.iloc[:, 1] == pdbname].drop(columns = [\"PDB ID\", \"pKa\"]) #make a subdf containing only residue entries which are in PYPKA (dask_df) \n",
    "        for j in range(len(pdb_df)):  #go through each residue in a pdb #each j is a datapoint!\n",
    "            chain, res_id =pdb_df.iloc[j]['Chain'], int(pdb_df.iloc[j]['Res ID'])\n",
    "            try: \n",
    "                residue=structure[0][chain][res_id] #a datapoint #TODO: make ID?\n",
    "                pypka_resname, PDBresname = pdb_df.iloc[j]['Res Name'], residue.get_resname() #pypka error\n",
    "                if pypka_resname=='NTR':\n",
    "                    resname=0\n",
    "                elif pypka_resname=='CTR':\n",
    "                    resname,pypka_resname=1,\"X\" #carboxyl\n",
    "                elif pypka_resname==PDBresname:\n",
    "                    resname=pypka_resname[0]\n",
    "                #pypka error: if not in ntr or == pdbresname, it will error and pass.\n",
    "                    #elif pypka_resname == PDBresname: #ACHTUNG! this navigates the pyka error. #TODO: mail him #TODO: THIS EXCLUDES NTR AND CTR!\n",
    "\n",
    "                cutouts_1_datapoint=generate_cutout_around_protonatable_site(residue, distance_cutoff, ns, counter, resname) #can be multiple #returns empty if disordered\n",
    "                if cutouts_1_datapoint: #cutouts_1_datapoint DNE if titratable site is disordered\n",
    "                    cutouts_apdb.append(*cutouts_1_datapoint) #append each residue/data point cutouts here #it will error here if disordered\n",
    "                    #for _ in cutouts_1_datapoint:\n",
    "\n",
    "                    #fnames.append(f\"{pdbname}{chain}{res_id}_{pypka_resname}{counter}\") \n",
    "                    if resname!=\"H\": #TODO: check if its quicker to do \"for cuts in cutouts a pdb\" or if resname==H\n",
    "                        #print(resname, 22)\n",
    "                        fnames.append(f\"{pdbname}{chain}{res_id}_{pypka_resname}{counter}\") \n",
    "                    else:\n",
    "                        print(resname, 22)\n",
    "                        fnames.append(f\"{pdbname}{chain}{res_id}_{pypka_resname}{counter + .1}\") \n",
    "                        fnames.append(f\"{pdbname}{chain}{res_id}_{pypka_resname}{counter + .2}\") \n",
    "                        #print(resname, cutouts_1_datapoint, counter)\n",
    "                    counter+=1\n",
    "                else:\n",
    "                    continue #pypka error\n",
    "\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Exception caught: {e}\")\n",
    "                raise  # \n",
    "                #pass #means pypka res not found in PDB\n",
    "        \n",
    "        #os.remove(f\"{local_folder}/PDB/pdb{pdbname}.ent\")  \n",
    "        if cutouts_apdb:\n",
    "            merged_and_solos, greaterN_pair_i =merge_or_not_cutouts(cutouts_apdb, distance_cutoff)#make a merged cutout or not based off radius criteria\n",
    "            print(fnames)\n",
    "            for cut, fname in zip(merged_and_solos, fnames):\n",
    "                if not cut:\n",
    "                    continue\n",
    "                \n",
    "                elif len(cut)==3: #means it is a merged cutout. second argument is the pairid #it is still 1-to-1 here but ima destroy it\n",
    "                    #print(greaterN_pair_i[0])\n",
    "                    #print(fname, fnames[greaterN_pair_i[0]],cut[1])\n",
    "                    #print(cut[1])\n",
    "                    Fname=\"\".join([fname,'_',fnames[greaterN_pair_i[0]],\"_\",cut[1]]) #cut1 is pairid AT, HH,...\n",
    "                    newfnames.append(Fname)\n",
    "                    centers_apdb.append(cut[2])\n",
    "                    del greaterN_pair_i[0]\n",
    "                    atoms_to_structure(cut[0], Fname) #save as pdb) #cut \n",
    "\n",
    "                else:\n",
    "                    newfnames.append(fname)\n",
    "                    centers_apdb.append(cut[1])\n",
    "                    #print(2,cut)\n",
    "                    atoms_to_structure(cut[0], fname) \n",
    "        all_fnames.append(newfnames)\n",
    "        all_cuts.append(cutouts_apdb)\n",
    "        all_centers.append(centers_apdb)\n",
    "\n",
    "    return all_fnames, all_cuts, all_centers #[c[1] for c in cutouts_apdb] #centers\n",
    "\n",
    "\n",
    "#fs, all_cuts, all_centers = get_cutout(dask_df, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo now: find the H's fo CTR and NTR >.<\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure that TIP3p and ff14sb in same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "#protonate(\"194l\", )\n",
    "def amber(input_pdb):\n",
    "    skript = f\"\"\"source leaprc.protein.ff14SB\n",
    "    source leaprc.water.tip3p\n",
    "    loadOff \"/Users/jessihoernschemeyer/miniconda3/envs/cfcnn/dat/leap/lib/amino19.lib\"\n",
    "    mol = loadpdb \"/Users/jessihoernschemeyer/pKaSchNet/cuts/{input_pdb}.pdb\"\n",
    "    savepdb mol \"/Users/jessihoernschemeyer/pKaSchNet/prot/{input_pdb}.pdb\"\n",
    "\n",
    "    quit\"\"\"\n",
    "    with open(\"ascript.py\",\"w\") as file: \n",
    "        file.writelines(skript)\n",
    "    return\n",
    "\n",
    "\n",
    "for f in fs[0]:\n",
    "    amber(f)\n",
    "    !tleap -s -f /Users/jessihoernschemeyer/pKaSchNet/ascript.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat '/Users/jessihoernschemeyer/pKaSchNet/prot/199lA70_ASP22_199lA31_HIS11.2_AH.pdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protonatable_sites = {\"G\":\"HE2\", \n",
    "                      \"C\":\"HG\", \n",
    "                      \"L\":\"HZ1\", \n",
    "                      \"A\":\"HD2\",\n",
    "                      \"H\": (\"HD1\",\"HE2\"), #this needs to be fixed \n",
    "                      \"T\": \"HH\"}\n",
    "\n",
    "\n",
    "#for fnames_apdb in all_fnames:\n",
    "    #for fname in fnames_apdb:\n",
    "        #recut(fname)\n",
    "def recut(fnames_apdb, centers_apdb, distance_cutoff): #the centers come in #fnames after protonation\n",
    "        \n",
    "    for fname, center in zip(fnames_apdb, centers_apdb):\n",
    "        struct = pdb_parser.get_structure(\"\",  f'/Users/jessihoernschemeyer/pKaSchNet/prot/{fname}.pdb')\n",
    "        ns = PDB.NeighborSearch(list(struct.get_atoms())) #set up ns , entire protein\n",
    "        if type(center)==tuple: #merged\n",
    "            \n",
    "            cut = set(ns.search(center[0], distance_cutoff, \"A\")) | set(ns.search(center[1], distance_cutoff, \"A\"))\n",
    "            deprotonate_merged(cut,fname)\n",
    "            fname.split(\"_\")[4]\n",
    "            cuts.append((fname.split(\"_\")[4],cut))\n",
    "        else: #single\n",
    "            cut = ns.search(center, distance_cutoff, \"A\")\n",
    "            deprotonate_singles(cut, fname)\n",
    "        \n",
    "        \n",
    "        cuts.append((fname.split(\"_\")[1], cut))\n",
    "\n",
    "    return cuts\n",
    "    \n",
    "        \n",
    "#for fname,centers in zip(fs,all_centers):\n",
    "    \n",
    "    #recut(fname, centers,5)\n",
    "\n",
    "#def deprotonate(cut):\n",
    "     \n",
    "    #Hatom\n",
    "\n",
    "\n",
    "protonatable_sites2 = {\"G\":\"GLH\", \n",
    "                      \"C\":\"CYS\", \n",
    "                      \"L\":\"LYS\", \n",
    "                      \"A\":\"ASH\",\n",
    "                      \"H\": \"HIP\",\n",
    "                      \"T\": \"TYR\"}\n",
    "\n",
    "cut = pdb_parser.get_structure(\"\",  local_folder + '/prot/199lA11_GLU3.pdb')\n",
    "\n",
    "    \n",
    "def deprotonate_singles(cut, f): #turn one acidic into all its others?\n",
    "    res = f.split(\"_\")[1][0]\n",
    "    #print(len([atom for atom in cut.get_atoms()]))\n",
    "    atom_to_delete = protonatable_sites[res]\n",
    "    found_atoms = [atom for atom in cut.get_atoms() if atom.get_name() == atom_to_delete and atom.get_parent().get_resname()[0] == res] #to delete\n",
    "    #dads = str(atom.get_parent()\n",
    "    #found_atoms_res = [(atom.get_parent().get_resname())[0] for atom in found_atoms_all]# if ] # if atom.get_parent().get_id() == 'GLH']\n",
    "    for atom in found_atoms:\n",
    "        atom.get_parent().detach_child(atom_to_delete)\n",
    "    #print(len([atom for atom in cut.get_atoms()]))\n",
    "    #atoms_to_structure(cut, 'test')\n",
    "deprotonate_singles(cut, fs[0])\n",
    "\n",
    "\n",
    "\n",
    "def deprotonate_merged(cut,fname):\n",
    "    key=fname.split(\"_\")[4] #AT...\n",
    "    keys=(key[0],key[1])\n",
    "    found_atoms = [atom for atom in cut if atom.get_name() in [protonatable_sites[keys[0]],protonatable_sites[keys[1]]]] #gets all the atoms for both\n",
    "    for atom in found_atoms:\n",
    "        res=atom.get_parent().get_resname()\n",
    "        print(2,protonatable_sites2[res[0]])\n",
    "\n",
    "\n",
    "#cuts=recut(fs[0][1:2],all_centers[0][1:2], 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2145313325.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[74], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    if protonatable_sites2[key2] == atom.get_parent().get_resname():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "protonatable_sites2 = {\"G\":\"GLH\", \n",
    "                      \"C\":\"CYS\", \n",
    "                      \"L\":\"LYS\", \n",
    "                      \"A\":\"ASH\",\n",
    "                      \"H\": \"HIP\",\n",
    "                      \"T\": \"TYR\"}\n",
    "\n",
    "protonatable_sites = {\"G\":\"HE2\", \n",
    "                      \"C\":\"HG\", \n",
    "                      \"L\":\"HZ1\", \n",
    "                      \"A\":\"HD2\",\n",
    "                      \"H\": (\"HD1\",\"HE2\"), #this needs to be fixed \n",
    "                      \"T\": \"HH\"}\n",
    "\n",
    "\n",
    "cut = pdb_parser.get_structure(\"\",  '/Users/jessihoernschemeyer/pKaSchNet/prot/199lA10_ASP2_199lA161_TYR37_AT.pdb')\n",
    "#residue1\n",
    "key='/Users/jessihoernschemeyer/pKaSchNet/prot/199lA10_ASP2_199lA161_TYR37_AT.pdb'.split(\"_\")[4] #AT...\n",
    "key1,key2=key[0],key[1] #A,T\n",
    "res1,res2=protonatable_sites2[key1], protonatable_sites2[key2] #ASH, TYR\n",
    "for atom in cut.get_atoms():\n",
    "    #for 2 option residues, not HIS\n",
    "    if protonatable_sites[key1] == atom.get_name(): #if hd2 == atom name\n",
    "        if protonatable_sites2[key1] == atom.get_parent().get_resname():\n",
    "            #detach atom\n",
    "            #get the new cut with first removed\n",
    "            #save\n",
    "            \n",
    "        if protonatable_sites2[key2] == atom.get_parent().get_resname():\n",
    "    \n",
    "    #res2\n",
    "\n",
    "    \n",
    "    #if atom.get_name() in protonatable_sites\n",
    "#found_atoms = [atom for atom in cut.get_atoms() if atom.get_name() in [protonatable_sites[keys[0]],protonatable_sites[keys[1]]] and atom.get_parent().get_resname() in (protonatable_sites2[keys[0]], protonatable_sites2[keys[1]])]  #gets all the atoms for both\n",
    "#for atom in found_atoms:\n",
    "    #res=atom.get_parent().get_resname()\n",
    "    #print(atom, atom.get_parent())\n",
    "#found_atoms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detach HH HH atom2 TYR\n",
      "detach HD2 HD2 atom1 ASH\n",
      "detach  HD2 HH from im detached cut of resA\n"
     ]
    }
   ],
   "source": [
    "key='/Users/jessihoernschemeyer/pKaSchNet/prot/199lA10_ASP2_199lA161_TYR37_AT.pdb'.split(\"_\")[4] #AT...\n",
    "found_atoms=[]\n",
    "key1,key2=key[0],key[1] #A,T\n",
    "res1,res2, atom1, atom2=protonatable_sites2[key1], protonatable_sites2[key2], protonatable_sites[key1], protonatable_sites[key2]  #ASH, TYR \n",
    "if key1=='H':\n",
    "    if key2=='H':\n",
    "        print(\"double H\",key1,key2)\n",
    "    else:\n",
    "        print(f\"{res1} is his. {res2} is else\")\n",
    "\n",
    "elif key2=='H':\n",
    "    if key1=='H':\n",
    "        print(\"double H\",key1,key2)\n",
    "    else:\n",
    "        print(f\"{res2} is his. {res1} is else\")\n",
    "    \n",
    "\n",
    "\n",
    "else: #not his\n",
    "    for atom in cut.get_atoms():\n",
    "        #for 2 option residues, not HIS\n",
    "        if atom.get_name() in (atom1, atom2): #if hd2 == atom name\n",
    "            \n",
    "            parent_res = atom.get_parent()\n",
    "            parent_res_name = parent_res.get_resname()\n",
    "            found_atoms.append((parent_res, atom))\n",
    "\n",
    "    for ion in found_atoms:\n",
    "            ion_resname, ion_atomname = ion[0].get_resname(), ion[1].get_name()\n",
    "            if ion_resname==res1:\n",
    "                if ion_atomname==atom1: #if both are true!!\n",
    "                    print(f\"detach {ion_atomname} {atom1} atom1 {res1}\")\n",
    "                    cutt=\"im detached cut of resA\"\n",
    "                    #detach atom1\n",
    "                    #get the new cut with first removed cut_resA_d = \n",
    "                    #save\n",
    "                    atoms_to_iterate_thru = tuple(t[1].get_name() for t in found_atoms)\n",
    "                    if atom2 in atoms_to_iterate_thru:\n",
    "                        print(f\"detach  {ion_atomname} {atom2} from {cutt}\")\n",
    "                        #save double deprotonated\n",
    "            elif ion_resname==res2:\n",
    "                if ion_atomname==atom2:\n",
    "                    print(f\"detach {ion_atomname} {atom2} atom2 {res2}\")\n",
    "                    #save resB_d. name in file   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=['199lA11_GLU3', '199lA10_ASP2_199lA161_TYR37_AT', '199lA70_ASP22_199lA31_HIS11.2_AH']\n",
    "all_centers = [np.array([46.052, 26.737, 23.817]), (np.array([40.873,  9.228,  6.324]), np.array([40.835,  7.343,  4.385])), (np.array([40.873,  9.228,  6.324]), np.array([40.835,  7.343,  4.385]))]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functioniert !!!!! but i just used a merged cutout to engineer the deprotonate single for his, oops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = pdb_parser.get_structure(\"\",  '/Users/jessihoernschemeyer/pKaSchNet/prot/199lA70_ASP22_199lA31_HIS11.2_AH.pdb')\n",
    "cut2 = cut.copy()\n",
    "print(len([atom for atom in cut.get_atoms()]))\n",
    "print(len([atom for atom in cut2.get_atoms()]))\n",
    "hisatoms = protonatable_sites[\"H\"]\n",
    "found_atoms = [atom for atom in cut.get_atoms() if atom.get_name() in hisatoms and atom.get_parent().get_resname()[0] == 'H']\n",
    "residue=found_atoms[0].get_parent()\n",
    "residue.detach_child(hisatoms[0])\n",
    "print(len([atom for atom in cut.get_atoms()]))\n",
    "#save --> HIE\n",
    "residue.detach_child(hisatoms[1])\n",
    "print(len([atom for atom in cut.get_atoms()]))\n",
    "#save --> HIS\n",
    "found_atoms = [atom for atom in cut2.get_atoms() if atom.get_name() == hisatoms[1] and atom.get_parent().get_resname()[0] == 'H'][0]\n",
    "#print(len([atom for atom in cut2.get_atoms()]))\n",
    "residue=found_atoms.get_parent()\n",
    "\n",
    "residue.detach_child(hisatoms[1])\n",
    "#Save --> HID\n",
    "\n",
    "print(len([atom for atom in cut2.get_atoms()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cut = pdb_parser.get_structure(\"\",  local_folder + '/prot/199lA11_GLU3.pdb')\n",
    "\n",
    "def deprotonate_singles(cut, f): #turn one acidic into all its others?\n",
    "    res = f.split(\"_\")[1][0]\n",
    "    print(len([atom for atom in cut.get_atoms()]))\n",
    "    found_atoms = [atom for atom in cut.get_atoms() if atom.get_name() == protonatable_sites[res] and atom.get_parent().get_resname()[0] == res] #to delete\n",
    "    #dads = str(atom.get_parent()\n",
    "    #found_atoms_res = [(atom.get_parent().get_resname())[0] for atom in found_atoms_all]# if ] # if atom.get_parent().get_id() == 'GLH']\n",
    "    for atom in found_atoms:\n",
    "        atom.get_parent().detach_child('HE2')\n",
    "    #print(len([atom for atom in cut.get_atoms()]))\n",
    "    if res == 'H':\n",
    "        hisatoms = protonatable_sites[res]\n",
    "        found_atoms = [atom for atom in cut.get_atoms() if atom.get_name() in hisatoms and atom.get_parent().get_resname()[0] == res]\n",
    "        \n",
    "    return found_atoms\n",
    "atoms = deprotonate_singles(cut, fs[2])\n",
    "#a=found_atoms[0]\n",
    "#a.get_parent().detach_child(a.get_id())\n",
    "#for a in atoms:\n",
    "    #print(a.get_parent())\n",
    "atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 'prot/199lA162_LYS39_199lA159_ASP36_LA.pdb'\n",
    "\n",
    "protonatable_sites2 = {\"G\":\"GLH\", \n",
    "                      \"C\":\"CYS\", \n",
    "                      \"L\":\"LYS\", \n",
    "                      \"A\":\"ASH\",\n",
    "                      \"H\": \"HIP\",\n",
    "                      \"T\": \"TYR\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we dont take it from the hydrogen atoms for constantness, since those Hs arent in all, dont wanna bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biopython search time: 0.0002980232238769531 seconds\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual PDB file path\n",
    "import time\n",
    "pdb_file = f'{local_folder}/prot/097_1a0f_GLU_B_190+99_1a0f_ASP_B_193~GA.pdb.pdb'\n",
    "\n",
    "# Parse the structure\n",
    "parser = PDBParser()\n",
    "structure = parser.get_structure(\"example\", pdb_file)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Search for atom by name\n",
    "found_atoms = [atom for atom in structure.get_atoms() if atom.get_name() == \"OE1\"]\n",
    "a=found_atoms[0]\n",
    "a.get_parent().detach_child(a.get_id())\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Biopython search time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m recut(fs[\u001b[39m0\u001b[39;49m],all_centers[\u001b[39m0\u001b[39;49m], \u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[81], line 19\u001b[0m, in \u001b[0;36mrecut\u001b[0;34m(fnames_apdb, centers_apdb, distance_cutoff)\u001b[0m\n\u001b[1;32m     17\u001b[0m     cut1 \u001b[39m=\u001b[39m ns\u001b[39m.\u001b[39msearch(center[\u001b[39m0\u001b[39m], distance_cutoff, \u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     cut2 \u001b[39m=\u001b[39m ns\u001b[39m.\u001b[39msearch(center[\u001b[39m1\u001b[39m], distance_cutoff, \u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     merged \u001b[39m=\u001b[39m {cut1,cut2}\n\u001b[1;32m     20\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     cut2 \u001b[39m=\u001b[39m ns\u001b[39m.\u001b[39msearch(center, distance_cutoff, \u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "recut(fs[0],all_centers[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname=\"1a0fB190_GLU97_1a0fB193_ASP99_GA\"\n",
    "key = fname.split(\"_\")[4]\n",
    "res1,res2=key[0],key[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do move everything to same directory\n",
    "\n",
    "can delete from cutouts as we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acidic\n",
    "protonatable_sites = [\"HE2\", \"HG\", \"HZ1\", \"OD2\", \"HE2\", \"HD1\", \"HH\"] # glu cys lys asp hie hid tyr\n",
    "protonatable_sites = {\"G\":\"HE2\", \"C\":\"HG\", \"L\":\"HZ1\", \"A\":\"HD2\",\"H1\": \"HD1\", \"H2\":\"HE2\", \"T\": \"HH\"}\n",
    "in=\"194l_A_7_GLU-0_0.pdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed '/HH/d' /Users/jessihoernschemeyer/pKaSchNet/194l_A_53_TYR-9.pdb #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Structure id=protein>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessihoernschemeyer/miniconda3/envs/cfcnn/lib/python3.9/site-packages/Bio/PDB/PDBParser.py:388: PDBConstructionWarning: Ignoring unrecognized record '\\outl0' at line 10\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Bio.PDB import PDBParser\n",
    "\n",
    "# Create a PDB parser object\n",
    "parser = PDBParser()\n",
    "\n",
    "# Load the PDB file with a non-standard extension\n",
    "structure = parser.get_structure('protein', '/Users/jessihoernschemeyer/pKaSchNet/cuts/og.rtf.HIE')\n",
    "\n",
    "# Now you can work with the 'structure' object as usual\n",
    "print(structure)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!sed '/delete_this/d' file > newfile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if a solo cutout we can use sed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import *\n",
    "from Bio import PDB\n",
    "from ase import Atoms, Atom\n",
    "import torch\n",
    "from matscipy.neighbours import neighbour_list as msp_neighbor_list\n",
    "pdb_parser = PDB.PDBParser()\n",
    "def PDB_to_schnet_input_and_names_map(cut,r):\n",
    "\n",
    "    pos, names, B, a = [],[], [], []\n",
    "    z_symbol = {'H' : 1,\n",
    "        'C' : 6,\n",
    "        'N' : 7,\n",
    "        'O' : 8,\n",
    "        'S': 16}\n",
    "    #struct = pdb_parser.get_structure(\"\",  f'/Users/jessihoernschemeyer/pKaSchNet/{file}')\n",
    "\n",
    "    for atom in cut:\n",
    "        id=atom.get_full_id()\n",
    "        res, name = id[3][1], id[4][0]\n",
    "        names.append(name)\n",
    "        pos.append(atom.get_coord())\n",
    "        a.append(atom)\n",
    "\n",
    "\n",
    "    z=[z_symbol.get(name[0]) for name in names]\n",
    "    #Z IS MADE FROM THE NAMES\n",
    "    atoms = Atoms([z_symbol.get(name[0]) for name in names], pos)\n",
    "    atoms.set_cell([[1,0,0], [0,1,0], [0,0,1]])\n",
    "\n",
    "    d, i, j = msp_neighbor_list('dij',  atoms, [r for i in range(len(atoms))])\n",
    "    inputs = {'Z':torch.tensor(z).long(), 'R':torch.tensor(d).float(), 'idx_i':torch.tensor(i).long(), 'idx_j': torch.tensor(j).long()}\n",
    "\n",
    "  \n",
    "    return inputs, [names, a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = pdb_parser.get_structure(\"\",  f'/Users/jessihoernschemeyer/pKaSchNet/prot_194l_A_18_ASP-2.pdb')\n",
    "ns = PDB.NeighborSearch(list(struct.get_atoms())) #set up ns , entire protein\n",
    "cut1 = ns.search(cs[2], 5, \"A\")\n",
    "ns = PDB.NeighborSearch(list(struct.get_atoms())) #set up ns , entire protein\n",
    "cut2 = ns.search(cs[1], 5, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATOM      1  N   LEU     1     -16.599  20.501   9.740  1.00  0.00\n",
      "ATOM      2  H   LEU     1     -16.579  19.602  10.200  1.00  0.00\n",
      "ATOM      3  CA  LEU     1     -16.662  20.530   8.292  1.00  0.00\n",
      "ATOM      4  HA  LEU     1     -17.566  21.052   7.978  1.00  0.00\n",
      "ATOM      5  CB  LEU     1     -15.454  21.253   7.705  1.00  0.00\n",
      "ATOM      6  HB2 LEU     1     -15.429  22.278   8.073  1.00  0.00\n",
      "ATOM      7  HB3 LEU     1     -14.541  20.737   8.003  1.00  0.00\n",
      "ATOM      8  CG  LEU     1     -15.558  21.262   6.183  1.00  0.00\n",
      "ATOM      9  HG  LEU     1     -15.583  20.237   5.814  1.00  0.00\n",
      "ATOM     10  CD1 LEU     1     -16.835  21.984   5.766  1.00  0.00\n",
      "ATOM     11 HD11 LEU     1     -16.811  23.010   6.134  1.00  0.00\n",
      "ATOM     12 HD12 LEU     1     -16.910  21.991   4.678  1.00  0.00\n",
      "ATOM     13 HD13 LEU     1     -17.699  21.468   6.185  1.00  0.00\n",
      "ATOM     14  CD2 LEU     1     -14.351  21.985   5.596  1.00  0.00\n",
      "ATOM     15 HD21 LEU     1     -13.438  21.469   5.894  1.00  0.00\n",
      "ATOM     16 HD22 LEU     1     -14.424  21.992   4.508  1.00  0.00\n",
      "ATOM     17 HD23 LEU     1     -14.325  23.010   5.965  1.00  0.00\n",
      "ATOM     18  C   LEU     1     -16.687  19.122   7.716  1.00  0.00\n",
      "ATOM     19  O   LEU     1     -16.654  18.126   8.472  1.00  0.00\n",
      "ATOM     20  OXT LEU     1     -17.736  19.750   7.435  1.00  0.00\n",
      "ATOM     21  N   LYS     2     -11.671  17.686  14.022  1.00  0.00\n",
      "ATOM     22  H   LYS     2     -11.487  16.698  14.130  1.00  0.00\n",
      "ATOM     23  CA  LYS     2     -13.045  18.141  14.102  1.00  0.00\n",
      "ATOM     24  HA  LYS     2     -13.146  18.850  14.923  1.00  0.00\n",
      "ATOM     25  CB  LYS     2     -13.471  18.828  12.809  1.00  0.00\n",
      "ATOM     26  HB2 LYS     2     -12.829  19.689  12.626  1.00  0.00\n",
      "ATOM     27  HB3 LYS     2     -13.384  18.127  11.979  1.00  0.00\n",
      "ATOM     28  CG  LYS     2     -14.897  19.284  12.931  1.00  0.00\n",
      "ATOM     29  HG2 LYS     2     -15.536  18.420  13.116  1.00  0.00\n",
      "ATOM     30  HG3 LYS     2     -14.981  19.982  13.764  1.00  0.00\n",
      "ATOM     31  CD  LYS     2     -15.333  19.969  11.652  1.00  0.00\n",
      "ATOM     32  HD2 LYS     2     -14.768  20.894  11.541  1.00  0.00\n",
      "ATOM     33  HD3 LYS     2     -15.119  19.309  10.811  1.00  0.00\n",
      "ATOM     34  CE  LYS     2     -16.811  20.282  11.682  1.00  0.00\n",
      "ATOM     35  HE2 LYS     2     -17.054  20.866  12.570  1.00  0.00\n",
      "ATOM     36  HE3 LYS     2     -17.096  20.841  10.791  1.00  0.00\n",
      "ATOM     37  NZ  LYS     2     -17.552  18.998  11.717  1.00  0.00\n",
      "ATOM     38  HZ1 LYS     2     -17.289  18.480  12.543  1.00  0.00\n",
      "ATOM     39  HZ2 LYS     2     -18.545  19.183  11.738  1.00  0.00\n",
      "ATOM     40  HZ3 LYS     2     -17.327  18.457  10.894  1.00  0.00\n",
      "ATOM     41  C   LYS     2     -13.994  16.976  14.345  1.00  0.00\n",
      "ATOM     42  O   LYS     2     -13.563  15.830  14.447  1.00  0.00\n",
      "ATOM     43  OXT LYS     2     -15.205  17.168  14.441  1.00  0.00\n",
      "TER   \n",
      "ATOM     44  O   WAT     3     -14.999  15.436  10.929  1.00  0.00\n",
      "ATOM     45  H1  WAT     3     -14.042  15.436  10.929  1.00  0.00\n",
      "ATOM     46  H2  WAT     3     -15.239  16.363  10.929  1.00  0.00\n",
      "TER   \n",
      "END   \n"
     ]
    }
   ],
   "source": [
    "!cat prot_194l_A_13_LYS-1.pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-17.552,  18.998,  11.717], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct = pdb_parser.get_structure(\"\",  f'/Users/jessihoernschemeyer/pKaSchNet/prot_194l_A_18_ASP-2.pdb')\n",
    "ns = PDB.NeighborSearch(list(struct.get_atoms())) #set up ns , entire protein\n",
    "cut1 = ns.search(cs[2], 5, \"A\")\n",
    "ns = PDB.NeighborSearch(list(struct.get_atoms())) #set up ns , entire protein\n",
    "cut2 = ns.search(cs[1], 5, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m struct \u001b[39m=\u001b[39m pdb_parser\u001b[39m.\u001b[39mget_structure(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/Users/jessihoernschemeyer/pKaSchNet/prot_194l_A_18_ASP-2.pdb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m ns \u001b[39m=\u001b[39m PDB\u001b[39m.\u001b[39mNeighborSearch(\u001b[39mlist\u001b[39m(struct\u001b[39m.\u001b[39mget_atoms())) \u001b[39m#set up ns , entire protein\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m cut1 \u001b[39m=\u001b[39m ns\u001b[39m.\u001b[39msearch(cs[\u001b[39m2\u001b[39m], \u001b[39m5\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m input1, extras1 \u001b[39m=\u001b[39m PDB_to_schnet_input_and_names_map(cut1,\u001b[39m6\u001b[39m)\n\u001b[1;32m     10\u001b[0m struct \u001b[39m=\u001b[39m pdb_parser\u001b[39m.\u001b[39mget_structure(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/Users/jessihoernschemeyer/pKaSchNet/prot_194l_A_13_LYS-1.pdb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cs' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!cat 194l_A_18_ASP-2.pdb\n",
    "!cat 194l_A_13_LYS-1.pdb\n",
    "struct = pdb_parser.get_structure(\"\",  f'/Users/jessihoernschemeyer/pKaSchNet/prot_194l_A_18_ASP-2.pdb')\n",
    "\n",
    "\n",
    "ns = PDB.NeighborSearch(list(struct.get_atoms())) #set up ns , entire protein\n",
    "cut1 = ns.search(cs[2], 5, \"A\")\n",
    "input1, extras1 = PDB_to_schnet_input_and_names_map(cut1,6)\n",
    "\n",
    "struct = pdb_parser.get_structure(\"\",  f'/Users/jessihoernschemeyer/pKaSchNet/prot_194l_A_13_LYS-1.pdb')\n",
    "\n",
    "len([atom for atom in struct.get_atoms()])\n",
    "\n",
    "ns = PDB.NeighborSearch(list(struct.get_atoms())) #set up ns , entire protein\n",
    "cut2 = ns.search(cs[1], 5, \"A\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Atom N>\n",
      "<Atom N>\n",
      "<Atom N>\n",
      "<Atom N>\n"
     ]
    }
   ],
   "source": [
    "a=struct.get_residues()\n",
    "for b in a:\n",
    "    print(b['N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Residue LYS het=  resseq=1 icode= >\n",
      "<Residue LYS het=  resseq=1 icode= >\n",
      "<Residue LYS het=  resseq=1 icode= >\n",
      "<Residue LYS het=  resseq=1 icode= >\n",
      "<Residue LYS het=  resseq=1 icode= >\n",
      "<Residue LYS het=  resseq=1 icode= >\n",
      "<Residue LYS het=  resseq=1 icode= >\n",
      "<Residue LYS het=  resseq=1 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASN het=  resseq=4 icode= >\n",
      "<Residue ASN het=  resseq=4 icode= >\n",
      "<Residue ASN het=  resseq=4 icode= >\n",
      "<Residue ASN het=  resseq=4 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASN het=  resseq=4 icode= >\n",
      "<Residue LEU het=  resseq=2 icode= >\n",
      "<Residue LEU het=  resseq=2 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue LEU het=  resseq=2 icode= >\n",
      "<Residue LEU het=  resseq=2 icode= >\n",
      "<Residue LEU het=  resseq=2 icode= >\n",
      "<Residue LEU het=  resseq=2 icode= >\n",
      "<Residue LEU het=  resseq=2 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASH het=  resseq=3 icode= >\n",
      "<Residue ASN het=  resseq=4 icode= >\n",
      "\n",
      "<Residue LEU het=  resseq=1 icode= >\n",
      "<Residue LEU het=  resseq=1 icode= >\n",
      "<Residue LEU het=  resseq=1 icode= >\n",
      "<Residue LEU het=  resseq=1 icode= >\n",
      "<Residue LEU het=  resseq=1 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LEU het=  resseq=1 icode= >\n",
      "<Residue LEU het=  resseq=1 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue WAT het=  resseq=3 icode= >\n",
      "<Residue WAT het=  resseq=3 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n",
      "<Residue LYS het=  resseq=2 icode= >\n"
     ]
    }
   ],
   "source": [
    "for atom in cut1:\n",
    "    print(atom.get_parent())\n",
    "print(\"\")\n",
    "for atom in cut2:\n",
    "    print(atom.get_parent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cut1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input1.get('Z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schnetpack.representation.schnet import SchNet\n",
    "from torch.nn import Sequential\n",
    "from schnetpack.model import NeuralNetworkPotential\n",
    "from torch import nn\n",
    "from schnetpack.nn import Dense\n",
    "from schnetpack.nn.radial import GaussianRBF\n",
    "from schnetpack.nn.cutoff import CosineCutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39mset_printoptions(profile\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mtensor_dict.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m r\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "weights = torch.load('tensor_dict.pth')\n",
    "r=10\n",
    "output_weight = torch.load('output_tensor.pth')\n",
    "\n",
    "Model = SchNet(n_atom_basis=128, n_interactions=6, radial_basis=GaussianRBF(50, r), cutoff_fn=CosineCutoff(r))\n",
    "for keys, weight in weights.items():\n",
    "    left = f\"Model.{keys}\" \n",
    "    right = f\"torch.nn.Parameter(torch.{weight})\"\n",
    "    execu = f\"{left} = {right}\"\n",
    "    st = execu.replace(\"\\n       \",\"\")\n",
    "    st2 = st.replace(\"representation.\",\"\")\n",
    "    try:\n",
    "        exec(st2)\n",
    "    finally:\n",
    "        right2 = f\"torch.nn.Parameter({weight})\"\n",
    "        E=f\"{left} = {right2}\"\n",
    "        s = execu.replace(\"\\n       \",\"\")\n",
    "        s2 = s.replace(\"representation.\",\"\")\n",
    "        exec(s2)\n",
    "\n",
    "modelll=nn.Sequential(Dense(128,64), Dense(64,1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    modelll[0].weight = nn.Parameter(output_weight.get('dense1_weight'))\n",
    "    modelll[0].bias = nn.Parameter(output_weight.get('dense1_bias'))\n",
    "    modelll[1].weight = nn.Parameter(output_weight.get('dense2_weight'))\n",
    "    modelll[1].bias = nn.Parameter(output_weight.get('dense2_bias'))\n",
    "\n",
    "#input1, extras1 = PDB_to_schnet_input_and_names_map(cut1)\n",
    "#outputs1 = Model(input1)\n",
    "#E1 = modelll(outputs1.get('scalar_representation'))\n",
    "#Eatoms1 = [[atom for atom in extras1[1]], [e for e in E1]]\n",
    "#Eatoms1_dict = dict(zip([atom for atom in extras1[1]], [e for e in E1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "outputs1 = Model(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=10\n",
    "input1, extras1 = PDB_to_schnet_input_and_names_map(cut1)\n",
    "len(input1.get('Z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mschnetpack\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mradial\u001b[39;00m \u001b[39mimport\u001b[39;00m GaussianRBF\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mschnetpack\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcutoff\u001b[39;00m \u001b[39mimport\u001b[39;00m CosineCutoff\n\u001b[0;32m----> 8\u001b[0m torch\u001b[39m.\u001b[39mset_printoptions(profile\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mtensor_dict.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m r\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from schnetpack.representation.schnet import SchNet\n",
    "from torch.nn import Sequential\n",
    "from schnetpack.model import NeuralNetworkPotential\n",
    "from torch import nn\n",
    "from schnetpack.nn import Dense\n",
    "from schnetpack.nn.radial import GaussianRBF\n",
    "from schnetpack.nn.cutoff import CosineCutoff\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "weights = torch.load('tensor_dict.pth')\n",
    "r=10\n",
    "output_weight = torch.load('output_tensor.pth')\n",
    "\n",
    "Model = SchNet(n_atom_basis=128, n_interactions=6, radial_basis=GaussianRBF(50, r), cutoff_fn=CosineCutoff(r))\n",
    "for keys, weight in weights.items():\n",
    "    left = f\"Model.{keys}\" \n",
    "    right = f\"torch.nn.Parameter(torch.{weight})\"\n",
    "    execu = f\"{left} = {right}\"\n",
    "    st = execu.replace(\"\\n       \",\"\")\n",
    "    st2 = st.replace(\"representation.\",\"\")\n",
    "    try:\n",
    "        exec(st2)\n",
    "    finally:\n",
    "        right2 = f\"torch.nn.Parameter({weight})\"\n",
    "        E=f\"{left} = {right2}\"\n",
    "        s = execu.replace(\"\\n       \",\"\")\n",
    "        s2 = s.replace(\"representation.\",\"\")\n",
    "        exec(s2)\n",
    "\n",
    "modelll=nn.Sequential(Dense(128,64), Dense(64,1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    modelll[0].weight = nn.Parameter(output_weight.get('dense1_weight'))\n",
    "    modelll[0].bias = nn.Parameter(output_weight.get('dense1_bias'))\n",
    "    modelll[1].weight = nn.Parameter(output_weight.get('dense2_weight'))\n",
    "    modelll[1].bias = nn.Parameter(output_weight.get('dense2_bias'))\n",
    "\n",
    "input1, extras1 = PDB_to_schnet_input_and_names_map(cut1)\n",
    "outputs1 = Model(input1)\n",
    "E1 = modelll(outputs1.get('scalar_representation'))\n",
    "Eatoms1 = [[atom for atom in extras1[1]], [e for e in E1]]\n",
    "Eatoms1_dict = dict(zip([atom for atom in extras1[1]], [e for e in E1]))\n",
    "\n",
    "Model = SchNet(n_atom_basis=128, n_interactions=6, radial_basis=GaussianRBF(50, r), cutoff_fn=CosineCutoff(r))\n",
    "for keys, weight in weights.items():\n",
    "    left = f\"Model.{keys}\" \n",
    "    right = f\"torch.nn.Parameter(torch.{weight})\"\n",
    "    execu = f\"{left} = {right}\"\n",
    "    st = execu.replace(\"\\n       \",\"\")\n",
    "    st2 = st.replace(\"representation.\",\"\")\n",
    "    try:\n",
    "        exec(st2)\n",
    "    finally:\n",
    "        right2 = f\"torch.nn.Parameter({weight})\"\n",
    "        E=f\"{left} = {right2}\"\n",
    "        s = execu.replace(\"\\n       \",\"\")\n",
    "        s2 = s.replace(\"representation.\",\"\")\n",
    "        exec(s2)\n",
    "\n",
    "modelll=nn.Sequential(Dense(128,64), Dense(64,1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    modelll[0].weight = nn.Parameter(output_weight.get('dense1_weight'))\n",
    "    modelll[0].bias = nn.Parameter(output_weight.get('dense1_bias'))\n",
    "    modelll[1].weight = nn.Parameter(output_weight.get('dense2_weight'))\n",
    "    modelll[1].bias = nn.Parameter(output_weight.get('dense2_bias'))\n",
    "\n",
    "input2, _ = PDB_to_schnet_input_and_names_map(cut2)\n",
    "outputs2 = Model(input2)\n",
    "E2 = modelll(outputs2.get('scalar_representation'))\n",
    "\n",
    "Eatoms2 = [[atom for atom in _[1]], [e for e in E2]]\n",
    "Eatoms2_dict = dict(zip([atom for atom in _[1]], [e for e in E2]))\n",
    "\n",
    "common_entries = set(Eatoms1[0]).intersection(set(Eatoms2[0]))\n",
    "for x in list(common_entries):\n",
    "    print(Eatoms2_dict.get(x), Eatoms1_dict.get(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelll' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(modelll\u001b[39m.\u001b[39mstate_dict()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelll' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(modelll.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_entries = set(Eatoms1[0]).intersection(set(Eatoms2[0]))\n",
    "for x in list(common_entries):\n",
    "    print(Eatoms2_dict.get(x), Eatoms1_dict.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cut[0].get_full_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
