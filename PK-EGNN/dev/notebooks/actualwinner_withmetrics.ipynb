{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_002114\n",
      "params: 19477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 2.0621 | mae val 1.2336\n",
      "     additional metrics:  [1/10]  train 7.2658 | val 2.8827\n",
      "\n",
      "[2/10]  train mae 1.3531 | mae val 1.4853\n",
      "     additional metrics:  [2/10]  train 3.5908 | val 3.9234\n",
      "\n",
      "[3/10]  train mae 1.1949 | mae val 1.0629\n",
      "     additional metrics:  [3/10]  train 2.8943 | val 2.1773\n",
      "\n",
      "[4/10]  train mae 1.0942 | mae val 1.0373\n",
      "     additional metrics:  [4/10]  train 2.5576 | val 2.0406\n",
      "\n",
      "[5/10]  train mae 0.9542 | mae val 1.0138\n",
      "     additional metrics:  [5/10]  train 2.0779 | val 2.0878\n",
      "\n",
      "[6/10]  train mae 0.9741 | mae val 0.9294\n",
      "     additional metrics:  [6/10]  train 2.0233 | val 1.7356\n",
      "\n",
      "[7/10]  train mae 0.8765 | mae val 0.9586\n",
      "     additional metrics:  [7/10]  train 1.7555 | val 1.8588\n",
      "\n",
      "[8/10]  train mae 0.8452 | mae val 0.8553\n",
      "     additional metrics:  [8/10]  train 1.6366 | val 1.5473\n",
      "\n",
      "[9/10]  train mae 0.8117 | mae val 0.8640\n",
      "     additional metrics:  [9/10]  train 1.5432 | val 1.4979\n",
      "\n",
      "[10/10]  train mae 0.7772 | mae val 0.8403\n",
      "     additional metrics:  [10/10]  train 1.4435 | val 1.4571\n",
      "\n",
      "142.79056477546692 sec\n"
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_002418\n",
      "params: 19477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mse 8.0043 | mse val 3.7785\n",
      "     additional metrics:  [1/10]  train 2.2650 | val 1.3917\n",
      "\n",
      "[2/10]  train mse 3.5211 | mse val 2.7217\n",
      "     additional metrics:  [2/10]  train 1.4014 | val 1.1171\n",
      "\n",
      "[3/10]  train mse 2.6798 | mse val 2.5309\n",
      "     additional metrics:  [3/10]  train 1.1612 | val 1.1299\n",
      "\n",
      "[4/10]  train mse 2.2919 | mse val 2.2743\n",
      "     additional metrics:  [4/10]  train 1.0803 | val 1.0970\n",
      "\n",
      "[5/10]  train mse 2.0205 | mse val 2.2295\n",
      "     additional metrics:  [5/10]  train 1.0308 | val 1.1071\n",
      "\n",
      "[6/10]  train mse 1.8992 | mse val 1.8027\n",
      "     additional metrics:  [6/10]  train 0.9842 | val 0.9809\n",
      "\n",
      "[7/10]  train mse 1.6268 | mse val 1.7840\n",
      "     additional metrics:  [7/10]  train 0.8995 | val 0.9867\n",
      "\n",
      "[8/10]  train mse 1.5097 | mse val 1.6459\n",
      "     additional metrics:  [8/10]  train 0.8741 | val 0.9512\n",
      "\n",
      "[9/10]  train mse 1.4292 | mse val 1.6130\n",
      "     additional metrics:  [9/10]  train 0.8568 | val 0.9318\n",
      "\n",
      "[10/10]  train mse 1.4762 | mse val 1.6972\n",
      "     additional metrics:  [10/10]  train 0.8823 | val 0.9790\n",
      "\n",
      "109.65607762336731 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mse',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
