{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_223339\n",
      "params: 17695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.5971 | val 1.2026\n",
      "[2/3]  train 1.4604 | val 1.0831\n",
      "[3/3]  train 1.3527 | val 1.0277\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=3, batch_size=1,\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_223450\n",
      "params: 17695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.6055 | val 1.2054\n",
      "[2/3]  train 1.4681 | val 1.0860\n",
      "[3/3]  train 1.3560 | val 1.0269\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=3, batch_size=1,\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_223506\n",
      "params: 17695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.6846 | val 1.0430\n",
      "[2/3]  train 1.3162 | val 0.9955\n",
      "[3/3]  train 1.2104 | val 1.1008\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=3, batch_size=1,\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_223627\n",
      "params: 17697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.9545 | val 1.3749\n",
      "[2/3]  train 1.5383 | val 1.1018\n",
      "[3/3]  train 1.3066 | val 1.0033\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=3, batch_size=1,\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_223553\n",
      "params: 17705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.4138 | val 1.0971\n",
      "[2/3]  train 1.2822 | val 1.0316\n",
      "[3/3]  train 1.2361 | val 1.0102\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=3, batch_size=1,\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_223701\n",
      "params: 16377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.2412 | val 1.0048\n",
      "[2/3]  train 1.2412 | val 1.0127\n",
      "[3/3]  train 1.2269 | val 1.0246\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=3, batch_size=1,\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_223736\n",
      "params: 16375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.2840 | val 1.0795\n",
      "[2/3]  train 1.2543 | val 1.0525\n",
      "[3/3]  train 1.2375 | val 1.0336\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=3, batch_size=1,\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_223828\n",
      "params: 16375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.3027 | val 1.0417\n",
      "[2/3]  train 1.2507 | val 1.0201\n",
      "[3/3]  train 1.2428 | val 1.0153\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=3, batch_size=1,\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_224450\n",
      "params: 16348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.7770 | val 1.4589\n",
      "[2/3]  train 1.7317 | val 1.4089\n",
      "[3/3]  train 1.6833 | val 1.3561\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='pool',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=3, batch_size=1,\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.7608 | val 1.4988\n",
      "[2/3]  train 1.7608 | val 1.4988\n",
      "[3/3]  train 1.7608 | val 1.4988\n"
     ]
    }
   ],
   "source": [
    "# n‑conv without RBF / Attn\n",
    "use_rbf      =Fa,\n",
    "use_attn     =False,\n",
    "use_boost    =False,     # Linear(1→1) after aggregator\n",
    "use_prot     =True,      # protein‑level EGNN\n",
    "use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "# re‑instantiate:\n",
    "model = Model(cfg)\n",
    "\n",
    "\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3]  train 1.2384 | val 1.0388\n",
      "[2/3]  train 1.2398 | val 1.0388\n",
      "[3/3]  train 1.2375 | val 1.0388\n"
     ]
    }
   ],
   "source": [
    "# n‑conv without RBF / Attn\n",
    "use_rbf      =False,\n",
    "use_attn     =False,\n",
    "use_boost    =False,     # Linear(1→1) after aggregator\n",
    "use_prot     =True,      # protein‑level EGNN\n",
    "use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "# re‑instantiate:\n",
    "model = Model(cfg)\n",
    "\n",
    "\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
