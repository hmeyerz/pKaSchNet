{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'norm_coors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3b5c13c59170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProteinModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainable params :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{model.n_trainable_parameters():,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3b5c13c59170>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_NEIGHBORS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mnum_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m118\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nearest_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         ).to(cfg.device)\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'norm_coors'"
     ]
    }
   ],
   "source": [
    "#centroids.per\n",
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# =====================================================================\n",
    "# 1) model – everything behind one nn.Module\n",
    "# =====================================================================\n",
    "class ProteinModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # -------------------------------- backbone --------------------------------\n",
    "        self.egnn = StackedEGNN(\n",
    "            dim=cfg.dim, depth=cfg.depth, hidden_dim=cfg.hidden_dim,\n",
    "            dropout=cfg.dropout, num_positions=cfg.N_NEIGHBORS,\n",
    "            num_tokens=118, num_nearest_neighbors=cfg.num_neighbors\n",
    "        ).to(cfg.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(\n",
    "            LearnableRBF(num_basis=cfg.basis, cutoff=10.0).to(cfg.device),\n",
    "            enabled=cfg.use_rbf\n",
    "        )\n",
    "        self.attn = TunableBlock(\n",
    "            AttentionBlock(embed_dim=cfg.dim+cfg.basis,\n",
    "                           num_heads=cfg.dim+cfg.basis,\n",
    "                           hidden_dim=cfg.hidden_dim).to(cfg.device),\n",
    "            enabled=cfg.use_attn\n",
    "        )\n",
    "        # -------------------------------- aggregation choices ---------------------\n",
    "        if cfg.aggregator == 'linear':\n",
    "            self.nconv = None; out_dim = cfg.dim+cfg.basis\n",
    "        elif cfg.aggregator in ('nconv','nconv+linear'):\n",
    "            k = cfg.dim + cfg.basis\n",
    "            self.nconv = nn.Conv1d(cfg.N_NEIGHBORS, 1, kernel_size=k, padding=0).to(cfg.device)\n",
    "            out_dim = 1\n",
    "        elif cfg.aggregator == 'pool':          # global max – keeps head alive but unused\n",
    "            self.nconv = None; out_dim = cfg.dim+cfg.basis\n",
    "        else:\n",
    "            raise ValueError(f\"unknown aggregator {cfg.aggregator}\")\n",
    "\n",
    "        self.pred_head = nn.Linear(out_dim, 1).to(cfg.device) if cfg.use_pred_head else nn.Identity()\n",
    "\n",
    "        # second‑level EGNN on centroids (can be disabled via TunableBlock too)\n",
    "        self.prot_egnn = TunableBlock(\n",
    "            EGNN(dim=1, update_coors=False, num_nearest_neighbors=3).to(cfg.device),\n",
    "            enabled=True\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def forward(self, z, x):                      # z:(R,N)  x:(R,N,3)\n",
    "        h_list, coords = self.egnn(z, x)          # h:(R,N,dim) coords:(R,N,3)\n",
    "        h = h_list[0] if isinstance(h_list,(list,tuple)) else h\n",
    "\n",
    "        centroids = coords.mean(dim=1).unsqueeze(1)          # (R,1,3)\n",
    "        rbf = self.rbf(centroids, coords)                    # (R,N,N,basis) or passthrough\n",
    "\n",
    "        h_T = h.transpose(1,2)                               # (R,dim,N)\n",
    "        r_T = rbf.transpose(1,2) if cfg.use_rbf else torch.empty_like(h_T[..., :cfg.basis])\n",
    "        tok = torch.cat((r_T, h_T), 1)                       # (R,dim+basis,N)\n",
    "\n",
    "        tok = tok.permute(2,0,1)                             # (N,R,C)\n",
    "        tok,_ = self.attn(tok)                               # (N,R,C) (or identity)\n",
    "        tok = tok.permute(1,0,2)                             # (R,N,C)\n",
    "\n",
    "        # ---- aggregation route --------------------------------------\n",
    "        if self.nconv is not None:\n",
    "            tok = self.nconv(tok)                            # (R,1,1)\n",
    "            tok = tok.squeeze(-1)                            # (R,1)\n",
    "        elif cfg.aggregator == 'pool':\n",
    "            tok = tok.max(dim=1).values                      # (R,C)\n",
    "        else:                                                # 'linear'\n",
    "            tok = tok                                        # (R,N,C) – max over N\n",
    "            tok = tok.max(dim=1).values                      # (R,C)\n",
    "\n",
    "        preds = self.pred_head(tok)                          # (R,1) or (R,1) Identity\n",
    "\n",
    "        # protein‑level EGNN keeps gradients but (optionally) does nothing\n",
    "        preds = preds.unsqueeze(0)                           # (1,R,1)\n",
    "        coords_cent = centroids.permute(1,0,2)               # (1,R,3)\n",
    "        preds = self.prot_egnn(preds, coords_cent)[0].squeeze(0)  # (R,1)\n",
    "\n",
    "        return preds                                         # (R,1)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def n_trainable_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "model = ProteinModel(cfg)\n",
    "print(\"Trainable params :\", f\"{model.n_trainable_parameters():,}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 0) hyper‑parameter “dashboard” – Py 3.6‑compatible\n",
    "# ---------------------------------------------------------------\n",
    "from types import SimpleNamespace\n",
    "import torch, datetime\n",
    "# ================================================================\n",
    "# 0) ── hyper‑parameters / switches  (edit here, nothing else)\n",
    "# ================================================================\n",
    "cfg = {\n",
    "    # backbone\n",
    "    'dim'           : 12,\n",
    "    'basis'         : 6,\n",
    "    'depth'         : 2,\n",
    "    'hidden_dim'    : 4,\n",
    "    'num_neighbors' : 8,           # k in EGNN\n",
    "    'dropout'       : 0.02,\n",
    "    # neighbourhood tensor\n",
    "    'N_NEIGHBORS'   : 100,\n",
    "    # optional modules            ─────────   choose ablation\n",
    "    #   'linear'        →   pred_head only\n",
    "    #   'nconv+linear'  →   nconv then pred_head\n",
    "    #   'nconv'         →   nconv only\n",
    "    #   'pool'          →   max‑pool, ignores nconv & pred_head\n",
    "    'aggregator'    : 'linear',\n",
    "    'use_rbf'       : True,\n",
    "    'use_attn'      : True,\n",
    "    'use_nconv'     : False,\n",
    "    'use_pred_head' : True,\n",
    "    # loss / scheduler\n",
    "    'loss_type'     : 'mae',        # 'mae' or 'mse'\n",
    "    'sched_metric'  : 'val_mae',    # what ReduceLRO looks at\n",
    "    # training\n",
    "    'lr'            : 5e-3,\n",
    "    'epochs'        : 20,\n",
    "    'batch_size'    : 1,\n",
    "    'device'        : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'runid'         : datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "}\n",
    "\n",
    "print(\"Run‑ID:\", cfg['runid'])\n",
    "\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 100\n",
    "BATCH_SIZE  =  1           # not safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 12, 6 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=4\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "# =====================================================================\n",
    "# 2) dataset / dataloader – *identical* to your earlier code\n",
    "# =====================================================================\n",
    "N_NEIGHBORS = cfg.N_NEIGHBORS      # just to keep names aligned\n",
    "# ...  (pad_collate, InMemoryHoodDataset)  ...\n",
    "\n",
    "train_loader = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "val_loader   = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "from architecture import TunableBlock, StackedEGNN, AttentionBlock, LearnableRBF\n",
    "\n",
    "egnn_net = TunableBlock(\n",
    "    StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                num_nearest_neighbors=num_neighbors, norm_coors=True),\n",
    "    enabled=True\n",
    ")\n",
    "\n",
    "rbf_layer = TunableBlock(\n",
    "    LearnableRBF(num_basis=basis, cutoff=cutoff),\n",
    "    enabled=True\n",
    ")\n",
    "\n",
    "mha_layer = TunableBlock(\n",
    "    AttentionBlock(embed_dim=dim + basis, num_heads=num_heads, hidden_dim=hidden_dim),\n",
    "    enabled=True\n",
    ")\n",
    "\n",
    "nconv_layer = TunableBlock(\n",
    "    nn.Conv1d(in_channels=N_NEIGHBORS, out_channels=1, kernel_size=dim + basis, padding=0),  # match your shape\n",
    "    enabled=True\n",
    ")\n",
    "\n",
    "pred_head = TunableBlock(\n",
    "    nn.Linear(1, 1),  # could also be Linear(dim + basis, 1) if used earlier\n",
    "    enabled=True\n",
    ")\n",
    "\n",
    "protein_egnn = TunableBlock(\n",
    "    EGNN(dim=1, update_coors=True, norm_coors=True, norm_feats=True,\n",
    "         fourier_features=6, valid_radius=8),\n",
    "    enabled=True\n",
    ")\n",
    "\n",
    "conv = TunableBlock(\n",
    "    nn.Conv1d(1, 1, 7, padding=3),\n",
    "    enabled=True\n",
    ")\n",
    "\n",
    "mha_layer.enabled = False\n",
    "rbf_layer.enabled = True\n",
    "nconv_layer.enabled = False\n",
    "pred_head.enabled = True\n",
    "conv.enabled = False\n",
    "\n",
    "modules = [egnn_net, rbf_layer, mha_layer, nconv_layer, pred_head, protein_egnn, conv]\n",
    "enabled_params = [p for m in modules if m.enabled for p in m.parameters()]\n",
    "optimizer = torch.optim.AdamW(enabled_params, lr=5e-3)\n",
    "\n",
    "# =====================================================================\n",
    "# 3) training utilities (loss + scheduler are user‑switchable)\n",
    "# =====================================================================\n",
    "criterion_map = {'mse': nn.MSELoss, 'mae': nn.L1Loss}\n",
    "criterion = criterion_map[cfg.loss_type.lower()]()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "scaler  = GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    h_out, coords = egnn_net(z_r, x_r)\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out\n",
    "\n",
    "    centroids = coords.mean(dim=1).unsqueeze(1)\n",
    "    rbf = rbf_layer(centroids, coords)\n",
    "\n",
    "    h0 = h.transpose(1, 2)\n",
    "    r0 = rbf.transpose(1, 2)\n",
    "    tok = torch.cat((r0, h0), dim=1)\n",
    "\n",
    "    tok_input = tok.permute(2, 0, 1)  # [N, R, C]\n",
    "\n",
    "    mha_out = mha_layer(tok_input)\n",
    "\n",
    "    if isinstance(mha_out, tuple):\n",
    "        tok = mha_out[0]\n",
    "    else:\n",
    "        tok = mha_out  # passthrough from disabled module\n",
    "\n",
    "    tok = tok.permute(1, 0, 2)  # [R, N, C]\n",
    "\n",
    "    tok = tok.permute(1, 0, 2)                # (R, N, C)\n",
    "    \n",
    "    tok = nconv_layer(tok)                    # (R, 1, 1)\n",
    "    tok = tok.squeeze(-1)                     # (R, 1)\n",
    "\n",
    "    preds = pred_head(tok)                    # (R, 1)\n",
    "\n",
    "    t = preds[:, 0].T.unsqueeze(2)            # (1, R, 1)\n",
    "    coords = centroids.permute(1, 0, 2)       # (1, R, 3)\n",
    "    preds = protein_egnn(t, coords)[0].permute(1, 2, 0)  # (R, 1, 1)\n",
    "\n",
    "    preds = conv(preds)                       # (R, 1, 1)\n",
    "    return preds.squeeze(-1), coords\n",
    "# =====================================================================\n",
    "# 4) training / validation loop (records both MAE & MSE every epoch)\n",
    "# =====================================================================\n",
    "def run_epoch(loader, train:bool):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    mae_meter, mse_meter = [], []\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg.device)\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg.device)\n",
    "        y_r   = y.view(-1)[valid].to(cfg.device)\n",
    "\n",
    "        with (autocast(enabled=(cfg.device=='cuda'))):\n",
    "            preds = model(z_r, x_r).flatten()\n",
    "            loss  = criterion(preds, y_r)\n",
    "            mae   = nn.L1Loss()(preds, y_r).item()\n",
    "            mse   = nn.MSELoss()(preds, y_r).item()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        mae_meter.append(mae)\n",
    "        mse_meter.append(mse)\n",
    "    return np.mean(mae_meter), np.mean(mse_meter)\n",
    "\n",
    "history = {'train_mae':[], 'val_mae':[], 'train_mse':[], 'val_mse':[]}\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(cfg.epochs):\n",
    "    tr_mae, tr_mse = run_epoch(train_loader, True)\n",
    "    vl_mae, vl_mse = run_epoch(val_loader,   False)\n",
    "\n",
    "    history['train_mae'].append(tr_mae); history['train_mse'].append(tr_mse)\n",
    "    history['val_mae'].append(vl_mae);   history['val_mse'].append(vl_mse)\n",
    "\n",
    "    metric_to_step = vl_mae if cfg.scheduler_metric=='val_mae' else vl_mse\n",
    "    scheduler.step(metric_to_step)\n",
    "\n",
    "    print(f\"[{epoch+1:03d}/{cfg.epochs}] \"\n",
    "          f\"train MAE {tr_mae:.4f} | val MAE {vl_mae:.4f} | \"\n",
    "          f\"train MSE {tr_mse:.4f} | val MSE {vl_mse:.4f}\")\n",
    "\n",
    "print(\"Total time:\", (time.time()-t0)/60, \"min\")\n",
    "\n",
    "\n",
    "#    with torch.no_grad():\n",
    "#        for z, x, y, mask in val_loader:\n",
    "#            valid   = mask.view(-1)\n",
    "#            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "#            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "#            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "#            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "#            \n",
    "#            preds = pred_head(feats)       \n",
    "#            t=preds.unsqueeze(0)\n",
    "            #preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "#            loss  = criterion(preds.flatten(), y_res)\n",
    "#            vl_losses.append(loss.item())\n",
    "\n",
    "#    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "#print(time.time() - t0,\"sec\")##\n",
    "\n",
    "# =====================================================================\n",
    "# 5) save *everything required* to resume later\n",
    "# =====================================================================\n",
    "ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optimizer'  : optimizer.state_dict(),\n",
    "    'scheduler'  : scheduler.state_dict(),\n",
    "    'cfg'        : asdict(cfg),\n",
    "    'history'    : history,\n",
    "}\n",
    "name = f\"checkpoint_{cfg.runid}.pt\"\n",
    "#torch.save(ckpt, name)\n",
    "print(\"Saved\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_205300\n",
      "Trainable parameters: 17,935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/020]  train MAE 1.3625 | val MAE 1.2577  ||  train MSE 3.8920 | val MSE 3.8825\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ab5a630f887c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mtr_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0mvl_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvl_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ab5a630f887c>\u001b[0m in \u001b[0;36mepoch_loop\u001b[0;34m(loader, train)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 1) ── model definition  (uses your *unchanged* architecture.py)\n",
    "# ================================================================\n",
    "import torch, torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "\n",
    "class Cfg(dict):\n",
    "    \"\"\"dot‑access + dict‑access wrapper (Py 3.6 safe).\"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    def as_dict(self):               # for checkpoints\n",
    "        return dict(self)\n",
    "cfg = Cfg(\n",
    "    # ------------- backbone -------------\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4,\n",
    "    num_neighbors=8,          # k in EGNN\n",
    "    dropout=0.02,\n",
    "    norm_coors=True,          # <── NEW: make it tunable\n",
    "    N_NEIGHBORS=100,\n",
    "\n",
    "    # ---------- ablation switches -------\n",
    "    aggregator='linear', use_rbf=True, use_attn=True,\n",
    "    use_nconv=False, use_pred_head=True,\n",
    "\n",
    "    # ---------- training / misc ---------\n",
    "    loss_type='mae', sched_metric='val_mae',\n",
    "    lr=5e-3, epochs=5, batch_size=1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "print(cfg)\n",
    "\n",
    "# ================================================================\n",
    "# 0) ── hyper‑parameters / switches  (edit here, nothing else)\n",
    "\n",
    "\n",
    "class ProteinModel(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(ProteinModel, self).__init__()\n",
    "        self.c = c\n",
    "\n",
    "        # ---------------- EGNN backbone ----------------\n",
    "        self.egnn = StackedEGNN(\n",
    "    dim               = cfg.dim,\n",
    "    depth             = cfg.depth,\n",
    "    hidden_dim        = cfg.hidden_dim,\n",
    "    dropout           = cfg.dropout,\n",
    "    num_positions     = cfg.N_NEIGHBORS,\n",
    "    num_tokens        = 98,\n",
    "    num_nearest_neighbors = cfg.num_neighbors,\n",
    "    norm_coors        = cfg.norm_coors     # <── ONLY extra argument\n",
    ").to(cfg.device)\n",
    "\n",
    "        # --------------- optional blocks ----------------\n",
    "        self.rbf  = TunableBlock(\n",
    "            LearnableRBF(num_basis=c['basis'], cutoff=10.0).to(c['device']),\n",
    "            enabled=c['use_rbf']\n",
    "        )\n",
    "        self.attn = TunableBlock(\n",
    "            AttentionBlock(embed_dim=c['dim']+c['basis'],\n",
    "                           num_heads=c['dim']+c['basis'],\n",
    "                           hidden_dim=c['hidden_dim']).to(c['device']),\n",
    "            enabled=c['use_attn']\n",
    "        )\n",
    "\n",
    "        if c['aggregator'] in ('nconv', 'nconv+linear'):\n",
    "            k = c['dim'] + c['basis']\n",
    "            self.nconv = nn.Conv1d(c['N_NEIGHBORS'], 1, kernel_size=k, padding=0)\\\n",
    "                             .to(c['device'])\n",
    "            out_dim = 1\n",
    "        else:\n",
    "            self.nconv = None\n",
    "            out_dim = c['dim'] + c['basis']\n",
    "\n",
    "        self.pred_head = (nn.Linear(out_dim, 1).to(c['device'])\n",
    "                          if c['use_pred_head'] else nn.Identity())\n",
    "\n",
    "        # ---- protein‑level EGNN on centroids (update_coors = True) ----\n",
    "        self.prot_egnn = TunableBlock(\n",
    "            EGNN(dim=1, update_coors=True, num_nearest_neighbors=3)\\\n",
    "                .to(c['device']),\n",
    "            enabled=True\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    def forward(self, z, x):                 # z:(R,N)  x:(R,N,3)\n",
    "        h_list, coords = self.egnn(z, x)      # h:(R,N,dim)\n",
    "        h = h_list[0] if isinstance(h_list,(list,tuple)) else h_list\n",
    "\n",
    "        centroids = coords.mean(dim=1).unsqueeze(1)            # (R,1,3)\n",
    "        rbf = self.rbf(centroids, coords)                      # or passthrough\n",
    "\n",
    "        h_T = h.transpose(1,2)                                 # (R,dim,N)\n",
    "        if self.c['use_rbf']:\n",
    "            r_T = rbf.transpose(1,2)\n",
    "        else:\n",
    "            r_T = torch.empty(0, device=h.device)\n",
    "\n",
    "        tok = torch.cat((r_T, h_T), 1)                         # (R,dim+basis,N)\n",
    "\n",
    "        tok = tok.permute(2,0,1)                               # (N,R,C)\n",
    "        tok,_ = self.attn(tok)                                 # (N,R,C) / identity\n",
    "        tok = tok.permute(1,0,2)                               # (R,N,C)\n",
    "\n",
    "        # -------- aggregator routes --------\n",
    "        if self.nconv is not None:\n",
    "            tok = self.nconv(tok).squeeze(-1)                  # (R,1)\n",
    "        elif self.c['aggregator'] == 'pool':                   # max‑pool\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "        else:                                                  # linear\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "\n",
    "        preds = self.pred_head(tok)                            # (R,1)\n",
    "\n",
    "        # ---- residue → protein aggregate -----\n",
    "        preds_ = preds.unsqueeze(0)                            # (1,R,1)\n",
    "        coords_ = centroids.permute(1,0,2)                     # (1,R,3)\n",
    "        preds  = self.prot_egnn(preds_, coords_)[0].squeeze(0) # (R,1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "# ── make sure cfg behaves like a dict no matter what it is\n",
    "if not isinstance(cfg, dict):\n",
    "    cfg = vars(cfg)          # SimpleNamespace → ordinary dict\n",
    "\n",
    "model = ProteinModel(cfg)\n",
    "print(\"Trainable parameters:\", \"{:,}\".format(model.n_params()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:5], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "# =====================================================================\n",
    "# 2) dataset / dataloader – *identical* to your earlier code\n",
    "# =====================================================================\n",
    "N_NEIGHBORS = cfg.N_NEIGHBORS      # just to keep names aligned\n",
    "# ...  (pad_collate, InMemoryHoodDataset)  ...\n",
    "\n",
    "train_loader = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "val_loader   = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3) ── optimisation bits\n",
    "# ================================================================\n",
    "criterion = nn.L1Loss() if cfg['loss_type']=='mae' else nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "scaler = GradScaler(enabled=(cfg['device']=='cuda'))\n",
    "\n",
    "# ================================================================\n",
    "# 4) ── train / validate\n",
    "# ================================================================\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    mae_acc, mse_acc = [], []\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg['device'])\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg['device'])\n",
    "        y_r   = y.view(-1)[valid].to(cfg['device'])\n",
    "\n",
    "        with autocast(enabled=(cfg['device']=='cuda')):\n",
    "            out  = model(z_r, x_r).flatten()\n",
    "            loss = criterion(out, y_r)\n",
    "            mae  = nn.L1Loss()(out, y_r).item()\n",
    "            mse  = nn.MSELoss()(out, y_r).item()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        mae_acc.append(mae); mse_acc.append(mse)\n",
    "    return sum(mae_acc)/len(mae_acc), sum(mse_acc)/len(mse_acc)\n",
    "\n",
    "for ep in range(cfg['epochs']):\n",
    "    tr_mae, tr_mse = epoch_loop(train_loader, True)\n",
    "    vl_mae, vl_mse = epoch_loop(val_loader,   False)\n",
    "\n",
    "    metric = vl_mae if cfg['sched_metric']=='val_mae' else vl_mse\n",
    "    scheduler.step(metric)\n",
    "\n",
    "    print(\"[{:03d}/{:03d}]  \"\n",
    "          \"train MAE {:.4f} | val MAE {:.4f}  ||  \"\n",
    "          \"train MSE {:.4f} | val MSE {:.4f}\".format(\n",
    "              ep+1, cfg['epochs'], tr_mae, vl_mae, tr_mse, vl_mse))\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5) ── save everything needed to resume\n",
    "# ================================================================\n",
    "ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optim_state': optimizer.state_dict(),\n",
    "    'sched_state': scheduler.state_dict(),\n",
    "    'cfg'        : cfg,\n",
    "}\n",
    "ckpt_name = \"ckpt_{}.pt\".format(cfg['runid'])\n",
    "#torch.save(ckpt, ckpt_name)\n",
    "print(\"Saved checkpoint:\", ckpt_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_205840\n",
      "{'dim': 12, 'basis': 6, 'depth': 2, 'hidden_dim': 4, 'num_neighbors': 8, 'dropout': 0.02, 'norm_coors': True, 'N_NEIGHBORS': 100, 'aggregator': 'linear', 'use_rbf': True, 'use_attn': True, 'use_nconv': False, 'use_pred_head': True, 'loss_type': 'mae', 'sched_metric': 'val_mae', 'lr': 0.005, 'epochs': 5, 'batch_size': 1, 'device': 'cpu', 'runid': '20250726_205840'}\n",
      "Trainable parameters: 17,695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/005]  train MAE 1.4693 | val MAE 1.4530  ||  train MSE 3.8303 | val MSE 4.6516\n",
      "[002/005]  train MAE 1.2156 | val MAE 1.2802  ||  train MSE 2.9310 | val MSE 3.9881\n",
      "[003/005]  train MAE 1.1161 | val MAE 1.2816  ||  train MSE 2.7563 | val MSE 3.7701\n",
      "[004/005]  train MAE 1.1059 | val MAE 1.2627  ||  train MSE 2.7447 | val MSE 3.9004\n",
      "[005/005]  train MAE 1.0930 | val MAE 1.2660  ||  train MSE 2.6866 | val MSE 3.9391\n",
      "Saved checkpoint: ckpt_20250726_205840.pt\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 1) ── model definition  (uses your *unchanged* architecture.py)\n",
    "# ================================================================\n",
    "import torch, torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "\n",
    "class Cfg(dict):\n",
    "    \"\"\"dot‑access + dict‑access wrapper (Py 3.6 safe).\"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    def as_dict(self):               # for checkpoints\n",
    "        return dict(self)\n",
    "cfg = Cfg(\n",
    "    # ------------- backbone -------------\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4,\n",
    "    num_neighbors=8,          # k in EGNN\n",
    "    dropout=0.02,\n",
    "    norm_coors=True,          # <── NEW: make it tunable\n",
    "    N_NEIGHBORS=100,\n",
    "\n",
    "    # ---------- ablation switches -------\n",
    "    aggregator='linear', use_rbf=True, use_attn=True,\n",
    "    use_nconv=False, use_pred_head=True,\n",
    "\n",
    "    # ---------- training / misc ---------\n",
    "    loss_type='mae', sched_metric='val_mae',\n",
    "    lr=5e-3, epochs=5, batch_size=1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "print(cfg)\n",
    "\n",
    "# ================================================================\n",
    "# 0) ── hyper‑parameters / switches  (edit here, nothing else)\n",
    "\n",
    "\n",
    "class ProteinModel(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(ProteinModel, self).__init__()\n",
    "        self.c = c\n",
    "\n",
    "        # ---------------- EGNN backbone ----------------\n",
    "        self.egnn = StackedEGNN(\n",
    "    dim               = cfg.dim,\n",
    "    depth             = cfg.depth,\n",
    "    hidden_dim        = cfg.hidden_dim,\n",
    "    dropout           = cfg.dropout,\n",
    "    num_positions     = cfg.N_NEIGHBORS,\n",
    "    num_tokens        = 98,\n",
    "    num_nearest_neighbors = cfg.num_neighbors,\n",
    "    norm_coors        = cfg.norm_coors     # <── ONLY extra argument\n",
    ").to(cfg.device)\n",
    "\n",
    "        # --------------- optional blocks ----------------\n",
    "        self.rbf  = TunableBlock(\n",
    "            LearnableRBF(num_basis=c['basis'], cutoff=10.0).to(c['device']),\n",
    "            enabled=c['use_rbf']\n",
    "        )\n",
    "        self.attn = TunableBlock(\n",
    "            AttentionBlock(embed_dim=c['dim']+c['basis'],\n",
    "                           num_heads=c['dim']+c['basis'],\n",
    "                           hidden_dim=c['hidden_dim']).to(c['device']),\n",
    "            enabled=c['use_attn']\n",
    "        )\n",
    "\n",
    "        if c['aggregator'] in ('nconv', 'nconv+linear'):\n",
    "            k = c['dim'] + c['basis']\n",
    "            self.nconv = nn.Conv1d(c['N_NEIGHBORS'], 1, kernel_size=k, padding=0)\\\n",
    "                             .to(c['device'])\n",
    "            out_dim = 1\n",
    "        else:\n",
    "            self.nconv = None\n",
    "            out_dim = c['dim'] + c['basis']\n",
    "\n",
    "        self.pred_head = (nn.Linear(out_dim, 1).to(c['device'])\n",
    "                          if c['use_pred_head'] else nn.Identity())\n",
    "\n",
    "        # ---- protein‑level EGNN on centroids (update_coors = True) ----\n",
    "        self.prot_egnn = TunableBlock(\n",
    "            EGNN(dim=1, update_coors=True, num_nearest_neighbors=3)\\\n",
    "                .to(c['device']),\n",
    "            enabled=True\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    def forward(self, z, x):                 # z:(R,N)  x:(R,N,3)\n",
    "        h_list, coords = self.egnn(z, x)      # h:(R,N,dim)\n",
    "        h = h_list[0] if isinstance(h_list,(list,tuple)) else h_list\n",
    "\n",
    "        centroids = coords.mean(dim=1).unsqueeze(1)            # (R,1,3)\n",
    "        rbf = self.rbf(centroids, coords)                      # or passthrough\n",
    "\n",
    "        h_T = h.transpose(1,2)                                 # (R,dim,N)\n",
    "        if self.c['use_rbf']:\n",
    "            r_T = rbf.transpose(1,2)\n",
    "        else:\n",
    "            r_T = torch.empty(0, device=h.device)\n",
    "\n",
    "        tok = torch.cat((r_T, h_T), 1)                         # (R,dim+basis,N)\n",
    "\n",
    "        tok = tok.permute(2,0,1)                               # (N,R,C)\n",
    "        tok,_ = self.attn(tok)                                 # (N,R,C) / identity\n",
    "        tok = tok.permute(1,0,2)                               # (R,N,C)\n",
    "\n",
    "        # -------- aggregator routes --------\n",
    "        if self.nconv is not None:\n",
    "            tok = self.nconv(tok).squeeze(-1)                  # (R,1)\n",
    "        elif self.c['aggregator'] == 'pool':                   # max‑pool\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "        else:                                                  # linear\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "\n",
    "        preds = self.pred_head(tok)                            # (R,1)\n",
    "\n",
    "        # ---- residue → protein aggregate -----\n",
    "        preds_ = preds.unsqueeze(0)                            # (1,R,1)\n",
    "        coords_ = centroids.permute(1,0,2)                     # (1,R,3)\n",
    "        preds  = self.prot_egnn(preds_, coords_)[0].squeeze(0) # (R,1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "# ── make sure cfg behaves like a dict no matter what it is\n",
    "if not isinstance(cfg, dict):\n",
    "    cfg = vars(cfg)          # SimpleNamespace → ordinary dict\n",
    "\n",
    "model = ProteinModel(cfg)\n",
    "print(\"Trainable parameters:\", \"{:,}\".format(model.n_params()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:5], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "# =====================================================================\n",
    "# 2) dataset / dataloader – *identical* to your earlier code\n",
    "# =====================================================================\n",
    "N_NEIGHBORS = cfg.N_NEIGHBORS      # just to keep names aligned\n",
    "# ...  (pad_collate, InMemoryHoodDataset)  ...\n",
    "\n",
    "train_loader = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "val_loader   = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3) ── optimisation bits\n",
    "# ================================================================\n",
    "criterion = nn.L1Loss() if cfg['loss_type']=='mae' else nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "scaler = GradScaler(enabled=(cfg['device']=='cuda'))\n",
    "\n",
    "# ================================================================\n",
    "# 4) ── train / validate\n",
    "# ================================================================\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    mae_acc, mse_acc = [], []\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg['device'])\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg['device'])\n",
    "        y_r   = y.view(-1)[valid].to(cfg['device'])\n",
    "\n",
    "        with autocast(enabled=(cfg['device']=='cuda')):\n",
    "            out  = model(z_r, x_r).flatten()\n",
    "            loss = criterion(out, y_r)\n",
    "            mae  = nn.L1Loss()(out, y_r).item()\n",
    "            mse  = nn.MSELoss()(out, y_r).item()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        mae_acc.append(mae); mse_acc.append(mse)\n",
    "    return sum(mae_acc)/len(mae_acc), sum(mse_acc)/len(mse_acc)\n",
    "\n",
    "for ep in range(cfg['epochs']):\n",
    "    tr_mae, tr_mse = epoch_loop(train_loader, True)\n",
    "    vl_mae, vl_mse = epoch_loop(val_loader,   False)\n",
    "\n",
    "    metric = vl_mae if cfg['sched_metric']=='val_mae' else vl_mse\n",
    "    scheduler.step(metric)\n",
    "\n",
    "    print(\"[{:03d}/{:03d}]  \"\n",
    "          \"train MAE {:.4f} | val MAE {:.4f}  ||  \"\n",
    "          \"train MSE {:.4f} | val MSE {:.4f}\".format(\n",
    "              ep+1, cfg['epochs'], tr_mae, vl_mae, tr_mse, vl_mse))\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 5) ── save everything needed to resume\n",
    "# ================================================================\n",
    "ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optim_state': optimizer.state_dict(),\n",
    "    'sched_state': scheduler.state_dict(),\n",
    "    'cfg'        : cfg,\n",
    "}\n",
    "ckpt_name = \"ckpt_{}.pt\".format(cfg['runid'])\n",
    "#torch.save(ckpt, ckpt_name)\n",
    "print(\"Saved checkpoint:\", ckpt_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_210152\n",
      "{'dim': 12, 'basis': 6, 'depth': 2, 'hidden_dim': 4, 'num_neighbors': 8, 'dropout': 0.02, 'norm_coors': True, 'N_NEIGHBORS': 100, 'aggregator': 'linear', 'use_rbf': True, 'use_attn': True, 'use_nconv': False, 'use_pred_head': True, 'loss_type': 'mae', 'sched_metric': 'val_mae', 'study_metrics': True, 'lr': 0.005, 'epochs': 5, 'batch_size': 1, 'device': 'cpu', 'runid': '20250726_210152'}\n",
      "Trainable parameters: 17,695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] train MAE 1.1944 | val MAE 1.2649  ||  train MSE 2.9880 | val MSE 3.8912\n",
      "[2/5] train MAE 1.0796 | val MAE 1.2528  ||  train MSE 2.6411 | val MSE 3.8555\n",
      "[3/5] train MAE 1.0631 | val MAE 1.2542  ||  train MSE 2.5918 | val MSE 3.9007\n",
      "[4/5] train MAE 1.0472 | val MAE 1.2357  ||  train MSE 2.5451 | val MSE 3.7295\n",
      "[5/5] train MAE 1.0221 | val MAE 1.2382  ||  train MSE 2.4487 | val MSE 3.8262\n",
      "Saved checkpoint: ckpt_20250726_210152.pt\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 1) ── model definition  (uses your *unchanged* architecture.py)\n",
    "# ================================================================\n",
    "import torch, torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "\n",
    "class Cfg(dict):\n",
    "    \"\"\"dot‑access + dict‑access wrapper (Py 3.6 safe).\"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    def as_dict(self):               # for checkpoints\n",
    "        return dict(self)\n",
    "cfg = Cfg(\n",
    "    # ------------- backbone -------------\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4,\n",
    "    num_neighbors=8,          # k in EGNN\n",
    "    dropout=0.02,\n",
    "    norm_coors=True,          # <── NEW: make it tunable\n",
    "    N_NEIGHBORS=100,\n",
    "\n",
    "    # ---------- ablation switches -------\n",
    "    aggregator='linear', use_rbf=True, use_attn=True,\n",
    "    use_nconv=False, use_pred_head=True,\n",
    "\n",
    "    # ---------- training / misc ---------\n",
    "\n",
    "    loss_type='mae',          # 'mae' or 'mse'  ← primary loss\n",
    "    sched_metric='val_mae',   # what ReduceLRO sees\n",
    "    study_metrics=True,       # <── NEW: if False we skip the secondary metric\n",
    "\n",
    "    lr=5e-3, epochs=5, batch_size=1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "print(cfg)\n",
    "\n",
    "# ================================================================\n",
    "# 0) ── hyper‑parameters / switches  (edit here, nothing else)\n",
    "\n",
    "\n",
    "class ProteinModel(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(ProteinModel, self).__init__()\n",
    "        self.c = c\n",
    "\n",
    "        # ---------------- EGNN backbone ----------------\n",
    "        self.egnn = StackedEGNN(\n",
    "    dim               = cfg.dim,\n",
    "    depth             = cfg.depth,\n",
    "    hidden_dim        = cfg.hidden_dim,\n",
    "    dropout           = cfg.dropout,\n",
    "    num_positions     = cfg.N_NEIGHBORS,\n",
    "    num_tokens        = 98,\n",
    "    num_nearest_neighbors = cfg.num_neighbors,\n",
    "    norm_coors        = cfg.norm_coors     # <── ONLY extra argument\n",
    ").to(cfg.device)\n",
    "\n",
    "        # --------------- optional blocks ----------------\n",
    "        self.rbf  = TunableBlock(\n",
    "            LearnableRBF(num_basis=c['basis'], cutoff=10.0).to(c['device']),\n",
    "            enabled=c['use_rbf']\n",
    "        )\n",
    "        self.attn = TunableBlock(\n",
    "            AttentionBlock(embed_dim=c['dim']+c['basis'],\n",
    "                           num_heads=c['dim']+c['basis'],\n",
    "                           hidden_dim=c['hidden_dim']).to(c['device']),\n",
    "            enabled=c['use_attn']\n",
    "        )\n",
    "\n",
    "        if c['aggregator'] in ('nconv', 'nconv+linear'):\n",
    "            k = c['dim'] + c['basis']\n",
    "            self.nconv = nn.Conv1d(c['N_NEIGHBORS'], 1, kernel_size=k, padding=0)\\\n",
    "                             .to(c['device'])\n",
    "            out_dim = 1\n",
    "        else:\n",
    "            self.nconv = None\n",
    "            out_dim = c['dim'] + c['basis']\n",
    "\n",
    "        self.pred_head = (nn.Linear(out_dim, 1).to(c['device'])\n",
    "                          if c['use_pred_head'] else nn.Identity())\n",
    "\n",
    "        # ---- protein‑level EGNN on centroids (update_coors = True) ----\n",
    "        self.prot_egnn = TunableBlock(\n",
    "            EGNN(dim=1, update_coors=True, num_nearest_neighbors=3)\\\n",
    "                .to(c['device']),\n",
    "            enabled=True\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    def forward(self, z, x):                 # z:(R,N)  x:(R,N,3)\n",
    "        h_list, coords = self.egnn(z, x)      # h:(R,N,dim)\n",
    "        h = h_list[0] if isinstance(h_list,(list,tuple)) else h_list\n",
    "\n",
    "        centroids = coords.mean(dim=1).unsqueeze(1)            # (R,1,3)\n",
    "        rbf = self.rbf(centroids, coords)                      # or passthrough\n",
    "\n",
    "        h_T = h.transpose(1,2)                                 # (R,dim,N)\n",
    "        if self.c['use_rbf']:\n",
    "            r_T = rbf.transpose(1,2)\n",
    "        else:\n",
    "            r_T = torch.empty(0, device=h.device)\n",
    "\n",
    "        tok = torch.cat((r_T, h_T), 1)                         # (R,dim+basis,N)\n",
    "\n",
    "        tok = tok.permute(2,0,1)                               # (N,R,C)\n",
    "        tok,_ = self.attn(tok)                                 # (N,R,C) / identity\n",
    "        tok = tok.permute(1,0,2)                               # (R,N,C)\n",
    "\n",
    "        # -------- aggregator routes --------\n",
    "        if self.nconv is not None:\n",
    "            tok = self.nconv(tok).squeeze(-1)                  # (R,1)\n",
    "        elif self.c['aggregator'] == 'pool':                   # max‑pool\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "        else:                                                  # linear\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "\n",
    "        preds = self.pred_head(tok)                            # (R,1)\n",
    "\n",
    "        # ---- residue → protein aggregate -----\n",
    "        preds_ = preds.unsqueeze(0)                            # (1,R,1)\n",
    "        coords_ = centroids.permute(1,0,2)                     # (1,R,3)\n",
    "        preds  = self.prot_egnn(preds_, coords_)[0].squeeze(0) # (R,1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "# ── make sure cfg behaves like a dict no matter what it is\n",
    "if not isinstance(cfg, dict):\n",
    "    cfg = vars(cfg)          # SimpleNamespace → ordinary dict\n",
    "\n",
    "model = ProteinModel(cfg)\n",
    "print(\"Trainable parameters:\", \"{:,}\".format(model.n_params()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:5], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "# =====================================================================\n",
    "# 2) dataset / dataloader – *identical* to your earlier code\n",
    "# =====================================================================\n",
    "N_NEIGHBORS = cfg.N_NEIGHBORS      # just to keep names aligned\n",
    "# ...  (pad_collate, InMemoryHoodDataset)  ...\n",
    "\n",
    "train_loader = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "val_loader   = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3) ── optimisation bits\n",
    "# ================================================================\n",
    "criterion = nn.L1Loss() if cfg['loss_type']=='mae' else nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "scaler = GradScaler(enabled=(cfg['device']=='cuda'))\n",
    "# pick loss & the “other” metric once, outside the loop\n",
    "if cfg.loss_type.lower() == 'mae':\n",
    "    primary_loss_fn   = nn.L1Loss()\n",
    "    secondary_fn      = nn.MSELoss() if cfg.study_metrics else None\n",
    "    primary_name      = 'MAE'\n",
    "else:\n",
    "    primary_loss_fn   = nn.MSELoss()\n",
    "    secondary_fn      = nn.L1Loss()  if cfg.study_metrics else None\n",
    "    primary_name      = 'MSE'\n",
    "\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    primary_sum, secondary_sum, n = 0.0, 0.0, 0\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg.device)\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg.device)\n",
    "        y_r   = y.view(-1)[valid].to(cfg.device)\n",
    "\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            preds   = model(z_r, x_r).flatten()\n",
    "            loss    = primary_loss_fn(preds, y_r)\n",
    "            primary = loss.item()\n",
    "            if secondary_fn is not None:\n",
    "                secondary = secondary_fn(preds, y_r).item()\n",
    "            else:\n",
    "                secondary = 0.0\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        primary_sum   += primary\n",
    "        secondary_sum += secondary\n",
    "        n += 1\n",
    "\n",
    "    return primary_sum/n, (secondary_sum/n if secondary_fn is not None else None)\n",
    "\n",
    "# ================================================================\n",
    "# 4) ── train / validate\n",
    "# ================================================================\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    mae_acc, mse_acc = [], []\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg['device'])\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg['device'])\n",
    "        y_r   = y.view(-1)[valid].to(cfg['device'])\n",
    "\n",
    "        with autocast(enabled=(cfg['device']=='cuda')):\n",
    "            out  = model(z_r, x_r).flatten()\n",
    "            loss = criterion(out, y_r)\n",
    "            mae  = nn.L1Loss()(out, y_r).item()\n",
    "            mse  = nn.MSELoss()(out, y_r).item()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        mae_acc.append(mae); mse_acc.append(mse)\n",
    "    return sum(mae_acc)/len(mae_acc), sum(mse_acc)/len(mse_acc)\n",
    "\n",
    "for ep in range(cfg.epochs):\n",
    "    tr_primary, tr_sec = epoch_loop(train_loader, True)\n",
    "    vl_primary, vl_sec = epoch_loop(val_loader, False)\n",
    "\n",
    "    scheduler.step(vl_primary if cfg.sched_metric=='val_'+primary_name.lower()\n",
    "                   else vl_sec)\n",
    "\n",
    "    msg = \"[{}/{}] train {} {:.4f} | val {} {:.4f}\".format(\n",
    "            ep+1, cfg.epochs, primary_name, tr_primary, primary_name, vl_primary)\n",
    "    if cfg.study_metrics and tr_sec is not None:\n",
    "        other = 'MSE' if primary_name=='MAE' else 'MAE'\n",
    "        msg += \"  ||  train {} {:.4f} | val {} {:.4f}\".format(\n",
    "                other, tr_sec, other, vl_sec)\n",
    "    print(msg)\n",
    "\n",
    "# ================================================================\n",
    "# 5) ── save everything needed to resume\n",
    "# ================================================================\n",
    "ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optim_state': optimizer.state_dict(),\n",
    "    'sched_state': scheduler.state_dict(),\n",
    "    'cfg'        : cfg,\n",
    "}\n",
    "ckpt_name = \"ckpt_{}.pt\".format(cfg['runid'])\n",
    "#torch.save(ckpt, ckpt_name)\n",
    "print(\"Saved checkpoint:\", ckpt_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_210254\n",
      "{'dim': 12, 'basis': 6, 'depth': 2, 'hidden_dim': 4, 'num_neighbors': 8, 'dropout': 0.02, 'norm_coors': True, 'N_NEIGHBORS': 100, 'aggregator': 'linear', 'use_rbf': True, 'use_attn': True, 'use_nconv': False, 'use_pred_head': True, 'loss_type': 'mae', 'sched_metric': 'val_mae', 'study_metrics': True, 'lr': 0.005, 'epochs': 5, 'batch_size': 1, 'device': 'cpu', 'runid': '20250726_210254'}\n",
      "Trainable parameters: 17,695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] train MAE 1.2831 | val MAE 1.5003  ||  train MSE 3.1863 | val MSE 4.8607\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fe285a7c96ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mtr_primary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_sec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0mvl_primary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvl_sec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fe285a7c96ff>\u001b[0m in \u001b[0;36mepoch_loop\u001b[0;34m(loader, train)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 1) ── model definition  (uses your *unchanged* architecture.py)\n",
    "# ================================================================\n",
    "import torch, torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "\n",
    "class Cfg(dict):\n",
    "    \"\"\"dot‑access + dict‑access wrapper (Py 3.6 safe).\"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    def as_dict(self):               # for checkpoints\n",
    "        return dict(self)\n",
    "cfg = Cfg(\n",
    "    # ------------- backbone -------------\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4,\n",
    "    num_neighbors=8,          # k in EGNN\n",
    "    dropout=0.02,\n",
    "    norm_coors=True,          # <── NEW: make it tunable\n",
    "    N_NEIGHBORS=100,\n",
    "\n",
    "    # ---------- ablation switches -------\n",
    "    aggregator='linear', use_rbf=True, use_attn=True,\n",
    "    use_nconv=False, use_pred_head=True,\n",
    "\n",
    "    # ---------- training / misc ---------\n",
    "\n",
    "    loss_type='mae',          # 'mae' or 'mse'  ← primary loss\n",
    "    sched_metric='val_mae',   # what ReduceLRO sees\n",
    "    study_metrics=True,       # <── NEW: if False we skip the secondary metric\n",
    "\n",
    "    lr=5e-3, epochs=5, batch_size=1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "print(cfg)\n",
    "\n",
    "# ================================================================\n",
    "# 0) ── hyper‑parameters / switches  (edit here, nothing else)\n",
    "\n",
    "\n",
    "class ProteinModel(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(ProteinModel, self).__init__()\n",
    "        self.c = c\n",
    "\n",
    "        # ---------------- EGNN backbone ----------------\n",
    "        self.egnn = StackedEGNN(\n",
    "    dim               = cfg.dim,\n",
    "    depth             = cfg.depth,\n",
    "    hidden_dim        = cfg.hidden_dim,\n",
    "    dropout           = cfg.dropout,\n",
    "    num_positions     = cfg.N_NEIGHBORS,\n",
    "    num_tokens        = 98,\n",
    "    num_nearest_neighbors = cfg.num_neighbors,\n",
    "    norm_coors        = cfg.norm_coors     # <── ONLY extra argument\n",
    ").to(cfg.device)\n",
    "\n",
    "        # --------------- optional blocks ----------------\n",
    "        self.rbf  = TunableBlock(\n",
    "            LearnableRBF(num_basis=c['basis'], cutoff=10.0).to(c['device']),\n",
    "            enabled=c['use_rbf']\n",
    "        )\n",
    "        self.attn = TunableBlock(\n",
    "            AttentionBlock(embed_dim=c['dim']+c['basis'],\n",
    "                           num_heads=c['dim']+c['basis'],\n",
    "                           hidden_dim=c['hidden_dim']).to(c['device']),\n",
    "            enabled=c['use_attn']\n",
    "        )\n",
    "\n",
    "        if c['aggregator'] in ('nconv', 'nconv+linear'):\n",
    "            k = c['dim'] + c['basis']\n",
    "            self.nconv = nn.Conv1d(c['N_NEIGHBORS'], 1, kernel_size=k, padding=0)\\\n",
    "                             .to(c['device'])\n",
    "            out_dim = 1\n",
    "        else:\n",
    "            self.nconv = None\n",
    "            out_dim = c['dim'] + c['basis']\n",
    "\n",
    "        self.pred_head = (nn.Linear(out_dim, 1).to(c['device'])\n",
    "                          if c['use_pred_head'] else nn.Identity())\n",
    "\n",
    "        # ---- protein‑level EGNN on centroids (update_coors = True) ----\n",
    "        self.prot_egnn = TunableBlock(\n",
    "            EGNN(dim=1, update_coors=True, num_nearest_neighbors=3)\\\n",
    "                .to(c['device']),\n",
    "            enabled=True\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    def forward(self, z, x):                 # z:(R,N)  x:(R,N,3)\n",
    "        h_list, coords = self.egnn(z, x)      # h:(R,N,dim)\n",
    "        h = h_list[0] if isinstance(h_list,(list,tuple)) else h_list\n",
    "\n",
    "        centroids = coords.mean(dim=1).unsqueeze(1)            # (R,1,3)\n",
    "        rbf = self.rbf(centroids, coords)                      # or passthrough\n",
    "\n",
    "        h_T = h.transpose(1,2)                                 # (R,dim,N)\n",
    "        if self.c['use_rbf']:\n",
    "            r_T = rbf.transpose(1,2)\n",
    "        else:\n",
    "            r_T = torch.empty(0, device=h.device)\n",
    "\n",
    "        tok = torch.cat((r_T, h_T), 1)                         # (R,dim+basis,N)\n",
    "\n",
    "        tok = tok.permute(2,0,1)                               # (N,R,C)\n",
    "        tok,_ = self.attn(tok)                                 # (N,R,C) / identity\n",
    "        tok = tok.permute(1,0,2)                               # (R,N,C)\n",
    "\n",
    "        # -------- aggregator routes --------\n",
    "        if self.nconv is not None:\n",
    "            tok = self.nconv(tok).squeeze(-1)                  # (R,1)\n",
    "        elif self.c['aggregator'] == 'pool':                   # max‑pool\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "        else:                                                  # linear\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "\n",
    "        preds = self.pred_head(tok)                            # (R,1)\n",
    "\n",
    "        # ---- residue → protein aggregate -----\n",
    "        preds_ = preds.unsqueeze(0)                            # (1,R,1)\n",
    "        coords_ = centroids.permute(1,0,2)                     # (1,R,3)\n",
    "        preds  = self.prot_egnn(preds_, coords_)[0].squeeze(0) # (R,1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "# ── make sure cfg behaves like a dict no matter what it is\n",
    "if not isinstance(cfg, dict):\n",
    "    cfg = vars(cfg)          # SimpleNamespace → ordinary dict\n",
    "\n",
    "model = ProteinModel(cfg)\n",
    "print(\"Trainable parameters:\", \"{:,}\".format(model.n_params()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:5], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "# =====================================================================\n",
    "# 2) dataset / dataloader – *identical* to your earlier code\n",
    "# =====================================================================\n",
    "N_NEIGHBORS = cfg.N_NEIGHBORS      # just to keep names aligned\n",
    "# ...  (pad_collate, InMemoryHoodDataset)  ...\n",
    "\n",
    "train_loader = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "val_loader   = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3) ── optimisation bits\n",
    "# ================================================================\n",
    "criterion = nn.L1Loss() if cfg['loss_type']=='mae' else nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "scaler = GradScaler(enabled=(cfg['device']=='cuda'))\n",
    "# pick loss & the “other” metric once, outside the loop\n",
    "if cfg.loss_type.lower() == 'mae':\n",
    "    primary_loss_fn   = nn.L1Loss()\n",
    "    secondary_fn      = nn.MSELoss() if cfg.study_metrics else None\n",
    "    primary_name      = 'MAE'\n",
    "else:\n",
    "    primary_loss_fn   = nn.MSELoss()\n",
    "    secondary_fn      = nn.L1Loss()  if cfg.study_metrics else None\n",
    "    primary_name      = 'MSE'\n",
    "\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    primary_sum, secondary_sum, n = 0.0, 0.0, 0\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg.device)\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg.device)\n",
    "        y_r   = y.view(-1)[valid].to(cfg.device)\n",
    "\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            preds   = model(z_r, x_r).flatten()\n",
    "            loss    = primary_loss_fn(preds, y_r)\n",
    "            primary = loss.item()\n",
    "            if secondary_fn is not None:\n",
    "                secondary = secondary_fn(preds, y_r).item()\n",
    "            else:\n",
    "                secondary = 0.0\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        primary_sum   += primary\n",
    "        secondary_sum += secondary\n",
    "        n += 1\n",
    "\n",
    "    return primary_sum/n, (secondary_sum/n if secondary_fn is not None else None)\n",
    "\n",
    "# ================================================================\n",
    "# 4) ── train / validate\n",
    "# ================================================================\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    mae_acc, mse_acc = [], []\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg['device'])\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg['device'])\n",
    "        y_r   = y.view(-1)[valid].to(cfg['device'])\n",
    "\n",
    "        with autocast(enabled=(cfg['device']=='cuda')):\n",
    "            out  = model(z_r, x_r).flatten()\n",
    "            loss = criterion(out, y_r)\n",
    "            mae  = nn.L1Loss()(out, y_r).item()\n",
    "            mse  = nn.MSELoss()(out, y_r).item()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        mae_acc.append(mae); mse_acc.append(mse)\n",
    "    return sum(mae_acc)/len(mae_acc), sum(mse_acc)/len(mse_acc)\n",
    "\n",
    "for ep in range(cfg.epochs):\n",
    "    tr_primary, tr_sec = epoch_loop(train_loader, True)\n",
    "    vl_primary, vl_sec = epoch_loop(val_loader, False)\n",
    "\n",
    "    scheduler.step(vl_primary if cfg.sched_metric=='val_'+primary_name.lower()\n",
    "                   else vl_sec)\n",
    "\n",
    "    msg = \"[{}/{}] train {} {:.4f} | val {} {:.4f}\".format(\n",
    "            ep+1, cfg.epochs, primary_name, tr_primary, primary_name, vl_primary)\n",
    "    if cfg.study_metrics and tr_sec is not None:\n",
    "        other = 'MSE' if primary_name=='MAE' else 'MAE'\n",
    "        msg += \"  ||  train {} {:.4f} | val {} {:.4f}\".format(\n",
    "                other, tr_sec, other, vl_sec)\n",
    "    print(msg)\n",
    "\n",
    "# ================================================================\n",
    "# 5) ── save everything needed to resume\n",
    "# ================================================================\n",
    "ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optim_state': optimizer.state_dict(),\n",
    "    'sched_state': scheduler.state_dict(),\n",
    "    'cfg'        : cfg,\n",
    "}\n",
    "ckpt_name = \"ckpt_{}.pt\".format(cfg['runid'])\n",
    "#torch.save(ckpt, ckpt_name)\n",
    "print(\"Saved checkpoint:\", ckpt_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 1) ── model definition  (uses your *unchanged* architecture.py)\n",
    "# ================================================================\n",
    "import torch, torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "\n",
    "class Cfg(dict):\n",
    "    \"\"\"dot‑access + dict‑access wrapper (Py 3.6 safe).\"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    def as_dict(self):               # for checkpoints\n",
    "        return dict(self)\n",
    "cfg = Cfg(\n",
    "    # ------------- backbone -------------\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4,\n",
    "    num_neighbors=8,          # k in EGNN\n",
    "    dropout=0.02,\n",
    "    norm_coors=True,          # <── NEW: make it tunable\n",
    "    N_NEIGHBORS=100,\n",
    "\n",
    "    # ---------- ablation switches -------\n",
    "    aggregator='linear', use_rbf=True, use_attn=True,\n",
    "    use_nconv=False, use_pred_head=True,\n",
    "\n",
    "    # ---------- training / misc ---------\n",
    "\n",
    "    loss_type='mae',          # 'mae' or 'mse'  ← primary loss\n",
    "    sched_metric='val_mae',   # what ReduceLRO sees\n",
    "    study_metrics=True,       # <── NEW: if False we skip the secondary metric\n",
    "\n",
    "    lr=5e-3, epochs=5, batch_size=1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "print(cfg)\n",
    "\n",
    "# ================================================================\n",
    "# 0) ── hyper‑parameters / switches  (edit here, nothing else)\n",
    "\n",
    "\n",
    "class ProteinModel(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(ProteinModel, self).__init__()\n",
    "        self.c = c\n",
    "\n",
    "        # ---------------- EGNN backbone ----------------\n",
    "        self.egnn = StackedEGNN(\n",
    "    dim               = cfg.dim,\n",
    "    depth             = cfg.depth,\n",
    "    hidden_dim        = cfg.hidden_dim,\n",
    "    dropout           = cfg.dropout,\n",
    "    num_positions     = cfg.N_NEIGHBORS,\n",
    "    num_tokens        = 98,\n",
    "    num_nearest_neighbors = cfg.num_neighbors,\n",
    "    norm_coors        = cfg.norm_coors     # <── ONLY extra argument\n",
    ").to(cfg.device)\n",
    "\n",
    "        # --------------- optional blocks ----------------\n",
    "        self.rbf  = TunableBlock(\n",
    "            LearnableRBF(num_basis=c['basis'], cutoff=10.0).to(c['device']),\n",
    "            enabled=c['use_rbf']\n",
    "        )\n",
    "        self.attn = TunableBlock(\n",
    "            AttentionBlock(embed_dim=c['dim']+c['basis'],\n",
    "                           num_heads=c['dim']+c['basis'],\n",
    "                           hidden_dim=c['hidden_dim']).to(c['device']),\n",
    "            enabled=c['use_attn']\n",
    "        )\n",
    "\n",
    "        if c['aggregator'] in ('nconv', 'nconv+linear'):\n",
    "            k = c['dim'] + c['basis']\n",
    "            self.nconv = nn.Conv1d(c['N_NEIGHBORS'], 1, kernel_size=k, padding=0)\\\n",
    "                             .to(c['device'])\n",
    "            out_dim = 1\n",
    "        else:\n",
    "            self.nconv = None\n",
    "            out_dim = c['dim'] + c['basis']\n",
    "\n",
    "        self.pred_head = (nn.Linear(out_dim, 1).to(c['device'])\n",
    "                          if c['use_pred_head'] else nn.Identity())\n",
    "\n",
    "        # ---- protein‑level EGNN on centroids (update_coors = True) ----\n",
    "        self.prot_egnn = TunableBlock(\n",
    "            EGNN(dim=1, update_coors=True, num_nearest_neighbors=3)\\\n",
    "                .to(c['device']),\n",
    "            enabled=True\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    def forward(self, z, x):                 # z:(R,N)  x:(R,N,3)\n",
    "        h_list, coords = self.egnn(z, x)      # h:(R,N,dim)\n",
    "        h = h_list[0] if isinstance(h_list,(list,tuple)) else h_list\n",
    "\n",
    "        centroids = coords.mean(dim=1).unsqueeze(1)            # (R,1,3)\n",
    "        rbf = self.rbf(centroids, coords)                      # or passthrough\n",
    "\n",
    "        h_T = h.transpose(1,2)                                 # (R,dim,N)\n",
    "        if self.c['use_rbf']:\n",
    "            r_T = rbf.transpose(1,2)\n",
    "        else:\n",
    "            r_T = torch.empty(0, device=h.device)\n",
    "\n",
    "        tok = torch.cat((r_T, h_T), 1)                         # (R,dim+basis,N)\n",
    "\n",
    "        tok = tok.permute(2,0,1)                               # (N,R,C)\n",
    "        tok,_ = self.attn(tok)                                 # (N,R,C) / identity\n",
    "        tok = tok.permute(1,0,2)                               # (R,N,C)\n",
    "\n",
    "        # -------- aggregator routes --------\n",
    "        if self.nconv is not None:\n",
    "            tok = self.nconv(tok).squeeze(-1)                  # (R,1)\n",
    "        elif self.c['aggregator'] == 'pool':                   # max‑pool\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "        else:                                                  # linear\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "\n",
    "        preds = self.pred_head(tok)                            # (R,1)\n",
    "\n",
    "        # ---- residue → protein aggregate -----\n",
    "        preds_ = preds.unsqueeze(0)                            # (1,R,1)\n",
    "        coords_ = centroids.permute(1,0,2)                     # (1,R,3)\n",
    "        preds  = self.prot_egnn(preds_, coords_)[0].squeeze(0) # (R,1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "# ── make sure cfg behaves like a dict no matter what it is\n",
    "if not isinstance(cfg, dict):\n",
    "    cfg = vars(cfg)          # SimpleNamespace → ordinary dict\n",
    "\n",
    "model = ProteinModel(cfg)\n",
    "print(\"Trainable parameters:\", \"{:,}\".format(model.n_params()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:5], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "# =====================================================================\n",
    "# 2) dataset / dataloader – *identical* to your earlier code\n",
    "# =====================================================================\n",
    "N_NEIGHBORS = cfg.N_NEIGHBORS      # just to keep names aligned\n",
    "# ...  (pad_collate, InMemoryHoodDataset)  ...\n",
    "\n",
    "train_loader = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "val_loader   = DataLoader( ... , batch_size=cfg.batch_size, collate_fn=pad_collate)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 3) ── optimisation bits\n",
    "# ================================================================\n",
    "criterion = nn.L1Loss() if cfg['loss_type']=='mae' else nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "scaler = GradScaler(enabled=(cfg['device']=='cuda'))\n",
    "# pick loss & the “other” metric once, outside the loop\n",
    "if cfg.loss_type.lower() == 'mae':\n",
    "    primary_loss_fn   = nn.L1Loss()\n",
    "    secondary_fn      = nn.MSELoss() if cfg.study_metrics else None\n",
    "    primary_name      = 'MAE'\n",
    "else:\n",
    "    primary_loss_fn   = nn.MSELoss()\n",
    "    secondary_fn      = nn.L1Loss()  if cfg.study_metrics else None\n",
    "    primary_name      = 'MSE'\n",
    "\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    primary_sum, secondary_sum, n = 0.0, 0.0, 0\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg.device)\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg.device)\n",
    "        y_r   = y.view(-1)[valid].to(cfg.device)\n",
    "\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            preds   = model(z_r, x_r).flatten()\n",
    "            loss    = primary_loss_fn(preds, y_r)\n",
    "            primary = loss.item()\n",
    "            if secondary_fn is not None:\n",
    "                secondary = secondary_fn(preds, y_r).item()\n",
    "            else:\n",
    "                secondary = 0.0\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        primary_sum   += primary\n",
    "        secondary_sum += secondary\n",
    "        n += 1\n",
    "\n",
    "    return primary_sum/n, (secondary_sum/n if secondary_fn is not None else None)\n",
    "\n",
    "# ================================================================\n",
    "# 4) ── train / validate\n",
    "# ================================================================\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    mae_acc, mse_acc = [], []\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg['device'])\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg['device'])\n",
    "        y_r   = y.view(-1)[valid].to(cfg['device'])\n",
    "\n",
    "        with autocast(enabled=(cfg['device']=='cuda')):\n",
    "            out  = model(z_r, x_r).flatten()\n",
    "            loss = criterion(out, y_r)\n",
    "            mae  = nn.L1Loss()(out, y_r).item()\n",
    "            mse  = nn.MSELoss()(out, y_r).item()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        mae_acc.append(mae); mse_acc.append(mse)\n",
    "    return sum(mae_acc)/len(mae_acc), sum(mse_acc)/len(mse_acc)\n",
    "\n",
    "for ep in range(cfg.epochs):\n",
    "    tr_primary, tr_sec = epoch_loop(train_loader, True)\n",
    "    vl_primary, vl_sec = epoch_loop(val_loader, False)\n",
    "\n",
    "    scheduler.step(vl_primary if cfg.sched_metric=='val_'+primary_name.lower()\n",
    "                   else vl_sec)\n",
    "\n",
    "    msg = \"[{}/{}] train {} {:.4f} | val {} {:.4f}\".format(\n",
    "            ep+1, cfg.epochs, primary_name, tr_primary, primary_name, vl_primary)\n",
    "    if cfg.study_metrics and tr_sec is not None:\n",
    "        other = 'MSE' if primary_name=='MAE' else 'MAE'\n",
    "        msg += \"  ||  train {} {:.4f} | val {} {:.4f}\".format(\n",
    "                other, tr_sec, other, vl_sec)\n",
    "    print(msg)\n",
    "\n",
    "# ================================================================\n",
    "# 5) ── save everything needed to resume\n",
    "# ================================================================\n",
    "ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optim_state': optimizer.state_dict(),\n",
    "    'sched_state': scheduler.state_dict(),\n",
    "    'cfg'        : cfg,\n",
    "}\n",
    "ckpt_name = \"ckpt_{}.pt\".format(cfg['runid'])\n",
    "#torch.save(ckpt, ckpt_name)\n",
    "print(\"Saved checkpoint:\", ckpt_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'norm_coors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c677cb099779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProteinModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainable params :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{model.n_trainable_parameters():,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c677cb099779>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_NEIGHBORS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mnum_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m118\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nearest_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         ).to(cfg.device)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'norm_coors'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
