{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_214548\n",
      "{'dim': 12, 'basis': 6, 'depth': 2, 'hidden_dim': 4, 'num_neighbors': 8, 'dropout': 0.02, 'norm_coors': True, 'N_NEIGHBORS': 100, 'aggregator': 'linear', 'use_rbf': True, 'use_attn': True, 'use_nconv': False, 'use_pred_head': True, 'loss_type': 'mae', 'sched_metric': 'val_mae', 'study_metrics': True, 'lr': 0.005, 'epochs': 5, 'batch_size': 1, 'device': 'cpu', 'runid': '20250726_214548', 'seed': 0, 'save_attn': False, 'num_paths': 2, 'split_mode': 'random', 'split_ratio': 0.8, 'split_seed': 0, 'split_files': {'train': 'train_list.txt', 'val': 'val_list.txt'}}\n",
      "Trainable parameters: 17,695\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ac215d7b4472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0mtr_primary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_sec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0mvl_primary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvl_sec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ac215d7b4472>\u001b[0m in \u001b[0;36mepoch_loop\u001b[0;34m(loader, train)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0mmae_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mz_r\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 1) ── model definition  (uses your *unchanged* architecture.py)\n",
    "# ================================================================\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Cfg(dict):\n",
    "    \"\"\"dot‑access + dict‑access wrapper (Py 3.6 safe).\"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    def as_dict(self):               # for checkpoints\n",
    "        return dict(self)\n",
    "cfg = Cfg(\n",
    "    # ------------- backbone -------------\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4,\n",
    "    num_neighbors=8,          # k in EGNN\n",
    "    dropout=0.02,\n",
    "    norm_coors=True,          # <── NEW: make it tunable\n",
    "    N_NEIGHBORS=100,\n",
    "\n",
    "    # ---------- ablation switches -------\n",
    "    aggregator='linear', use_rbf=True, use_attn=True,\n",
    "    use_nconv=False, use_pred_head=True,\n",
    "\n",
    "    # ---------- training / misc ---------\n",
    "\n",
    "    loss_type='mae',          # 'mae' or 'mse'  ← primary loss\n",
    "    sched_metric='val_mae',   # what ReduceLRO sees\n",
    "    study_metrics=True,       # <── NEW: if False we skip the secondary metric\n",
    "\n",
    "    lr=5e-3, epochs=5, batch_size=1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "\n",
    "    seed        = 0,       # ← set to int for deterministic run, None for random\n",
    "    save_attn   = False,    # ← flip to True when you need the weights\n",
    "\n",
    "    num_paths = 2,\n",
    "    split_mode  = 'random',     # 'random' | 'file'\n",
    "    split_ratio = 0.8,          # used when split_mode == 'random'\n",
    "    split_seed  = 0,          # reproducible shuffle\n",
    "    split_files = {             # used when split_mode == 'file'\n",
    "        'train': 'train_list.txt',\n",
    "        'val'  : 'val_list.txt',\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "PIN_MEMORY=False\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "print(cfg)\n",
    "# ── reproducibility (incl. cuDNN, DataLoader shuffles, etc.)\n",
    "if cfg.seed is not None:\n",
    "    import random, os\n",
    "    random.seed(cfg.seed);  np.random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(cfg.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 0) ── hyper‑parameters / switches  (edit here, nothing else)\n",
    "\n",
    "\n",
    "class ProteinModel(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(ProteinModel, self).__init__()\n",
    "        self.c = c\n",
    "\n",
    "        # ---------------- EGNN backbone ----------------\n",
    "        self.egnn = StackedEGNN(\n",
    "    dim               = cfg.dim,\n",
    "    depth             = cfg.depth,\n",
    "    hidden_dim        = cfg.hidden_dim,\n",
    "    dropout           = cfg.dropout,\n",
    "    num_positions     = cfg.N_NEIGHBORS,\n",
    "    num_tokens        = 98,\n",
    "    num_nearest_neighbors = cfg.num_neighbors,\n",
    "    norm_coors        = cfg.norm_coors     # <── ONLY extra argument\n",
    ").to(cfg.device)\n",
    "\n",
    "        # --------------- optional blocks ----------------\n",
    "        self.rbf  = TunableBlock(\n",
    "            LearnableRBF(num_basis=c['basis'], cutoff=10.0).to(c['device']),\n",
    "            enabled=c['use_rbf']\n",
    "        )\n",
    "        self.attn = TunableBlock(\n",
    "            AttentionBlock(embed_dim=c['dim']+c['basis'],\n",
    "                           num_heads=c['dim']+c['basis'],\n",
    "                           hidden_dim=c['hidden_dim']).to(c['device']),\n",
    "            enabled=c['use_attn']\n",
    "        )\n",
    "\n",
    "        if c['aggregator'] in ('nconv', 'nconv+linear'):\n",
    "            k = c['dim'] + c['basis']\n",
    "            self.nconv = nn.Conv1d(c['N_NEIGHBORS'], 1, kernel_size=k, padding=0)\\\n",
    "                             .to(c['device'])\n",
    "            out_dim = 1\n",
    "        else:\n",
    "            self.nconv = None\n",
    "            out_dim = c['dim'] + c['basis']\n",
    "\n",
    "        self.pred_head = (nn.Linear(out_dim, 1).to(c['device'])\n",
    "                          if c['use_pred_head'] else nn.Identity())\n",
    "\n",
    "        # ---- protein‑level EGNN on centroids (update_coors = True) ----\n",
    "        self.prot_egnn = TunableBlock(\n",
    "            EGNN(dim=1, update_coors=True, num_nearest_neighbors=3)\\\n",
    "                .to(c['device']),\n",
    "            enabled=True\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    def forward(self, z, x):                 # z:(R,N)  x:(R,N,3)\n",
    "        h_list, coords = self.egnn(z, x)      # h:(R,N,dim)\n",
    "        h = h_list[0] if isinstance(h_list,(list,tuple)) else h_list\n",
    "\n",
    "        centroids = coords.mean(dim=1).unsqueeze(1)            # (R,1,3)\n",
    "        rbf = self.rbf(centroids, coords)                      # or passthrough\n",
    "\n",
    "        h_T = h.transpose(1,2)                                 # (R,dim,N)\n",
    "        if self.c['use_rbf']:\n",
    "            r_T = rbf.transpose(1,2)\n",
    "        else:\n",
    "            r_T = torch.empty(0, device=h.device)\n",
    "\n",
    "        tok = torch.cat((r_T, h_T), 1)                         # (R,dim+basis,N)\n",
    "\n",
    "        tok = tok.permute(2,0,1)                               # (N,R,C)\n",
    "        tok,_ = self.attn(tok)                                 # (N,R,C) / identity\n",
    "        tok = tok.permute(1,0,2)                               # (R,N,C)\n",
    "\n",
    "        # -------- aggregator routes --------\n",
    "        if self.nconv is not None:\n",
    "            tok = self.nconv(tok).squeeze(-1)                  # (R,1)\n",
    "        elif self.c['aggregator'] == 'pool':                   # max‑pool\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "        else:                                                  # linear\n",
    "            tok = tok.max(dim=1).values                        # (R,C)\n",
    "\n",
    "        preds = self.pred_head(tok)                            # (R,1)\n",
    "\n",
    "        # ---- residue → protein aggregate -----\n",
    "        preds_ = preds.unsqueeze(0)                            # (1,R,1)\n",
    "        coords_ = centroids.permute(1,0,2)                     # (1,R,3)\n",
    "        preds  = self.prot_egnn(preds_, coords_)[0].squeeze(0) # (R,1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def n_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "# ── make sure cfg behaves like a dict no matter what it is\n",
    "if not isinstance(cfg, dict):\n",
    "    cfg = vars(cfg)          # SimpleNamespace → ordinary dict\n",
    "\n",
    "model = ProteinModel(cfg)\n",
    "print(\"Trainable parameters:\", \"{:,}\".format(model.n_params()))\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from train_utils import (set_seed, InMemoryHoodDataset,\n",
    "                         pad_collate, one_epoch, make_split)\n",
    "\n",
    "\n",
    "\n",
    "# ── reproducibility\n",
    "#set_seed(cfg.seed)\n",
    "# =====================================================================\n",
    "# 2) dataset / dataloader – *identical* to your earlier code\n",
    "# =====================================================================\n",
    "\n",
    "N_NEIGHBORS = cfg.N_NEIGHBORS      # just to keep names aligned\n",
    "BATCH_SIZE =cfg.batch_size\n",
    "PIN_MEMORY = False\n",
    "# ...  (pad_collate, InMemoryHoodDataset)  ...\n",
    "# ---------------------------------------------------------------\n",
    "# 2)  file list with optional cap + reproducible shuffle\n",
    "# ---------------------------------------------------------------\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "if cfg.split_mode == 'random' and cfg.num_paths is not None:\n",
    "    all_paths = all_paths[:cfg.num_paths]\n",
    "train_paths, val_paths = make_split(all_paths, cfg)\n",
    "\n",
    "\n",
    "# ── datasets and loaders\n",
    "train_ds = InMemoryHoodDataset(train_paths, n_neighbors=cfg.N_NEIGHBORS,\n",
    "                               pin_memory=False)\n",
    "val_ds   = InMemoryHoodDataset(val_paths,   n_neighbors=cfg.N_NEIGHBORS,\n",
    "                               pin_memory=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    collate_fn=lambda b: pad_collate(b, cfg.N_NEIGHBORS, cfg.device),\n",
    "    num_workers=0, pin_memory=False)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "    collate_fn=lambda b: pad_collate(b, cfg.N_NEIGHBORS, cfg.device),\n",
    "    num_workers=0, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# ================================================================\n",
    "# 3) ── optimisation bits\n",
    "# ================================================================\n",
    "criterion = nn.L1Loss() if cfg['loss_type']=='mae' else nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "scaler = GradScaler(enabled=(cfg['device']=='cuda'))\n",
    "# pick loss & the “other” metric once, outside the loop\n",
    "if cfg.loss_type.lower() == 'mae':\n",
    "    primary_loss_fn   = nn.L1Loss()\n",
    "    secondary_fn      = nn.MSELoss() if cfg.study_metrics else None\n",
    "    primary_name      = 'MAE'\n",
    "else:\n",
    "    primary_loss_fn   = nn.MSELoss()\n",
    "    secondary_fn      = nn.L1Loss()  if cfg.study_metrics else None\n",
    "    primary_name      = 'MSE'\n",
    "\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    primary_sum, secondary_sum, n = 0.0, 0.0, 0\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg.device)\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg.device)\n",
    "        y_r   = y.view(-1)[valid].to(cfg.device)\n",
    "\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            preds   = model(z_r, x_r).flatten()\n",
    "            loss    = primary_loss_fn(preds, y_r)\n",
    "            primary = loss.item()\n",
    "            if secondary_fn is not None:\n",
    "                secondary = secondary_fn(preds, y_r).item()\n",
    "            else:\n",
    "                secondary = 0.0\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        primary_sum   += primary\n",
    "        secondary_sum += secondary\n",
    "        n += 1\n",
    "\n",
    "    return primary_sum/n, (secondary_sum/n if secondary_fn is not None else None)\n",
    "\n",
    "# ================================================================\n",
    "# 4) ── train / validate\n",
    "# ================================================================\n",
    "def epoch_loop(loader, train):\n",
    "    if train: model.train()\n",
    "    else:     model.eval()\n",
    "\n",
    "    mae_acc, mse_acc = [], []\n",
    "    for z,x,y,mask in loader:\n",
    "        valid = mask.view(-1)\n",
    "        z_r   = z.view(-1, z.size(2))[valid].to(cfg['device'])\n",
    "        x_r   = x.view(-1, x.size(2), 3)[valid].to(cfg['device'])\n",
    "        y_r   = y.view(-1)[valid].to(cfg['device'])\n",
    "\n",
    "        with autocast(enabled=(cfg['device']=='cuda')):\n",
    "            out  = model(z_r, x_r).flatten()\n",
    "            loss = criterion(out, y_r)\n",
    "            mae  = nn.L1Loss()(out, y_r).item()\n",
    "            mse  = nn.MSELoss()(out, y_r).item()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        mae_acc.append(mae); mse_acc.append(mse)\n",
    "    return sum(mae_acc)/len(mae_acc), sum(mse_acc)/len(mse_acc)\n",
    "\n",
    "for ep in range(cfg.epochs):\n",
    "    tr_primary, tr_sec = epoch_loop(train_loader, True)\n",
    "    vl_primary, vl_sec = epoch_loop(val_loader, False)\n",
    "\n",
    "    scheduler.step(vl_primary if cfg.sched_metric=='val_'+primary_name.lower()\n",
    "                   else vl_sec)\n",
    "\n",
    "    msg = \"[{}/{}] train {} {:.4f} | val {} {:.4f}\".format(\n",
    "            ep+1, cfg.epochs, primary_name, tr_primary, primary_name, vl_primary)\n",
    "    if cfg.study_metrics and tr_sec is not None:\n",
    "        other = 'MSE' if primary_name=='MAE' else 'MAE'\n",
    "        msg += \"  ||  train {} {:.4f} | val {} {:.4f}\".format(\n",
    "                other, tr_sec, other, vl_sec)\n",
    "    print(msg)\n",
    "\n",
    "# ================================================================\n",
    "# 5) ── save everything needed to resume\n",
    "# ================================================================\n",
    "ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optim_state': optimizer.state_dict(),\n",
    "    'sched_state': scheduler.state_dict(),\n",
    "    'cfg'        : cfg,\n",
    "}\n",
    "ckpt_name = \"ckpt_{}.pt\".format(cfg['runid'])\n",
    "#torch.save(ckpt, ckpt_name)\n",
    "print(\"Saved checkpoint:\", ckpt_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_214855\n",
      "params 17695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] train MAE:1.3717 | val MAE:1.0425 || train MSE:3.8119 | val MSE:2.1939\n",
      "[2/5] train MAE:1.3013 | val MAE:1.1405 || train MSE:3.7406 | val MSE:2.5144\n",
      "[3/5] train MAE:1.3006 | val MAE:1.0161 || train MSE:3.7640 | val MSE:2.1072\n",
      "[4/5] train MAE:1.2782 | val MAE:1.0175 || train MSE:3.6054 | val MSE:2.1122\n",
      "[5/5] train MAE:1.2581 | val MAE:1.0355 || train MSE:3.5746 | val MSE:2.1833\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard  – tweak here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    num_neighbors=8, hood_k=100,                      # <── hood size\n",
    "    aggregator='linear', use_rbf=True, use_attn=True,\n",
    "    use_nconv=False, use_pred_head=True,\n",
    "    norm_coors=True,\n",
    "\n",
    "    loss_type='mae', sched_metric='val_mae', study_metrics=True,\n",
    "    lr=5e-3, epochs=5, batch_size=1, device='cpu',\n",
    "    seed=0, analysis_mode=False, save_attn=False,      # <── new switch\n",
    "    num_paths=4,                                   # cap #files\n",
    "    split_mode='random', split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed);  np.random.seed(cfg.seed);  torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) helper: split + dataset + collate\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class InMemDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k, algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p, allow_pickle=True)\n",
    "                z,pos,sites,y=d['z'],d['pos'],d['sites'],d['pks']\n",
    "                if len(sites)==0: continue\n",
    "                nbr.fit(pos); idx=nbr.kneighbors(sites,return_distance=False)\n",
    "                self.data.append((torch.from_numpy(z[idx]),\n",
    "                                  torch.from_numpy(pos[idx]),\n",
    "                                  torch.from_numpy(y)))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,pos,y=self.data[i]\n",
    "        return (z,pos,y,self.ids[i])\n",
    "\n",
    "def pad(batch, k, device, return_ids):\n",
    "    if return_ids: ids=[b[3] for b in batch]\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zs=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pos=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    ys=torch.full((B,S),float('nan'),device=device)\n",
    "    mask=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zs[b,:s]=z.to(device); pos[b,:s]=p.to(device)\n",
    "        ys[b,:s]=y.to(device); mask[b,:s]=True\n",
    "    return (zs,pos,ys,mask,ids) if return_ids else (zs,pos,ys,mask)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model (unchanged architecture.py)\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__()\n",
    "        self.egnn=StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                              c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "        self.rbf =TunableBlock(LearnableRBF(c.basis,10.).to(c.device),c.use_rbf)\n",
    "        self.attn=TunableBlock(AttentionBlock(c.dim+c.basis,c.dim+c.basis,c.hidden_dim)\\\n",
    "                               .to(c.device),c.use_attn)\n",
    "        if c.aggregator.startswith('nconv'):\n",
    "            self.nconv=nn.Conv1d(c.hood_k,1,c.dim+c.basis).to(c.device)\n",
    "            out=1\n",
    "        else: self.nconv=None; out=c.dim+c.basis\n",
    "        self.head=nn.Linear(out,1).to(c.device) if c.use_pred_head else nn.Identity()\n",
    "        self.prot=EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device)\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]\n",
    "        cent=coord.mean(1,keepdim=True)\n",
    "        r=self.rbf(cent,coord) if cfg.use_rbf else h.new_zeros(h.size(0),cfg.hood_k,cfg.basis)\n",
    "        tok=torch.cat((r.transpose(1,2),h.transpose(1,2)),1)\n",
    "        tok,_=self.attn(tok.permute(2,0,1)); tok=tok.permute(1,0,2)\n",
    "        tok=self.nconv(tok).squeeze(-1) if self.nconv is not None else tok.max(1).values\n",
    "        p=self.head(tok); p=self.prot(p.unsqueeze(0),cent.permute(1,0,2))[0].squeeze(0)\n",
    "        return p\n",
    "model=Model(cfg); print(\"params\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "tr_ds=InMemDS(tr,cfg.hood_k); val_ds=InMemDS(val,cfg.hood_k)\n",
    "collate=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(tr_ds,batch_size=cfg.batch_size,shuffle=True,collate_fn=collate)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=collate)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training util\n",
    "# ================================================================\n",
    "primary_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "secondary_fn= nn.MSELoss() if cfg.study_metrics and cfg.loss_type=='mae' else \\\n",
    "              nn.L1Loss() if cfg.study_metrics else None\n",
    "pname='MAE' if cfg.loss_type=='mae' else 'MSE'; sname='MSE' if pname=='MAE' else 'MAE'\n",
    "opt=torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch=torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval()\n",
    "    ps=ss=0;n=0\n",
    "    for batch in loader:\n",
    "        if cfg.analysis_mode: z,x,y,m,ids=batch\n",
    "        else:                 z,x,y,m=batch\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=primary_fn(pred,y)\n",
    "            sec=secondary_fn(pred,y).item() if secondary_fn else 0.0\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        ps+=loss.item(); ss+=sec; n+=1\n",
    "    return ps/n, (ss/n if secondary_fn else None)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr_p,tr_s=run(tr_loader,True)\n",
    "    va_p,va_s=run(va_loader,False)\n",
    "    sch.step(va_p)\n",
    "    msg=f\"[{e+1}/{cfg.epochs}] train {pname}:{tr_p:.4f} | val {pname}:{va_p:.4f}\"\n",
    "    if secondary_fn: msg+=f\" || train {sname}:{tr_s:.4f} | val {sname}:{va_s:.4f}\"\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_214916\n",
      "params 17695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] train MAE:1.3717 | val MAE:1.0425 || train MSE:3.8119 | val MSE:2.1939\n",
      "[2/5] train MAE:1.3013 | val MAE:1.1405 || train MSE:3.7406 | val MSE:2.5144\n",
      "[3/5] train MAE:1.3006 | val MAE:1.0161 || train MSE:3.7640 | val MSE:2.1072\n",
      "[4/5] train MAE:1.2782 | val MAE:1.0175 || train MSE:3.6054 | val MSE:2.1122\n",
      "[5/5] train MAE:1.2581 | val MAE:1.0355 || train MSE:3.5746 | val MSE:2.1833\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0) dashboard  – tweak here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "cfg = Cfg(\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    num_neighbors=8, hood_k=100,                      # <── hood size\n",
    "    aggregator='linear', use_rbf=True, use_attn=True,\n",
    "    use_nconv=False, use_pred_head=True,\n",
    "    norm_coors=True,\n",
    "\n",
    "    loss_type='mae', sched_metric='val_mae', study_metrics=True,\n",
    "    lr=5e-3, epochs=5, batch_size=1, device='cpu',\n",
    "    seed=0, analysis_mode=False, save_attn=False,      # <── new switch\n",
    "    num_paths=4,                                   # cap #files\n",
    "    split_mode='random', split_ratio=0.8, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "random.seed(cfg.seed);  np.random.seed(cfg.seed);  torch.manual_seed(cfg.seed)\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 2) helper: split + dataset + collate\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class InMemDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k, algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p, allow_pickle=True)\n",
    "                z,pos,sites,y=d['z'],d['pos'],d['sites'],d['pks']\n",
    "                if len(sites)==0: continue\n",
    "                nbr.fit(pos); idx=nbr.kneighbors(sites,return_distance=False)\n",
    "                self.data.append((torch.from_numpy(z[idx]),\n",
    "                                  torch.from_numpy(pos[idx]),\n",
    "                                  torch.from_numpy(y)))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,pos,y=self.data[i]\n",
    "        return (z,pos,y,self.ids[i])\n",
    "\n",
    "def pad(batch, k, device, return_ids):\n",
    "    if return_ids: ids=[b[3] for b in batch]\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zs=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pos=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    ys=torch.full((B,S),float('nan'),device=device)\n",
    "    mask=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zs[b,:s]=z.to(device); pos[b,:s]=p.to(device)\n",
    "        ys[b,:s]=y.to(device); mask[b,:s]=True\n",
    "    return (zs,pos,ys,mask,ids) if return_ids else (zs,pos,ys,mask)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model (unchanged architecture.py)\n",
    "# ================================================================\n",
    "from architecture import StackedEGNN, LearnableRBF, AttentionBlock, TunableBlock\n",
    "from egnn_pytorch import EGNN\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__()\n",
    "        self.egnn=StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                              c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "        self.rbf =TunableBlock(LearnableRBF(c.basis,10.).to(c.device),c.use_rbf)\n",
    "        self.attn=TunableBlock(AttentionBlock(c.dim+c.basis,c.dim+c.basis,c.hidden_dim)\\\n",
    "                               .to(c.device),c.use_attn)\n",
    "        if c.aggregator.startswith('nconv'):\n",
    "            self.nconv=nn.Conv1d(c.hood_k,1,c.dim+c.basis).to(c.device)\n",
    "            out=1\n",
    "        else: self.nconv=None; out=c.dim+c.basis\n",
    "        self.head=nn.Linear(out,1).to(c.device) if c.use_pred_head else nn.Identity()\n",
    "        self.prot=EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device)\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]\n",
    "        cent=coord.mean(1,keepdim=True)\n",
    "        r=self.rbf(cent,coord) if cfg.use_rbf else h.new_zeros(h.size(0),cfg.hood_k,cfg.basis)\n",
    "        tok=torch.cat((r.transpose(1,2),h.transpose(1,2)),1)\n",
    "        tok,_=self.attn(tok.permute(2,0,1)); tok=tok.permute(1,0,2)\n",
    "        tok=self.nconv(tok).squeeze(-1) if self.nconv is not None else tok.max(1).values\n",
    "        p=self.head(tok); p=self.prot(p.unsqueeze(0),cent.permute(1,0,2))[0].squeeze(0)\n",
    "        return p\n",
    "model=Model(cfg); print(\"params\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "tr_ds=InMemDS(tr,cfg.hood_k); val_ds=InMemDS(val,cfg.hood_k)\n",
    "collate=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(tr_ds,batch_size=cfg.batch_size,shuffle=True,collate_fn=collate)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=collate)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training util\n",
    "# ================================================================\n",
    "primary_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "secondary_fn= nn.MSELoss() if cfg.study_metrics and cfg.loss_type=='mae' else \\\n",
    "              nn.L1Loss() if cfg.study_metrics else None\n",
    "pname='MAE' if cfg.loss_type=='mae' else 'MSE'; sname='MSE' if pname=='MAE' else 'MAE'\n",
    "opt=torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch=torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval()\n",
    "    ps=ss=0;n=0\n",
    "    for batch in loader:\n",
    "        if cfg.analysis_mode: z,x,y,m,ids=batch\n",
    "        else:                 z,x,y,m=batch\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=primary_fn(pred,y)\n",
    "            sec=secondary_fn(pred,y).item() if secondary_fn else 0.0\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        ps+=loss.item(); ss+=sec; n+=1\n",
    "    return ps/n, (ss/n if secondary_fn else None)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr_p,tr_s=run(tr_loader,True)\n",
    "    va_p,va_s=run(va_loader,False)\n",
    "    sch.step(va_p)\n",
    "    msg=f\"[{e+1}/{cfg.epochs}] train {pname}:{tr_p:.4f} | val {pname}:{va_p:.4f}\"\n",
    "    if secondary_fn: msg+=f\" || train {sname}:{tr_s:.4f} | val {sname}:{va_s:.4f}\"\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
