{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train L1 = 3.7255\n",
      "              |  val L1 = 3.7098\n",
      "Epoch   1 | train L1 = 3.3110\n",
      "              |  val L1 = 3.5907\n",
      "Epoch   2 | train L1 = 3.1786\n",
      "              |  val L1 = 3.3921\n",
      "Epoch   3 | train L1 = 2.9457\n",
      "              |  val L1 = 2.9334\n",
      "Epoch   4 | train L1 = 2.7034\n",
      "              |  val L1 = 2.5983\n",
      "Epoch   5 | train L1 = 2.6741\n",
      "              |  val L1 = 3.7105\n",
      "Epoch   6 | train L1 = 3.1938\n",
      "              |  val L1 = 3.5580\n",
      "Epoch   7 | train L1 = 3.3964\n",
      "              |  val L1 = 3.7247\n",
      "Epoch   8 | train L1 = 3.2884\n",
      "              |  val L1 = 3.7554\n",
      "Epoch   9 | train L1 = 3.1997\n",
      "              |  val L1 = 3.5579\n",
      "15.773443937301636 sec\n"
     ]
    }
   ],
   "source": [
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 15\n",
    "BATCH_SIZE  =  1           # now safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 2, 8 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=3\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) model pieces ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "egnn_net = StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                       dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                       num_nearest_neighbors=num_neighbors,\n",
    "                       norm_coors=True,\n",
    "                       num_edge_tokens=num_edge_tokens,\n",
    "                       num_global_tokens=num_global_tokens).to(device)\n",
    "\n",
    "rbf_layer = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device)\n",
    "mha_layer = AttentionBlock(embed_dim=dim + basis,\n",
    "                           num_heads=num_heads,\n",
    "                           hidden_dim=hidden_dim).to(device)\n",
    "pred_head = nn.Linear(dim + basis, 1).to(device)\n",
    "\n",
    "protein_egnn=EGNN(dim=1,update_coors=False,num_nearest_neighbors=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(egnn_net.parameters()) +\n",
    "    list(rbf_layer.parameters()) +\n",
    "    list(mha_layer.parameters()) +\n",
    "    list(pred_head.parameters()) +\n",
    "    list(protein_egnn.parameters()),\n",
    "    lr=5e-3\n",
    ")\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    \"\"\"\n",
    "    z_r : (R, N)       int32   – atomic numbers for R residues\n",
    "    x_r : (R, N, 3)    float32 – coordinates\n",
    "    returns (R, dim + basis)   – per-residue embeddings\n",
    "    \"\"\"\n",
    "    # ---------- EGNN ----------\n",
    "    h_out, coords = egnn_net(z_r, x_r)          # h_out is [tensor] or tensor\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out   # (R, N, dim)\n",
    "\n",
    "    # ---------- RBF on *input* coords (already (R,N,3)) ----------\n",
    "    #d   = torch.cdist(x_r, x_r)            # (R, N, N)\n",
    "    centroids=coords.mean(dim=1).unsqueeze(1)\n",
    "    rbf = rbf_layer(centroids,coords)                     # (R, N, N, basis)\n",
    "    #print(centroids.shape,coords.shape)\n",
    "    # ---------- concat & attention ----------\n",
    "    h0  = h.transpose(1, 2)                # (R, dim,   N)\n",
    "    r0  = rbf.transpose(1, 2)        # (R, basis, N)\n",
    "    tok = torch.cat((r0, h0), dim=1)       # (R, dim+basis, N)\n",
    "\n",
    "    tok, _ = mha_layer(tok.permute(2, 0, 1))   # (N, R, C) → attn(+PE)\n",
    "    tok    = tok.permute(1, 0, 2).max(dim=1).values   # (R, C) max over neighbours\n",
    "    return tok,    centroids                              # (R, dim + basis)\n",
    "                                         # (R, dim+basis)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) training / validation loop ---------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    # ======== TRAIN ========\n",
    "    egnn_net.train(); rbf_layer.train(); mha_layer.train(); pred_head.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for z, x, y, mask in train_loader:                 # z:(B,S,N)  mask:(B,S)\n",
    "        # compress away padding →  (R, N), (R, N, 3), (R,)\n",
    "        valid      = mask.view(-1)\n",
    "        z_res      = z.view(-1, z.size(2))[valid].to(device)\n",
    "        x_res      = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "        y_res      = y.view(-1)[valid].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #model\n",
    "        feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "        \n",
    "        preds = pred_head(feats)       \n",
    "        t=preds.unsqueeze(0)\n",
    "        #preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "        loss  = criterion(preds.flatten(), y_res)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | train L1 = {np.mean(tr_losses):.4f}\")\n",
    "\n",
    "    \n",
    "    # ======== VALID ========\n",
    "    egnn_net.eval(); rbf_layer.eval(); mha_layer.eval(); pred_head.eval()\n",
    "    vl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z, x, y, mask in val_loader:\n",
    "            valid   = mask.view(-1)\n",
    "            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "            \n",
    "            preds = pred_head(feats)       \n",
    "            t=preds.unsqueeze(0)\n",
    "            #preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "            loss  = criterion(preds.flatten(), y_res)\n",
    "            vl_losses.append(loss.item())\n",
    "\n",
    "    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "print(time.time() - t0,\"sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with protein egnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train L1 = 3.5549\n",
      "              |  val L1 = 3.8103\n",
      "Epoch   1 | train L1 = 3.2875\n",
      "              |  val L1 = 3.4910\n",
      "Epoch   2 | train L1 = 2.9995\n",
      "              |  val L1 = 3.0372\n",
      "Epoch   3 | train L1 = 2.6572\n",
      "              |  val L1 = 2.6012\n",
      "Epoch   4 | train L1 = 2.4278\n",
      "              |  val L1 = 2.4152\n",
      "Epoch   5 | train L1 = 2.2994\n",
      "              |  val L1 = 2.2199\n",
      "Epoch   6 | train L1 = 2.2000\n",
      "              |  val L1 = 2.0956\n",
      "Epoch   7 | train L1 = 2.1452\n",
      "              |  val L1 = 2.0702\n",
      "Epoch   8 | train L1 = 2.0996\n",
      "              |  val L1 = 1.9954\n",
      "Epoch   9 | train L1 = 2.0313\n",
      "              |  val L1 = 2.0054\n",
      "16.874836444854736 sec\n"
     ]
    }
   ],
   "source": [
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 15\n",
    "BATCH_SIZE  =  1           # now safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 2, 8 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=3\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) model pieces ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "egnn_net = StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                       dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                       num_nearest_neighbors=num_neighbors,\n",
    "                       norm_coors=True,\n",
    "                       num_edge_tokens=num_edge_tokens,\n",
    "                       num_global_tokens=num_global_tokens).to(device)\n",
    "\n",
    "rbf_layer = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device)\n",
    "mha_layer = AttentionBlock(embed_dim=dim + basis,\n",
    "                           num_heads=num_heads,\n",
    "                           hidden_dim=hidden_dim).to(device)\n",
    "pred_head = nn.Linear(dim + basis, 1).to(device)\n",
    "\n",
    "protein_egnn=EGNN(dim=1,update_coors=False,num_nearest_neighbors=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(egnn_net.parameters()) +\n",
    "    list(rbf_layer.parameters()) +\n",
    "    list(mha_layer.parameters()) +\n",
    "    list(pred_head.parameters()) +\n",
    "    list(protein_egnn.parameters()),\n",
    "    lr=5e-3\n",
    ")\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    \"\"\"\n",
    "    z_r : (R, N)       int32   – atomic numbers for R residues\n",
    "    x_r : (R, N, 3)    float32 – coordinates\n",
    "    returns (R, dim + basis)   – per-residue embeddings\n",
    "    \"\"\"\n",
    "    # ---------- EGNN ----------\n",
    "    h_out, coords = egnn_net(z_r, x_r)          # h_out is [tensor] or tensor\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out   # (R, N, dim)\n",
    "\n",
    "    # ---------- RBF on *input* coords (already (R,N,3)) ----------\n",
    "    #d   = torch.cdist(x_r, x_r)            # (R, N, N)\n",
    "    centroids=coords.mean(dim=1).unsqueeze(1)\n",
    "    rbf = rbf_layer(centroids,coords)                     # (R, N, N, basis)\n",
    "    #print(centroids.shape,coords.shape)\n",
    "    # ---------- concat & attention ----------\n",
    "    h0  = h.transpose(1, 2)                # (R, dim,   N)\n",
    "    r0  = rbf.transpose(1, 2)        # (R, basis, N)\n",
    "    tok = torch.cat((r0, h0), dim=1)       # (R, dim+basis, N)\n",
    "\n",
    "    tok, _ = mha_layer(tok.permute(2, 0, 1))   # (N, R, C) → attn(+PE)\n",
    "    tok    = tok.permute(1, 0, 2).max(dim=1).values   # (R, C) max over neighbours\n",
    "    return tok,    centroids.permute(1,0,2)                              # (R, dim + basis)\n",
    "                                         # (R, dim+basis)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) training / validation loop ---------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    # ======== TRAIN ========\n",
    "    egnn_net.train(); rbf_layer.train(); mha_layer.train(); pred_head.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for z, x, y, mask in train_loader:                 # z:(B,S,N)  mask:(B,S)\n",
    "        # compress away padding →  (R, N), (R, N, 3), (R,)\n",
    "        valid      = mask.view(-1)\n",
    "        z_res      = z.view(-1, z.size(2))[valid].to(device)\n",
    "        x_res      = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "        y_res      = y.view(-1)[valid].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #model\n",
    "        feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "        \n",
    "        preds = pred_head(feats)       \n",
    "        t=preds.unsqueeze(0)\n",
    "        preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "        loss  = criterion(preds.flatten(), y_res)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | train L1 = {np.mean(tr_losses):.4f}\")\n",
    "\n",
    "    \n",
    "    # ======== VALID ========\n",
    "    egnn_net.eval(); rbf_layer.eval(); mha_layer.eval(); pred_head.eval()\n",
    "    vl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z, x, y, mask in val_loader:\n",
    "            valid   = mask.view(-1)\n",
    "            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "            \n",
    "            preds = pred_head(feats)       \n",
    "            t=preds.unsqueeze(0)\n",
    "            preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "            loss  = criterion(preds.flatten(), y_res)\n",
    "            vl_losses.append(loss.item())\n",
    "\n",
    "    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "print(time.time() - t0,\"sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train L1 = 3.6461\n",
      "              |  val L1 = 3.7602\n",
      "Epoch   1 | train L1 = 3.3495\n",
      "              |  val L1 = 3.6736\n",
      "Epoch   2 | train L1 = 3.1531\n",
      "              |  val L1 = 3.4099\n",
      "Epoch   3 | train L1 = 2.6844\n",
      "              |  val L1 = 2.5964\n",
      "Epoch   4 | train L1 = 2.3363\n",
      "              |  val L1 = 2.3687\n",
      "Epoch   5 | train L1 = 2.1052\n",
      "              |  val L1 = 2.2530\n",
      "Epoch   6 | train L1 = 2.1028\n",
      "              |  val L1 = 2.0454\n",
      "Epoch   7 | train L1 = 1.9126\n",
      "              |  val L1 = 2.0342\n",
      "Epoch   8 | train L1 = 1.8397\n",
      "              |  val L1 = 1.9076\n",
      "Epoch   9 | train L1 = 1.8524\n",
      "              |  val L1 = 1.7860\n",
      "17.69021701812744 sec\n"
     ]
    }
   ],
   "source": [
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 15\n",
    "BATCH_SIZE  =  1           # now safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 6, 6 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=4\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) model pieces ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "egnn_net = StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                       dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                       num_nearest_neighbors=num_neighbors,\n",
    "                       norm_coors=True,\n",
    "                       num_edge_tokens=num_edge_tokens,\n",
    "                       num_global_tokens=num_global_tokens).to(device)\n",
    "\n",
    "rbf_layer = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device)\n",
    "mha_layer = AttentionBlock(embed_dim=dim + basis,\n",
    "                           num_heads=num_heads,\n",
    "                           hidden_dim=hidden_dim).to(device)\n",
    "pred_head = nn.Linear(dim + basis, 1).to(device)\n",
    "\n",
    "protein_egnn=EGNN(dim=1,update_coors=False,num_nearest_neighbors=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(egnn_net.parameters()) +\n",
    "    list(rbf_layer.parameters()) +\n",
    "    list(mha_layer.parameters()) +\n",
    "    list(pred_head.parameters()) +\n",
    "    list(protein_egnn.parameters()),\n",
    "    lr=5e-3\n",
    ")\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    \"\"\"\n",
    "    z_r : (R, N)       int32   – atomic numbers for R residues\n",
    "    x_r : (R, N, 3)    float32 – coordinates\n",
    "    returns (R, dim + basis)   – per-residue embeddings\n",
    "    \"\"\"\n",
    "    # ---------- EGNN ----------\n",
    "    h_out, coords = egnn_net(z_r, x_r)          # h_out is [tensor] or tensor\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out   # (R, N, dim)\n",
    "\n",
    "    # ---------- RBF on *input* coords (already (R,N,3)) ----------\n",
    "    #d   = torch.cdist(x_r, x_r)            # (R, N, N)\n",
    "    centroids=coords.mean(dim=1).unsqueeze(1)\n",
    "    rbf = rbf_layer(centroids,coords)                     # (R, N, N, basis)\n",
    "    #print(centroids.shape,coords.shape)\n",
    "    # ---------- concat & attention ----------\n",
    "    h0  = h.transpose(1, 2)                # (R, dim,   N)\n",
    "    r0  = rbf.transpose(1, 2)        # (R, basis, N)\n",
    "    tok = torch.cat((r0, h0), dim=1)       # (R, dim+basis, N)\n",
    "\n",
    "    tok, _ = mha_layer(tok.permute(2, 0, 1))   # (N, R, C) → attn(+PE)\n",
    "    tok    = tok.permute(1, 0, 2).max(dim=1).values   # (R, C) max over neighbours\n",
    "    return tok,    centroids.permute(1,0,2)                              # (R, dim + basis)\n",
    "                                         # (R, dim+basis)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) training / validation loop ---------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    # ======== TRAIN ========\n",
    "    egnn_net.train(); rbf_layer.train(); mha_layer.train(); pred_head.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for z, x, y, mask in train_loader:                 # z:(B,S,N)  mask:(B,S)\n",
    "        # compress away padding →  (R, N), (R, N, 3), (R,)\n",
    "        valid      = mask.view(-1)\n",
    "        z_res      = z.view(-1, z.size(2))[valid].to(device)\n",
    "        x_res      = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "        y_res      = y.view(-1)[valid].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #model\n",
    "        feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "        \n",
    "        preds = pred_head(feats)       \n",
    "        t=preds.unsqueeze(0)\n",
    "        preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "        loss  = criterion(preds.flatten(), y_res)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | train L1 = {np.mean(tr_losses):.4f}\")\n",
    "\n",
    "    \n",
    "    # ======== VALID ========\n",
    "    egnn_net.eval(); rbf_layer.eval(); mha_layer.eval(); pred_head.eval()\n",
    "    vl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z, x, y, mask in val_loader:\n",
    "            valid   = mask.view(-1)\n",
    "            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "            \n",
    "            preds = pred_head(feats)       \n",
    "            t=preds.unsqueeze(0)\n",
    "            preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "            loss  = criterion(preds.flatten(), y_res)\n",
    "            vl_losses.append(loss.item())\n",
    "\n",
    "    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "print(time.time() - t0,\"sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rbf 12 8 HIDDEN DIM 3, no pegnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train L1 = 3.4655\n",
      "              |  val L1 = 3.7604\n",
      "Epoch   1 | train L1 = 3.3019\n",
      "              |  val L1 = 3.5398\n",
      "Epoch   2 | train L1 = 3.0392\n",
      "              |  val L1 = 3.4065\n",
      "Epoch   3 | train L1 = 2.7296\n",
      "              |  val L1 = 2.8663\n",
      "Epoch   4 | train L1 = 2.6993\n",
      "              |  val L1 = 2.8542\n",
      "Epoch   5 | train L1 = 2.4855\n",
      "              |  val L1 = 2.6787\n",
      "Epoch   6 | train L1 = 2.1847\n",
      "              |  val L1 = 2.0534\n",
      "Epoch   7 | train L1 = 1.9407\n",
      "              |  val L1 = 1.8874\n",
      "Epoch   8 | train L1 = 1.8346\n",
      "              |  val L1 = 1.9031\n",
      "Epoch   9 | train L1 = 1.6756\n",
      "              |  val L1 = 1.8190\n",
      "178.06058812141418 sec\n"
     ]
    }
   ],
   "source": [
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 100\n",
    "BATCH_SIZE  =  1           # now safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 12, 8 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=3\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) model pieces ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "egnn_net = StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                       dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                       num_nearest_neighbors=num_neighbors,\n",
    "                       norm_coors=True,\n",
    "                       num_edge_tokens=num_edge_tokens,\n",
    "                       num_global_tokens=num_global_tokens).to(device)\n",
    "\n",
    "rbf_layer = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device)\n",
    "mha_layer = AttentionBlock(embed_dim=dim + basis,\n",
    "                           num_heads=num_heads,\n",
    "                           hidden_dim=hidden_dim).to(device)\n",
    "pred_head = nn.Linear(dim + basis, 1).to(device)\n",
    "\n",
    "protein_egnn=EGNN(dim=1,update_coors=False,num_nearest_neighbors=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(egnn_net.parameters()) +\n",
    "    list(rbf_layer.parameters()) +\n",
    "    list(mha_layer.parameters()) +\n",
    "    list(pred_head.parameters()) +\n",
    "    list(protein_egnn.parameters()),\n",
    "    lr=5e-3\n",
    ")\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    \"\"\"\n",
    "    z_r : (R, N)       int32   – atomic numbers for R residues\n",
    "    x_r : (R, N, 3)    float32 – coordinates\n",
    "    returns (R, dim + basis)   – per-residue embeddings\n",
    "    \"\"\"\n",
    "    # ---------- EGNN ----------\n",
    "    h_out, coords = egnn_net(z_r, x_r)          # h_out is [tensor] or tensor\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out   # (R, N, dim)\n",
    "\n",
    "    # ---------- RBF on *input* coords (already (R,N,3)) ----------\n",
    "    #d   = torch.cdist(x_r, x_r)            # (R, N, N)\n",
    "    centroids=coords.mean(dim=1).unsqueeze(1)\n",
    "    rbf = rbf_layer(centroids,coords)                     # (R, N, N, basis)\n",
    "    #print(centroids.shape,coords.shape)\n",
    "    # ---------- concat & attention ----------\n",
    "    h0  = h.transpose(1, 2)                # (R, dim,   N)\n",
    "    r0  = rbf.transpose(1, 2)        # (R, basis, N)\n",
    "    tok = torch.cat((r0, h0), dim=1)       # (R, dim+basis, N)\n",
    "\n",
    "    tok, _ = mha_layer(tok.permute(2, 0, 1))   # (N, R, C) → attn(+PE)\n",
    "    tok    = tok.permute(1, 0, 2).max(dim=1).values   # (R, C) max over neighbours\n",
    "    return tok,    centroids                              # (R, dim + basis)\n",
    "                                         # (R, dim+basis)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) training / validation loop ---------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    # ======== TRAIN ========\n",
    "    egnn_net.train(); rbf_layer.train(); mha_layer.train(); pred_head.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for z, x, y, mask in train_loader:                 # z:(B,S,N)  mask:(B,S)\n",
    "        # compress away padding →  (R, N), (R, N, 3), (R,)\n",
    "        valid      = mask.view(-1)\n",
    "        z_res      = z.view(-1, z.size(2))[valid].to(device)\n",
    "        x_res      = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "        y_res      = y.view(-1)[valid].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #model\n",
    "        feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "        \n",
    "        preds = pred_head(feats)       \n",
    "        t=preds.unsqueeze(0)\n",
    "        #preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "        loss  = criterion(preds.flatten(), y_res)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | train L1 = {np.mean(tr_losses):.4f}\")\n",
    "\n",
    "    \n",
    "    # ======== VALID ========\n",
    "    egnn_net.eval(); rbf_layer.eval(); mha_layer.eval(); pred_head.eval()\n",
    "    vl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z, x, y, mask in val_loader:\n",
    "            valid   = mask.view(-1)\n",
    "            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "            \n",
    "            preds = pred_head(feats)       \n",
    "            t=preds.unsqueeze(0)\n",
    "            #preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "            loss  = criterion(preds.flatten(), y_res)\n",
    "            vl_losses.append(loss.item())\n",
    "\n",
    "    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "print(time.time() - t0,\"sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now with pegnn and rbf 12 8 HD 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train L1 = 3.5314\n",
      "              |  val L1 = 3.8011\n",
      "Epoch   1 | train L1 = 3.3841\n",
      "              |  val L1 = 3.7654\n",
      "Epoch   2 | train L1 = 3.3483\n",
      "              |  val L1 = 3.7135\n",
      "Epoch   3 | train L1 = 3.2191\n",
      "              |  val L1 = 3.3462\n",
      "Epoch   4 | train L1 = 2.8690\n",
      "              |  val L1 = 2.9730\n",
      "Epoch   5 | train L1 = 2.6776\n",
      "              |  val L1 = 2.8889\n",
      "Epoch   6 | train L1 = 2.4746\n",
      "              |  val L1 = 2.7486\n",
      "Epoch   7 | train L1 = 2.1433\n",
      "              |  val L1 = 1.9590\n",
      "Epoch   8 | train L1 = 1.7988\n",
      "              |  val L1 = 1.5335\n",
      "Epoch   9 | train L1 = 1.6244\n",
      "              |  val L1 = 1.5674\n",
      "175.53697037696838 sec\n"
     ]
    }
   ],
   "source": [
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 100\n",
    "BATCH_SIZE  =  1           # now safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 12, 8 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=3\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) model pieces ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "egnn_net = StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                       dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                       num_nearest_neighbors=num_neighbors,\n",
    "                       norm_coors=True,\n",
    "                       num_edge_tokens=num_edge_tokens,\n",
    "                       num_global_tokens=num_global_tokens).to(device)\n",
    "\n",
    "rbf_layer = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device)\n",
    "mha_layer = AttentionBlock(embed_dim=dim + basis,\n",
    "                           num_heads=num_heads,\n",
    "                           hidden_dim=hidden_dim).to(device)\n",
    "pred_head = nn.Linear(dim + basis, 1).to(device)\n",
    "\n",
    "protein_egnn=EGNN(dim=1,update_coors=False,num_nearest_neighbors=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(egnn_net.parameters()) +\n",
    "    list(rbf_layer.parameters()) +\n",
    "    list(mha_layer.parameters()) +\n",
    "    list(pred_head.parameters()) +\n",
    "    list(protein_egnn.parameters()),\n",
    "    lr=5e-3\n",
    ")\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    \"\"\"\n",
    "    z_r : (R, N)       int32   – atomic numbers for R residues\n",
    "    x_r : (R, N, 3)    float32 – coordinates\n",
    "    returns (R, dim + basis)   – per-residue embeddings\n",
    "    \"\"\"\n",
    "    # ---------- EGNN ----------\n",
    "    h_out, coords = egnn_net(z_r, x_r)          # h_out is [tensor] or tensor\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out   # (R, N, dim)\n",
    "\n",
    "    # ---------- RBF on *input* coords (already (R,N,3)) ----------\n",
    "    #d   = torch.cdist(x_r, x_r)            # (R, N, N)\n",
    "    centroids=coords.mean(dim=1).unsqueeze(1)\n",
    "    rbf = rbf_layer(centroids,coords)                     # (R, N, N, basis)\n",
    "    #print(centroids.shape,coords.shape)\n",
    "    # ---------- concat & attention ----------\n",
    "    h0  = h.transpose(1, 2)                # (R, dim,   N)\n",
    "    r0  = rbf.transpose(1, 2)        # (R, basis, N)\n",
    "    tok = torch.cat((r0, h0), dim=1)       # (R, dim+basis, N)\n",
    "\n",
    "    tok, _ = mha_layer(tok.permute(2, 0, 1))   # (N, R, C) → attn(+PE)\n",
    "    tok    = tok.permute(1, 0, 2).max(dim=1).values   # (R, C) max over neighbours\n",
    "    return tok,    centroids.permute(1,0,2)                              # (R, dim + basis)\n",
    "                                         # (R, dim+basis)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) training / validation loop ---------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    # ======== TRAIN ========\n",
    "    egnn_net.train(); rbf_layer.train(); mha_layer.train(); pred_head.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for z, x, y, mask in train_loader:                 # z:(B,S,N)  mask:(B,S)\n",
    "        # compress away padding →  (R, N), (R, N, 3), (R,)\n",
    "        valid      = mask.view(-1)\n",
    "        z_res      = z.view(-1, z.size(2))[valid].to(device)\n",
    "        x_res      = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "        y_res      = y.view(-1)[valid].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #model\n",
    "        feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "        \n",
    "        preds = pred_head(feats)       \n",
    "        t=preds.unsqueeze(0)\n",
    "        preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "        loss  = criterion(preds.flatten(), y_res)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | train L1 = {np.mean(tr_losses):.4f}\")\n",
    "\n",
    "    \n",
    "    # ======== VALID ========\n",
    "    egnn_net.eval(); rbf_layer.eval(); mha_layer.eval(); pred_head.eval()\n",
    "    vl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z, x, y, mask in val_loader:\n",
    "            valid   = mask.view(-1)\n",
    "            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "            \n",
    "            preds = pred_head(feats)       \n",
    "            t=preds.unsqueeze(0)\n",
    "            preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "            loss  = criterion(preds.flatten(), y_res)\n",
    "            vl_losses.append(loss.item())\n",
    "\n",
    "    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "print(time.time() - t0,\"sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HIDDEN DIM 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with rbf, 6 6 HD 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train L1 = 3.7299\n",
      "              |  val L1 = 3.8041\n",
      "Epoch   1 | train L1 = 3.4110\n",
      "              |  val L1 = 3.8359\n",
      "Epoch   2 | train L1 = 3.4060\n",
      "              |  val L1 = 3.9078\n",
      "Epoch   3 | train L1 = 3.3977\n",
      "              |  val L1 = 3.7853\n",
      "Epoch   4 | train L1 = 3.3813\n",
      "              |  val L1 = 3.8105\n",
      "Epoch   5 | train L1 = 3.3805\n",
      "              |  val L1 = 3.7775\n",
      "Epoch   6 | train L1 = 3.3330\n",
      "              |  val L1 = 3.7182\n",
      "Epoch   7 | train L1 = 3.2467\n",
      "              |  val L1 = 3.4676\n",
      "Epoch   8 | train L1 = 3.0564\n",
      "              |  val L1 = 3.0519\n",
      "Epoch   9 | train L1 = 2.5600\n",
      "              |  val L1 = 2.8689\n",
      "128.25800108909607 sec\n"
     ]
    }
   ],
   "source": [
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 100\n",
    "BATCH_SIZE  =  1           # not safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 6, 6 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=4\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) model pieces ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "egnn_net = StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                       dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                       num_nearest_neighbors=num_neighbors,\n",
    "                       norm_coors=True,\n",
    "                       num_edge_tokens=num_edge_tokens,\n",
    "                       num_global_tokens=num_global_tokens).to(device)\n",
    "\n",
    "rbf_layer = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device)\n",
    "mha_layer = AttentionBlock(embed_dim=dim + basis,\n",
    "                           num_heads=num_heads,\n",
    "                           hidden_dim=hidden_dim).to(device)\n",
    "pred_head = nn.Linear(dim + basis, 1).to(device)\n",
    "\n",
    "protein_egnn=EGNN(dim=1,update_coors=False,num_nearest_neighbors=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(egnn_net.parameters()) +\n",
    "    list(rbf_layer.parameters()) +\n",
    "    list(mha_layer.parameters()) +\n",
    "    list(pred_head.parameters()) +\n",
    "    list(protein_egnn.parameters()),\n",
    "    lr=5e-3\n",
    ")\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    \"\"\"\n",
    "    z_r : (R, N)       int32   – atomic numbers for R residues\n",
    "    x_r : (R, N, 3)    float32 – coordinates\n",
    "    returns (R, dim + basis)   – per-residue embeddings\n",
    "    \"\"\"\n",
    "    # ---------- EGNN ----------\n",
    "    h_out, coords = egnn_net(z_r, x_r)          # h_out is [tensor] or tensor\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out   # (R, N, dim)\n",
    "\n",
    "    # ---------- RBF on *input* coords (already (R,N,3)) ----------\n",
    "    #d   = torch.cdist(x_r, x_r)            # (R, N, N)\n",
    "    centroids=coords.mean(dim=1).unsqueeze(1)\n",
    "    rbf = rbf_layer(centroids,coords)                     # (R, N, N, basis)\n",
    "    #print(centroids.shape,coords.shape)\n",
    "    # ---------- concat & attention ----------\n",
    "    h0  = h.transpose(1, 2)                # (R, dim,   N)\n",
    "    r0  = rbf.transpose(1, 2)        # (R, basis, N)\n",
    "    tok = torch.cat((r0, h0), dim=1)       # (R, dim+basis, N)\n",
    "\n",
    "    tok, _ = mha_layer(tok.permute(2, 0, 1))   # (N, R, C) → attn(+PE)\n",
    "    tok    = tok.permute(1, 0, 2).max(dim=1).values   # (R, C) max over neighbours\n",
    "    return tok,    centroids.permute(1,0,2)                              # (R, dim + basis)\n",
    "                                         # (R, dim+basis)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) training / validation loop ---------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    # ======== TRAIN ========\n",
    "    egnn_net.train(); rbf_layer.train(); mha_layer.train(); pred_head.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for z, x, y, mask in train_loader:                 # z:(B,S,N)  mask:(B,S)\n",
    "        # compress away padding →  (R, N), (R, N, 3), (R,)\n",
    "        valid      = mask.view(-1)\n",
    "        z_res      = z.view(-1, z.size(2))[valid].to(device)\n",
    "        x_res      = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "        y_res      = y.view(-1)[valid].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #model\n",
    "        feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "        \n",
    "        preds = pred_head(feats)       \n",
    "        t=preds.unsqueeze(0)\n",
    "        preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "        loss  = criterion(preds.flatten(), y_res)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | train L1 = {np.mean(tr_losses):.4f}\")\n",
    "\n",
    "    \n",
    "    # ======== VALID ========\n",
    "    egnn_net.eval(); rbf_layer.eval(); mha_layer.eval(); pred_head.eval()\n",
    "    vl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z, x, y, mask in val_loader:\n",
    "            valid   = mask.view(-1)\n",
    "            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "            \n",
    "            preds = pred_head(feats)       \n",
    "            t=preds.unsqueeze(0)\n",
    "            preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "            loss  = criterion(preds.flatten(), y_res)\n",
    "            vl_losses.append(loss.item())\n",
    "\n",
    "    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "print(time.time() - t0,\"sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-f126f83bcf66>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-f126f83bcf66>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    NO rbf, dim 4\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NO rbf, dim 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train L1 = 3.9070\n",
      "              |  val L1 = 3.7480\n",
      "Epoch   1 | train L1 = 3.3117\n",
      "              |  val L1 = 3.5646\n",
      "Epoch   2 | train L1 = 3.0381\n",
      "              |  val L1 = 3.1896\n",
      "Epoch   3 | train L1 = 2.7943\n",
      "              |  val L1 = 2.9842\n",
      "Epoch   4 | train L1 = 2.7332\n",
      "              |  val L1 = 2.9643\n",
      "Epoch   5 | train L1 = 2.6919\n",
      "              |  val L1 = 3.1137\n",
      "Epoch   6 | train L1 = 2.5600\n",
      "              |  val L1 = 2.9541\n",
      "Epoch   7 | train L1 = 2.4171\n",
      "              |  val L1 = 2.6732\n",
      "Epoch   8 | train L1 = 2.3921\n",
      "              |  val L1 = 2.8359\n",
      "Epoch   9 | train L1 = 2.3996\n",
      "              |  val L1 = 2.7632\n",
      "144.48865246772766 sec\n"
     ]
    }
   ],
   "source": [
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 100\n",
    "BATCH_SIZE  =  1           # not safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 12, 0 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=4\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) model pieces ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "egnn_net = StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                       dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                       num_nearest_neighbors=num_neighbors,\n",
    "                       norm_coors=True,\n",
    "                       num_edge_tokens=num_edge_tokens,\n",
    "                       num_global_tokens=num_global_tokens).to(device)\n",
    "\n",
    "rbf_layer = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device)\n",
    "mha_layer = AttentionBlock(embed_dim=dim + basis,\n",
    "                           num_heads=num_heads,\n",
    "                           hidden_dim=hidden_dim).to(device)\n",
    "pred_head = nn.Linear(dim + basis, 1).to(device)\n",
    "\n",
    "protein_egnn=EGNN(dim=1,update_coors=False,num_nearest_neighbors=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(egnn_net.parameters()) +\n",
    "    list(rbf_layer.parameters()) +\n",
    "    list(mha_layer.parameters()) +\n",
    "    list(pred_head.parameters()) +\n",
    "    list(protein_egnn.parameters()),\n",
    "    lr=5e-3\n",
    ")\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    \"\"\"\n",
    "    z_r : (R, N)       int32   – atomic numbers for R residues\n",
    "    x_r : (R, N, 3)    float32 – coordinates\n",
    "    returns (R, dim + basis)   – per-residue embeddings\n",
    "    \"\"\"\n",
    "    # ---------- EGNN ----------\n",
    "    h_out, coords = egnn_net(z_r, x_r)          # h_out is [tensor] or tensor\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out   # (R, N, dim)\n",
    "\n",
    "    # ---------- RBF on *input* coords (already (R,N,3)) ----------\n",
    "    #d   = torch.cdist(x_r, x_r)            # (R, N, N)\n",
    "    centroids=coords.mean(dim=1).unsqueeze(1)\n",
    "    #rbf = rbf_layer(centroids,coords)                     # (R, N, N, basis)\n",
    "    #print(centroids.shape,coords.shape)\n",
    "    # ---------- concat & attention ----------\n",
    "    h0  = h.transpose(1, 2)                # (R, dim,   N)\n",
    "    #r0  = rbf.transpose(1, 2)        # (R, basis, N)\n",
    "    #tok = torch.cat((h0), dim=1)       # (R, dim+basis, N)\n",
    "    tok=h0\n",
    "    tok, _ = mha_layer(tok.permute(2, 0, 1))   # (N, R, C) → attn(+PE)\n",
    "    tok    = tok.permute(1, 0, 2).max(dim=1).values   # (R, C) max over neighbours\n",
    "    return tok,    centroids.permute(1,0,2)                              # (R, dim + basis)\n",
    "                                         # (R, dim+basis)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) training / validation loop ---------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    # ======== TRAIN ========\n",
    "    egnn_net.train(); rbf_layer.train(); mha_layer.train(); pred_head.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for z, x, y, mask in train_loader:                 # z:(B,S,N)  mask:(B,S)\n",
    "        # compress away padding →  (R, N), (R, N, 3), (R,)\n",
    "        valid      = mask.view(-1)\n",
    "        z_res      = z.view(-1, z.size(2))[valid].to(device)\n",
    "        x_res      = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "        y_res      = y.view(-1)[valid].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #model\n",
    "        feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "        \n",
    "        preds = pred_head(feats)       \n",
    "        t=preds.unsqueeze(0)\n",
    "        preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "        loss  = criterion(preds.flatten(), y_res)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | train L1 = {np.mean(tr_losses):.4f}\")\n",
    "\n",
    "    \n",
    "    # ======== VALID ========\n",
    "    egnn_net.eval(); rbf_layer.eval(); mha_layer.eval(); pred_head.eval()\n",
    "    vl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z, x, y, mask in val_loader:\n",
    "            valid   = mask.view(-1)\n",
    "            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "            \n",
    "            preds = pred_head(feats)       \n",
    "            t=preds.unsqueeze(0)\n",
    "            preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "            loss  = criterion(preds.flatten(), y_res)\n",
    "            vl_losses.append(loss.item())\n",
    "\n",
    "    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "print(time.time() - t0,\"sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rbf, dim 12 6 basis (hd4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train L1 = 3.4965\n",
      "              |  val L1 = 3.8579\n",
      "Epoch   1 | train L1 = 3.3397\n",
      "              |  val L1 = 3.6933\n",
      "Epoch   2 | train L1 = 3.2384\n",
      "              |  val L1 = 3.6057\n",
      "Epoch   3 | train L1 = 3.0111\n",
      "              |  val L1 = 3.4762\n",
      "Epoch   4 | train L1 = 2.7586\n",
      "              |  val L1 = 3.1231\n",
      "Epoch   5 | train L1 = 2.5308\n",
      "              |  val L1 = 2.8734\n",
      "Epoch   6 | train L1 = 2.2200\n",
      "              |  val L1 = 2.2098\n",
      "Epoch   7 | train L1 = 1.8598\n",
      "              |  val L1 = 1.6053\n",
      "Epoch   8 | train L1 = 1.5820\n",
      "              |  val L1 = 1.7834\n",
      "Epoch   9 | train L1 = 1.6601\n",
      "              |  val L1 = 1.5638\n",
      "179.5386073589325 sec\n"
     ]
    }
   ],
   "source": [
    "#centroids.per\n",
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 100\n",
    "BATCH_SIZE  =  1           # not safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 12, 6 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=4\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) model pieces ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "egnn_net = StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                       dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                       num_nearest_neighbors=num_neighbors,\n",
    "                       norm_coors=True,\n",
    "                       num_edge_tokens=num_edge_tokens,\n",
    "                       num_global_tokens=num_global_tokens).to(device)\n",
    "\n",
    "rbf_layer = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device)\n",
    "mha_layer = AttentionBlock(embed_dim=dim + basis,\n",
    "                           num_heads=num_heads,\n",
    "                           hidden_dim=hidden_dim).to(device)\n",
    "pred_head = nn.Linear(dim + basis, 1).to(device)\n",
    "\n",
    "protein_egnn=EGNN(dim=1,update_coors=False,num_nearest_neighbors=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(egnn_net.parameters()) +\n",
    "    list(rbf_layer.parameters()) +\n",
    "    list(mha_layer.parameters()) +\n",
    "    list(pred_head.parameters()) +\n",
    "    list(protein_egnn.parameters()),\n",
    "    lr=5e-3\n",
    ")\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    \"\"\"\n",
    "    z_r : (R, N)       int32   – atomic numbers for R residues\n",
    "    x_r : (R, N, 3)    float32 – coordinates\n",
    "    returns (R, dim + basis)   – per-residue embeddings\n",
    "    \"\"\"\n",
    "    # ---------- EGNN ----------\n",
    "    h_out, coords = egnn_net(z_r, x_r)          # h_out is [tensor] or tensor\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out   # (R, N, dim)\n",
    "\n",
    "    # ---------- RBF on *input* coords (already (R,N,3)) ----------\n",
    "    #d   = torch.cdist(x_r, x_r)            # (R, N, N)\n",
    "    centroids=coords.mean(dim=1).unsqueeze(1)\n",
    "    rbf = rbf_layer(centroids,coords)                     # (R, N, N, basis)\n",
    "    #print(centroids.shape,coords.shape)\n",
    "    # ---------- concat & attention ----------\n",
    "    h0  = h.transpose(1, 2)                # (R, dim,   N)\n",
    "    r0  = rbf.transpose(1, 2)        # (R, basis, N)\n",
    "    tok = torch.cat((r0, h0), dim=1)       # (R, dim+basis, N)\n",
    "\n",
    "    tok, _ = mha_layer(tok.permute(2, 0, 1))   # (N, R, C) → attn(+PE)\n",
    "    tok    = tok.permute(1, 0, 2).max(dim=1).values   # (R, C) max over neighbours\n",
    "    return tok,    centroids.permute(1,0,2)                              # (R, dim + basis)\n",
    "                                         # (R, dim+basis)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) training / validation loop ---------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    # ======== TRAIN ========\n",
    "    egnn_net.train(); rbf_layer.train(); mha_layer.train(); pred_head.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for z, x, y, mask in train_loader:                 # z:(B,S,N)  mask:(B,S)\n",
    "        # compress away padding →  (R, N), (R, N, 3), (R,)\n",
    "        valid      = mask.view(-1)\n",
    "        z_res      = z.view(-1, z.size(2))[valid].to(device)\n",
    "        x_res      = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "        y_res      = y.view(-1)[valid].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #model\n",
    "        feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "        \n",
    "        preds = pred_head(feats)       \n",
    "        t=preds.unsqueeze(0)\n",
    "        preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "        loss  = criterion(preds.flatten(), y_res)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | train L1 = {np.mean(tr_losses):.4f}\")\n",
    "\n",
    "    \n",
    "    # ======== VALID ========\n",
    "    egnn_net.eval(); rbf_layer.eval(); mha_layer.eval(); pred_head.eval()\n",
    "    vl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z, x, y, mask in val_loader:\n",
    "            valid   = mask.view(-1)\n",
    "            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "            \n",
    "            preds = pred_head(feats)       \n",
    "            t=preds.unsqueeze(0)\n",
    "            preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "            loss  = criterion(preds.flatten(), y_res)\n",
    "            vl_losses.append(loss.item())\n",
    "\n",
    "    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "print(time.time() - t0,\"sec\")\n",
    "#mute(1,0,2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW W no pegnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | train L1 = 3.4942\n",
      "              |  val L1 = 3.8465\n",
      "Epoch   1 | train L1 = 3.3362\n",
      "              |  val L1 = 3.7035\n",
      "Epoch   2 | train L1 = 3.2498\n",
      "              |  val L1 = 3.6456\n",
      "Epoch   3 | train L1 = 3.0285\n",
      "              |  val L1 = 3.4390\n",
      "Epoch   4 | train L1 = 2.7406\n",
      "              |  val L1 = 3.3917\n",
      "Epoch   5 | train L1 = 2.5255\n",
      "              |  val L1 = 2.7860\n",
      "Epoch   6 | train L1 = 2.1905\n",
      "              |  val L1 = 2.2727\n",
      "Epoch   7 | train L1 = 1.8957\n",
      "              |  val L1 = 1.7053\n",
      "Epoch   8 | train L1 = 1.6098\n",
      "              |  val L1 = 1.5675\n",
      "Epoch   9 | train L1 = 1.5538\n",
      "              |  val L1 = 1.4907\n",
      "140.04440879821777 sec\n"
     ]
    }
   ],
   "source": [
    "#centroids.per\n",
    "import datetime, time\n",
    "from architecture import *\n",
    "import torch\n",
    "import glob, math, time, datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from egnn_pytorch import EGNN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from egnn_pytorch import EGNN_Network\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "# 0) start timer\n",
    "t0 = time.time()\n",
    "N_NEIGHBORS = 100\n",
    "BATCH_SIZE  =  1           # not safe to increase\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "# reproducibility + device\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# decide AMP only on GP0\n",
    "use_amp = (device.type == \"cuda\")\n",
    "if use_amp:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    class DummyCM:\n",
    "        def __enter__(self): pass\n",
    "        def __exit__(self, *args): pass\n",
    "    autocast = DummyCM\n",
    "    scaler   = None\n",
    "\n",
    "def init_model(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout):\n",
    "    \n",
    "    def build_egnn(dim,depth,hidden_dim,num_neighbors, num_edge_tokens,num_global_tokens,dropout):\n",
    "        return StackedEGNN(\n",
    "            dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            num_positions=1000, num_tokens=118,\n",
    "            num_nearest_neighbors=num_neighbors,\n",
    "            norm_coors=True,\n",
    "            num_edge_tokens=num_edge_tokens,\n",
    "            num_global_tokens=num_global_tokens\n",
    "        )\n",
    "    net   = build_egnn(dim,depth,hidden_dim,num_neighbors,num_edge_tokens,num_global_tokens,dropout).to(device)\n",
    "    mha   = AttentionBlock(embed_dim=dim+basis, num_heads=num_heads, hidden_dim=hidden_dim).to(device)\n",
    "    RBF   = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device) \n",
    "    return net, mha, RBF\n",
    "#net,mha,RBF=init_model\n",
    "# 3) instantiate everything\n",
    "dim, basis = 12, 6 #scale to 3,16 at least # dim must be divisible by 2\n",
    "depth=2 #scale to 2, at least\n",
    "hidden_dim=4\n",
    "num_heads=dim + basis \n",
    "num_edge_tokens=256\n",
    "num_global_tokens=256\n",
    "dropout=0.02\n",
    "cutoff=10.0\n",
    "num_neighbors=2\n",
    "\n",
    "\n",
    "runid=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, torch, glob\n",
    "\n",
    "class InMemoryHoodDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads *.npz files, constructs fixed-size neighbourhoods around each\n",
    "    site (anchor) and stores the result entirely in RAM.\n",
    "\n",
    "    For a protein with S sites the shapes are\n",
    "        z   : (S, N_NEIGHBORS)      int32\n",
    "        pos : (S, N_NEIGHBORS, 3)   float32\n",
    "        y   : (S,)                  float32\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, n_neighbors=N_NEIGHBORS, pin_memory=PIN_MEMORY):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"brute\")\n",
    "\n",
    "        for p in paths:\n",
    "            try:\n",
    "                dat = np.load(p, allow_pickle=True)\n",
    "                z_all   = dat[\"z\"].astype(np.int32)        # (N,)\n",
    "                pos_all = dat[\"pos\"].astype(np.float32)    # (N,3)\n",
    "                sites   = dat[\"sites\"].astype(np.float32)  # (S,3)\n",
    "                y       = dat[\"pks\"].astype(np.float32)    # (S,)\n",
    "\n",
    "                if len(sites) == 0:\n",
    "                    continue  # skip empty entries\n",
    "\n",
    "                nbrs.fit(pos_all)\n",
    "                idx = nbrs.kneighbors(sites, return_distance=False)   # (S, N_NEIGHBORS)\n",
    "\n",
    "                z_hood   = torch.from_numpy(z_all[idx])            # (S,N_NEIGHBORS)\n",
    "                pos_hood = torch.from_numpy(pos_all[idx])          # (S,N_NEIGHBORS,3)\n",
    "                y        = torch.from_numpy(y)                     # (S,)\n",
    "\n",
    "                if pin_memory:\n",
    "                    z_hood   = z_hood.pin_memory()\n",
    "                    pos_hood = pos_hood.pin_memory()\n",
    "                    y        = y.pin_memory()\n",
    "\n",
    "                self.data.append((z_hood, pos_hood, y))\n",
    "            except Exception as e:\n",
    "                print(f\"skipping {p}: {e}\")\n",
    "\n",
    "    def __len__(self):             return len(self.data)\n",
    "    def __getitem__(self, idx):    return self.data[idx]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) collate function  -------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pads the variable-length site dimension so the batch can be stacked\n",
    "    into one tensor.  A boolean mask keeps track of which elements are\n",
    "    real data (True) vs. padding (False).\n",
    "    \"\"\"\n",
    "    # batch = list[(z,pos,y), ...]         len = B\n",
    "    B               = len(batch)\n",
    "    S_max           = max(item[0].shape[0] for item in batch)   # longest protein\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    zs   = torch.zeros (B, S_max, N_NEIGHBORS ,   dtype=torch.int32 , device=device)\n",
    "    pos  = torch.zeros (B, S_max, N_NEIGHBORS ,3, dtype=torch.float32, device=device)\n",
    "    ys   = torch.full  ((B, S_max),  float(\"nan\"), dtype=torch.float32, device=device)\n",
    "    #ys   = torch.full  (B, S_max,               float(\"nan\"),        dtype=torch.float32, device=device)\n",
    "    mask = torch.zeros (B, S_max,                                   dtype=torch.bool,     device=device)\n",
    "\n",
    "    for b,(z,pos_b,y) in enumerate(batch):\n",
    "        S = z.shape[0]\n",
    "        zs  [b, :S] = z.to(device)\n",
    "        pos [b, :S] = pos_b.to(device)\n",
    "        ys  [b, :S] = y.to(device)\n",
    "        mask[b, :S] = True\n",
    "\n",
    "    return zs, pos, ys, mask             # shapes – see above\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 0) parameters you might want to expose at the top of the script\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# 3) data loaders ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "np.random.seed(0)\n",
    "all_paths = glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "np.random.shuffle(all_paths)\n",
    "train_paths, val_paths = all_paths[:20], all_paths[20:30]\n",
    "\n",
    "train_ds = InMemoryHoodDataset(train_paths)\n",
    "val_ds   = InMemoryHoodDataset(val_paths)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,  collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds  , batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, collate_fn=pad_collate,\n",
    "                          num_workers=0, pin_memory=PIN_MEMORY)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) model pieces ------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "egnn_net = StackedEGNN(dim=dim, depth=depth, hidden_dim=hidden_dim,\n",
    "                       dropout=dropout, num_positions=1000, num_tokens=118,\n",
    "                       num_nearest_neighbors=num_neighbors,\n",
    "                       norm_coors=True,\n",
    "                       num_edge_tokens=num_edge_tokens,\n",
    "                       num_global_tokens=num_global_tokens).to(device)\n",
    "\n",
    "rbf_layer = LearnableRBF(num_basis=basis, cutoff=cutoff).to(device)\n",
    "mha_layer = AttentionBlock(embed_dim=dim + basis,\n",
    "                           num_heads=num_heads,\n",
    "                           hidden_dim=hidden_dim).to(device)\n",
    "pred_head = nn.Linear(dim + basis, 1).to(device)\n",
    "\n",
    "protein_egnn=EGNN(dim=1,update_coors=False,num_nearest_neighbors=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(egnn_net.parameters()) +\n",
    "    list(rbf_layer.parameters()) +\n",
    "    list(mha_layer.parameters()) +\n",
    "    list(pred_head.parameters()) +\n",
    "    list(protein_egnn.parameters()),\n",
    "    lr=5e-3\n",
    ")\n",
    "\n",
    "epochs = 10  # or whatever you like\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) forward for a *compressed* batch (R residues, N neighbours)\n",
    "# ---------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) single-path forward – no shape guessing, no branching\n",
    "# ---------------------------------------------------------------------\n",
    "def forward_residues(z_r, x_r):\n",
    "    \"\"\"\n",
    "    z_r : (R, N)       int32   – atomic numbers for R residues\n",
    "    x_r : (R, N, 3)    float32 – coordinates\n",
    "    returns (R, dim + basis)   – per-residue embeddings\n",
    "    \"\"\"\n",
    "    # ---------- EGNN ----------\n",
    "    h_out, coords = egnn_net(z_r, x_r)          # h_out is [tensor] or tensor\n",
    "    h = h_out[0] if isinstance(h_out, (list, tuple)) else h_out   # (R, N, dim)\n",
    "\n",
    "    # ---------- RBF on *input* coords (already (R,N,3)) ----------\n",
    "    #d   = torch.cdist(x_r, x_r)            # (R, N, N)\n",
    "    centroids=coords.mean(dim=1).unsqueeze(1)\n",
    "    rbf = rbf_layer(centroids,coords)                     # (R, N, N, basis)\n",
    "    #print(centroids.shape,coords.shape)\n",
    "    # ---------- concat & attention ----------\n",
    "    h0  = h.transpose(1, 2)                # (R, dim,   N)\n",
    "    r0  = rbf.transpose(1, 2)        # (R, basis, N)\n",
    "    tok = torch.cat((r0, h0), dim=1)       # (R, dim+basis, N)\n",
    "\n",
    "    tok, _ = mha_layer(tok.permute(2, 0, 1))   # (N, R, C) → attn(+PE)\n",
    "    tok    = tok.permute(1, 0, 2).max(dim=1).values   # (R, C) max over neighbours\n",
    "    return tok,    centroids.permute(1,0,2)                              # (R, dim + basis)\n",
    "                                         # (R, dim+basis)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) training / validation loop ---------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "for epoch in range(epochs):\n",
    "    # ======== TRAIN ========\n",
    "    egnn_net.train(); rbf_layer.train(); mha_layer.train(); pred_head.train()\n",
    "    tr_losses = []\n",
    "\n",
    "    for z, x, y, mask in train_loader:                 # z:(B,S,N)  mask:(B,S)\n",
    "        # compress away padding →  (R, N), (R, N, 3), (R,)\n",
    "        valid      = mask.view(-1)\n",
    "        z_res      = z.view(-1, z.size(2))[valid].to(device)\n",
    "        x_res      = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "        y_res      = y.view(-1)[valid].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #model\n",
    "        feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "        \n",
    "        preds = pred_head(feats)       \n",
    "        t=preds.unsqueeze(0)\n",
    "        #preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "        loss  = criterion(preds.flatten(), y_res)\n",
    "\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); optimizer.step()\n",
    "\n",
    "        tr_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | train L1 = {np.mean(tr_losses):.4f}\")\n",
    "\n",
    "    \n",
    "    # ======== VALID ========\n",
    "    egnn_net.eval(); rbf_layer.eval(); mha_layer.eval(); pred_head.eval()\n",
    "    vl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z, x, y, mask in val_loader:\n",
    "            valid   = mask.view(-1)\n",
    "            z_res   = z.view(-1, z.size(2))[valid].to(device)\n",
    "            x_res   = x.view(-1, x.size(2), 3)[valid].to(device)\n",
    "            y_res   = y.view(-1)[valid].to(device)\n",
    "\n",
    "                #model\n",
    "            feats, centroids = forward_residues(z_res, x_res)         # (R, C)\n",
    "            \n",
    "            preds = pred_head(feats)       \n",
    "            t=preds.unsqueeze(0)\n",
    "            #preds=protein_egnn(t,centroids)[0]\n",
    "\n",
    "            loss  = criterion(preds.flatten(), y_res)\n",
    "            vl_losses.append(loss.item())\n",
    "\n",
    "    print(f\"              |  val L1 = {np.mean(vl_losses):.4f}\")\n",
    "print(time.time() - t0,\"sec\")\n",
    "#mute(1,0,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../data/pkegnn_INS/inputs/1afl.npz',\n",
       " '../../../data/pkegnn_INS/inputs/3n9x.npz',\n",
       " '../../../data/pkegnn_INS/inputs/5ftg.npz',\n",
       " '../../../data/pkegnn_INS/inputs/4kz7.npz',\n",
       " '../../../data/pkegnn_INS/inputs/3f12.npz',\n",
       " '../../../data/pkegnn_INS/inputs/6dy0.npz',\n",
       " '../../../data/pkegnn_INS/inputs/2lwl.npz',\n",
       " '../../../data/pkegnn_INS/inputs/3di1.npz',\n",
       " '../../../data/pkegnn_INS/inputs/5i4i.npz',\n",
       " '../../../data/pkegnn_INS/inputs/5j3y.npz']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no pegnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 111, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
