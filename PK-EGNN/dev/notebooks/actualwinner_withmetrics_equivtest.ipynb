{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_004152\n",
      "params: 19477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 2.0621 | mae val 1.2336\n",
      "     additional metrics:  [1/10]  train 7.2658 | val 2.8827\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b8b2daecacc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;31m# ================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mtr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0mva\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mva_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudy_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b8b2daecacc9>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cfg, loader, train)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b8b2daecacc9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0megnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                \u001b[0;31m# (R,N,dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mcent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# (R,1,3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pkegnn/pKaSchNet/PK-EGNN/results/thinkpad_results/architecture.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mh_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mh_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mh_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pkegnn/pKaSchNet/PK-EGNN/results/thinkpad_results/architecture.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mh_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0megnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/egnn_pytorch/egnn_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feats, coors, adj_mat, edges, mask, return_coor_changes)\u001b[0m\n\u001b[1;32m    446\u001b[0m                 \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0megnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0mcoor_changes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/egnn_pytorch/egnn_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feats, coors, edges, mask, adj_mat)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0medge_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mm_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_gate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_005810\n",
      "params: 17695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.2886 | mae val 1.1465\n",
      "     additional metrics:  [1/10]  train 3.5299 | val 3.1183\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6176a4d8a3df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;31m# ================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mtr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0mva\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mva_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudy_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-6176a4d8a3df>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cfg, loader, train)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudy_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "            #print(preds.shape,to)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=1.550e-06\n",
      "[batch 0  rot 1]  max|Δ|=1.788e-06\n",
      "[batch 0  rot 2]  max|Δ|=1.431e-06\n",
      "[batch 0  rot 3]  max|Δ|=1.788e-06\n",
      "[batch 0  rot 4]  max|Δ|=1.907e-06\n",
      "[batch 0  perm‑K ]  max|Δ|=2.034e+00\n",
      "[batch 0  perm‑R ]  max|Δ|=2.384e-07\n",
      "-------------------------------------------------------\n",
      "[batch 1  rot 0]  max|Δ|=2.623e-06\n",
      "[batch 1  rot 1]  max|Δ|=2.861e-06\n",
      "[batch 1  rot 2]  max|Δ|=3.457e-06\n",
      "[batch 1  rot 3]  max|Δ|=2.265e-06\n",
      "[batch 1  rot 4]  max|Δ|=2.265e-06\n",
      "[batch 1  perm‑K ]  max|Δ|=1.887e+00\n",
      "[batch 1  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "[batch 2  rot 0]  max|Δ|=2.384e-06\n",
      "[batch 2  rot 1]  max|Δ|=2.146e-06\n",
      "[batch 2  rot 2]  max|Δ|=2.623e-06\n",
      "[batch 2  rot 3]  max|Δ|=1.848e-06\n",
      "[batch 2  rot 4]  max|Δ|=2.623e-06\n",
      "[batch 2  perm‑K ]  max|Δ|=2.301e+00\n",
      "[batch 2  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 3.457e-06\n",
      "permK : 2.301e+00\n",
      "permR : 2.384e-07\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_010144\n",
      "params: 17695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1]  train mae 1.6055 | mae val 1.2054\n",
      "     additional metrics:  [1/1]  train 3.9628 | val 2.4540\n",
      "\n",
      "0.7018108367919922 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=9.537e-07\n",
      "[batch 0  rot 1]  max|Δ|=9.537e-07\n",
      "[batch 0  rot 2]  max|Δ|=9.537e-07\n",
      "[batch 0  rot 3]  max|Δ|=1.311e-06\n",
      "[batch 0  rot 4]  max|Δ|=7.153e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=5.068e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.311e-06\n",
      "permK : 5.068e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "            #print(preds.shape,to)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=1,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_010526\n",
      "params: 16367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1]  train mae 1.5971 | mae val 1.2055\n",
      "     additional metrics:  [1/1]  train 3.9318 | val 2.4525\n",
      "\n",
      "0.7177050113677979 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=0.000e+00\n",
      "[batch 0  rot 1]  max|Δ|=0.000e+00\n",
      "[batch 0  rot 2]  max|Δ|=0.000e+00\n",
      "[batch 0  rot 3]  max|Δ|=0.000e+00\n",
      "[batch 0  rot 4]  max|Δ|=0.000e+00\n",
      "[batch 0  perm‑K ]  max|Δ|=3.877e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 0.000e+00\n",
      "permK : 3.877e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "            #print(preds.shape,to)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=1,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_010614\n",
      "params: 16367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1]  train mae 1.6956 | mae val 1.0550\n",
      "     additional metrics:  [1/1]  train 4.2723 | val 2.1244\n",
      "\n",
      "0.9071965217590332 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 1]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 2]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 3]  max|Δ|=5.960e-08\n",
      "[batch 0  rot 4]  max|Δ|=1.192e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=4.585e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.192e-07\n",
      "permK : 4.585e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "            #print(preds.shape,to)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=1,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ KNN neighbours are distance‑sorted (checked 5 samples)\n"
     ]
    }
   ],
   "source": [
    "def sanity_check_knn(dataset, n_samples=5):\n",
    "    for i in range(min(n_samples, len(dataset))):\n",
    "        z, pos, y = dataset.data[i]          # shapes: (S,K), (S,K,3)\n",
    "        sites = pos[:, 0]                       # central atoms (K=hood_k)\n",
    "        for s in range(len(sites)):\n",
    "            d = ((pos[s] - sites[s]).norm(dim=1))   # K distances\n",
    "            if not torch.all(d[1:] >= d[:-1]):      # monotonic?\n",
    "                print(f\"❌ sample {i} site {s} not sorted!\")\n",
    "                return\n",
    "    print(\"✔ KNN neighbours are distance‑sorted (checked {:d} samples)\".format(n_samples))\n",
    "\n",
    "sanity_check_knn(train_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_011231\n",
      "params: 19479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1]  train mae 1.2278 | mae val 4.1253\n",
      "     additional metrics:  [1/1]  train 3.2103 | val 18.9056\n",
      "\n",
      "0.9689278602600098 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=2.384e-06\n",
      "[batch 0  rot 1]  max|Δ|=2.384e-06\n",
      "[batch 0  rot 2]  max|Δ|=1.907e-06\n",
      "[batch 0  rot 3]  max|Δ|=2.384e-06\n",
      "[batch 0  rot 4]  max|Δ|=2.384e-06\n",
      "[batch 0  perm‑K ]  max|Δ|=5.460e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 2.384e-06\n",
      "permK : 5.460e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "            #print(preds.shape,to)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=1,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m_ij  = φ_e(h_i, h_j, ‖x_i – x_j‖)         # edge message\n",
    "m_i   = Σ_j m_ij                            # SUM over neighbours\n",
    "h_i'  = φ_h(h_i, m_i)                       # new features\n",
    "x_i'  = x_i + φ_x(m_i) (co‑ordinate update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_012131\n",
      "params: 16348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1]  train mae 2.0103 | mae val 1.6925\n",
      "     additional metrics:  [1/1]  train 5.4504 | val 3.9141\n",
      "\n",
      "0.729222297668457 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 1]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 2]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 3]  max|Δ|=0.000e+00\n",
      "[batch 0  rot 4]  max|Δ|=1.192e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=8.344e-02\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.192e-07\n",
      "permK : 8.344e-02\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator == 'linear':\n",
    "            preds = self.agg(tok).max(1).values                 # (R,1)\n",
    "        elif self.c.aggregator == 'nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:                           # pool\n",
    "            # THIS must be max over dim=2 (neighbours!), not dim=1\n",
    "            preds = tok.max(2).values.mean(1, keepdim=True)     # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=1,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='pool',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_012459\n",
      "params: 16348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1]  train mae 2.0103 | mae val 1.6925\n",
      "     additional metrics:  [1/1]  train 5.4504 | val 3.9141\n",
      "\n",
      "0.9247465133666992 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 1]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 2]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 3]  max|Δ|=0.000e+00\n",
      "[batch 0  rot 4]  max|Δ|=1.192e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=8.344e-02\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.192e-07\n",
      "permK : 8.344e-02\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator == 'linear':\n",
    "            preds = self.agg(tok).max(1).values                 # (R,1)\n",
    "        elif self.c.aggregator == 'nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:                           # pool\n",
    "            # THIS must be max over dim=2 (neighbours!), not dim=1\n",
    "            preds = tok.max(2).values.mean(1, keepdim=True)     # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=1,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='pool',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_013022\n",
      "params: 16348\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 2.0679 | mae val 1.6369\n",
      "     additional metrics:  [1/10]  train 5.7051 | val 3.7122\n",
      "\n",
      "[2/10]  train mae 1.9043 | mae val 1.5044\n",
      "     additional metrics:  [2/10]  train 5.0144 | val 3.2630\n",
      "\n",
      "[3/10]  train mae 1.7744 | mae val 1.4418\n",
      "     additional metrics:  [3/10]  train 4.5309 | val 3.0694\n",
      "\n",
      "[4/10]  train mae 1.7134 | mae val 1.3815\n",
      "     additional metrics:  [4/10]  train 4.3170 | val 2.8967\n",
      "\n",
      "[5/10]  train mae 1.6554 | mae val 1.3670\n",
      "     additional metrics:  [5/10]  train 4.1240 | val 2.8574\n",
      "\n",
      "[6/10]  train mae 1.6412 | mae val 1.3597\n",
      "     additional metrics:  [6/10]  train 4.0755 | val 2.8380\n",
      "\n",
      "[7/10]  train mae 1.6349 | mae val 1.3280\n",
      "     additional metrics:  [7/10]  train 4.0553 | val 2.7526\n",
      "\n",
      "[8/10]  train mae 1.6060 | mae val 1.3082\n",
      "     additional metrics:  [8/10]  train 3.9623 | val 2.6997\n",
      "\n",
      "[9/10]  train mae 1.5867 | mae val 1.2715\n",
      "     additional metrics:  [9/10]  train 3.9030 | val 2.6055\n",
      "\n",
      "[10/10]  train mae 1.5501 | mae val 1.2268\n",
      "     additional metrics:  [10/10]  train 3.7960 | val 2.4979\n",
      "\n",
      "5.861749649047852 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=1.788e-07\n",
      "[batch 0  rot 1]  max|Δ|=1.788e-07\n",
      "[batch 0  rot 2]  max|Δ|=1.788e-07\n",
      "[batch 0  rot 3]  max|Δ|=1.788e-07\n",
      "[batch 0  rot 4]  max|Δ|=1.788e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.680e-03\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.788e-07\n",
      "permK : 1.680e-03\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.0030815601348876953\n"
     ]
    }
   ],
   "source": [
    "#winner\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator == 'linear':\n",
    "            preds = self.agg(tok).max(1).values                 # (R,1)\n",
    "        elif self.c.aggregator == 'nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:                           # pool\n",
    "            # THIS must be max over dim=2 (neighbours!), not dim=1\n",
    "            preds = tok.max(2).values.mean(1, keepdim=True)     # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n",
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='pool',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_013112\n",
      "params: 16367\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.3678 | mae val 1.0182\n",
      "     additional metrics:  [1/10]  train 3.3569 | val 2.1280\n",
      "\n",
      "[2/10]  train mae 1.2794 | mae val 1.0285\n",
      "     additional metrics:  [2/10]  train 3.1727 | val 2.1822\n",
      "\n",
      "[3/10]  train mae 1.2337 | mae val 1.0614\n",
      "     additional metrics:  [3/10]  train 3.1268 | val 2.2614\n",
      "\n",
      "[4/10]  train mae 1.2274 | mae val 1.0644\n",
      "     additional metrics:  [4/10]  train 3.1727 | val 2.2682\n",
      "\n",
      "[5/10]  train mae 1.2321 | mae val 1.0470\n",
      "     additional metrics:  [5/10]  train 3.2208 | val 2.2196\n",
      "\n",
      "[6/10]  train mae 1.2248 | mae val 1.0359\n",
      "     additional metrics:  [6/10]  train 3.1890 | val 2.1889\n",
      "\n",
      "[7/10]  train mae 1.2200 | mae val 1.0254\n",
      "     additional metrics:  [7/10]  train 3.1695 | val 2.1593\n",
      "\n",
      "[8/10]  train mae 1.2162 | mae val 1.0156\n",
      "     additional metrics:  [8/10]  train 3.1511 | val 2.1338\n",
      "\n",
      "[9/10]  train mae 1.2144 | mae val 1.0106\n",
      "     additional metrics:  [9/10]  train 3.1356 | val 2.1154\n",
      "\n",
      "[10/10]  train mae 1.2138 | mae val 1.0083\n",
      "     additional metrics:  [10/10]  train 3.1266 | val 2.1051\n",
      "\n",
      "6.558538436889648 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=5.960e-08\n",
      "[batch 0  rot 1]  max|Δ|=8.196e-08\n",
      "[batch 0  rot 2]  max|Δ|=5.960e-08\n",
      "[batch 0  rot 3]  max|Δ|=5.960e-08\n",
      "[batch 0  rot 4]  max|Δ|=5.960e-08\n",
      "[batch 0  perm‑K ]  max|Δ|=3.758e-02\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 8.196e-08\n",
      "permK : 3.758e-02\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.0629848837852478\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_013210\n",
      "params: 16367\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.3885 | mae val 1.0231\n",
      "     additional metrics:  [1/10]  train 3.3832 | val 2.1185\n",
      "\n",
      "[2/10]  train mae 1.2816 | mae val 1.0145\n",
      "     additional metrics:  [2/10]  train 3.1687 | val 2.1319\n",
      "\n",
      "[3/10]  train mae 1.2249 | mae val 1.0412\n",
      "     additional metrics:  [3/10]  train 3.1034 | val 2.1972\n",
      "\n",
      "[4/10]  train mae 1.2246 | mae val 1.0401\n",
      "     additional metrics:  [4/10]  train 3.1733 | val 2.1897\n",
      "\n",
      "[5/10]  train mae 1.2235 | mae val 1.0248\n",
      "     additional metrics:  [5/10]  train 3.1738 | val 2.1414\n",
      "\n",
      "[6/10]  train mae 1.2154 | mae val 1.0156\n",
      "     additional metrics:  [6/10]  train 3.1317 | val 2.1062\n",
      "\n",
      "[7/10]  train mae 1.2105 | mae val 1.0125\n",
      "     additional metrics:  [7/10]  train 3.1010 | val 2.0948\n",
      "\n",
      "[8/10]  train mae 1.2097 | mae val 1.0116\n",
      "     additional metrics:  [8/10]  train 3.0911 | val 2.0895\n",
      "\n",
      "[9/10]  train mae 1.2081 | mae val 1.0119\n",
      "     additional metrics:  [9/10]  train 3.0841 | val 2.0881\n",
      "\n",
      "[10/10]  train mae 1.2059 | mae val 1.0132\n",
      "     additional metrics:  [10/10]  train 3.0786 | val 2.0910\n",
      "\n",
      "6.888924598693848 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=1.356e-06\n",
      "[batch 0  rot 1]  max|Δ|=2.384e-06\n",
      "[batch 0  rot 2]  max|Δ|=2.563e-06\n",
      "[batch 0  rot 3]  max|Δ|=1.848e-06\n",
      "[batch 0  rot 4]  max|Δ|=5.156e-06\n",
      "[batch 0  perm‑K ]  max|Δ|=8.739e-02\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 5.156e-06\n",
      "permK : 8.739e-02\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.1619591861963272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_013329\n",
      "params: 16367\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.5507 | mae val 1.0079\n",
      "     additional metrics:  [1/10]  train 3.8115 | val 2.0867\n",
      "\n",
      "[2/10]  train mae 1.2471 | mae val 1.1769\n",
      "     additional metrics:  [2/10]  train 3.1513 | val 2.5985\n",
      "\n",
      "[3/10]  train mae 1.3027 | mae val 1.2052\n",
      "     additional metrics:  [3/10]  train 3.4073 | val 2.6954\n",
      "\n",
      "[4/10]  train mae 1.3337 | mae val 1.1403\n",
      "     additional metrics:  [4/10]  train 3.5148 | val 2.5015\n",
      "\n",
      "[5/10]  train mae 1.2920 | mae val 1.0555\n",
      "     additional metrics:  [5/10]  train 3.3721 | val 2.2421\n",
      "\n",
      "[6/10]  train mae 1.2332 | mae val 1.0267\n",
      "     additional metrics:  [6/10]  train 3.1390 | val 2.1562\n",
      "\n",
      "[7/10]  train mae 1.2295 | mae val 1.0116\n",
      "     additional metrics:  [7/10]  train 3.1168 | val 2.1080\n",
      "\n",
      "[8/10]  train mae 1.2206 | mae val 1.0090\n",
      "     additional metrics:  [8/10]  train 3.0837 | val 2.0860\n",
      "\n",
      "[9/10]  train mae 1.2360 | mae val 1.0097\n",
      "     additional metrics:  [9/10]  train 3.1314 | val 2.0811\n",
      "\n",
      "[10/10]  train mae 1.2409 | mae val 1.0091\n",
      "     additional metrics:  [10/10]  train 3.1421 | val 2.0831\n",
      "\n",
      "7.352017641067505 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=7.451e-08\n",
      "[batch 0  rot 1]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 2]  max|Δ|=1.043e-07\n",
      "[batch 0  rot 3]  max|Δ|=8.941e-08\n",
      "[batch 0  rot 4]  max|Δ|=1.192e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.092e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.192e-07\n",
      "permK : 1.092e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.09945277869701385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std of all pos_emb weights: [0.0]\n",
      "perm‑K  max|Δ| = 3.5762786865234375e-07\n"
     ]
    }
   ],
   "source": [
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_013423\n",
      "params: 18149\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.2849 | mae val 1.9432\n",
      "     additional metrics:  [1/10]  train 3.0402 | val 5.3374\n",
      "\n",
      "[2/10]  train mae 1.9140 | mae val 1.3153\n",
      "     additional metrics:  [2/10]  train 5.3635 | val 2.6428\n",
      "\n",
      "[3/10]  train mae 1.4063 | mae val 1.3795\n",
      "     additional metrics:  [3/10]  train 2.8902 | val 2.8052\n",
      "\n",
      "[4/10]  train mae 1.4163 | mae val 0.9753\n",
      "     additional metrics:  [4/10]  train 2.8889 | val 1.9173\n",
      "\n",
      "[5/10]  train mae 0.9356 | mae val 1.1540\n",
      "     additional metrics:  [5/10]  train 1.7292 | val 2.5166\n",
      "\n",
      "[6/10]  train mae 1.0839 | mae val 1.2083\n",
      "     additional metrics:  [6/10]  train 2.0315 | val 2.7017\n",
      "\n",
      "[7/10]  train mae 1.0911 | mae val 1.0455\n",
      "     additional metrics:  [7/10]  train 2.0044 | val 2.1617\n",
      "\n",
      "[8/10]  train mae 0.8662 | mae val 1.0175\n",
      "     additional metrics:  [8/10]  train 1.3532 | val 1.9758\n",
      "\n",
      "[9/10]  train mae 0.7540 | mae val 1.0620\n",
      "     additional metrics:  [9/10]  train 1.1012 | val 2.0516\n",
      "\n",
      "[10/10]  train mae 0.7726 | mae val 1.0604\n",
      "     additional metrics:  [10/10]  train 1.1158 | val 2.0514\n",
      "\n",
      "6.1261584758758545 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=7.153e-07\n",
      "[batch 0  rot 1]  max|Δ|=5.960e-07\n",
      "[batch 0  rot 2]  max|Δ|=4.768e-07\n",
      "[batch 0  rot 3]  max|Δ|=5.960e-07\n",
      "[batch 0  rot 4]  max|Δ|=3.576e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=3.184e+00\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 7.153e-07\n",
      "permK : 3.184e+00\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 3.9764692783355713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_014018\n",
      "params: 16367\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.3678 | mae val 1.0182\n",
      "     additional metrics:  [1/10]  train 3.3569 | val 2.1280\n",
      "\n",
      "[2/10]  train mae 1.2794 | mae val 1.0285\n",
      "     additional metrics:  [2/10]  train 3.1727 | val 2.1822\n",
      "\n",
      "[3/10]  train mae 1.2337 | mae val 1.0614\n",
      "     additional metrics:  [3/10]  train 3.1268 | val 2.2614\n",
      "\n",
      "[4/10]  train mae 1.2274 | mae val 1.0644\n",
      "     additional metrics:  [4/10]  train 3.1727 | val 2.2682\n",
      "\n",
      "[5/10]  train mae 1.2321 | mae val 1.0470\n",
      "     additional metrics:  [5/10]  train 3.2208 | val 2.2196\n",
      "\n",
      "[6/10]  train mae 1.2248 | mae val 1.0359\n",
      "     additional metrics:  [6/10]  train 3.1890 | val 2.1889\n",
      "\n",
      "[7/10]  train mae 1.2200 | mae val 1.0254\n",
      "     additional metrics:  [7/10]  train 3.1695 | val 2.1593\n",
      "\n",
      "[8/10]  train mae 1.2162 | mae val 1.0156\n",
      "     additional metrics:  [8/10]  train 3.1511 | val 2.1338\n",
      "\n",
      "[9/10]  train mae 1.2144 | mae val 1.0106\n",
      "     additional metrics:  [9/10]  train 3.1356 | val 2.1154\n",
      "\n",
      "[10/10]  train mae 1.2138 | mae val 1.0083\n",
      "     additional metrics:  [10/10]  train 3.1266 | val 2.1051\n",
      "\n",
      "5.500387668609619 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=5.960e-08\n",
      "[batch 0  rot 1]  max|Δ|=8.196e-08\n",
      "[batch 0  rot 2]  max|Δ|=5.960e-08\n",
      "[batch 0  rot 3]  max|Δ|=5.960e-08\n",
      "[batch 0  rot 4]  max|Δ|=5.960e-08\n",
      "[batch 0  perm‑K ]  max|Δ|=3.758e-02\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 8.196e-08\n",
      "permK : 3.758e-02\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.0629848837852478\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_014057\n",
      "params: 16367\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.5507 | mae val 1.0079\n",
      "     additional metrics:  [1/10]  train 3.8115 | val 2.0867\n",
      "\n",
      "[2/10]  train mae 1.2471 | mae val 1.1769\n",
      "     additional metrics:  [2/10]  train 3.1513 | val 2.5985\n",
      "\n",
      "[3/10]  train mae 1.3027 | mae val 1.2052\n",
      "     additional metrics:  [3/10]  train 3.4073 | val 2.6954\n",
      "\n",
      "[4/10]  train mae 1.3337 | mae val 1.1403\n",
      "     additional metrics:  [4/10]  train 3.5148 | val 2.5015\n",
      "\n",
      "[5/10]  train mae 1.2920 | mae val 1.0555\n",
      "     additional metrics:  [5/10]  train 3.3721 | val 2.2421\n",
      "\n",
      "[6/10]  train mae 1.2332 | mae val 1.0267\n",
      "     additional metrics:  [6/10]  train 3.1390 | val 2.1562\n",
      "\n",
      "[7/10]  train mae 1.2295 | mae val 1.0116\n",
      "     additional metrics:  [7/10]  train 3.1168 | val 2.1080\n",
      "\n",
      "[8/10]  train mae 1.2206 | mae val 1.0090\n",
      "     additional metrics:  [8/10]  train 3.0837 | val 2.0860\n",
      "\n",
      "[9/10]  train mae 1.2360 | mae val 1.0097\n",
      "     additional metrics:  [9/10]  train 3.1314 | val 2.0811\n",
      "\n",
      "[10/10]  train mae 1.2409 | mae val 1.0091\n",
      "     additional metrics:  [10/10]  train 3.1421 | val 2.0831\n",
      "\n",
      "8.429926872253418 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=7.451e-08\n",
      "[batch 0  rot 1]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 2]  max|Δ|=1.043e-07\n",
      "[batch 0  rot 3]  max|Δ|=8.941e-08\n",
      "[batch 0  rot 4]  max|Δ|=1.192e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.092e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.192e-07\n",
      "permK : 1.092e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.09945277869701385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_014149\n",
      "params: 16369\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.3911 | mae val 1.0238\n",
      "     additional metrics:  [1/10]  train 3.7992 | val 2.1272\n",
      "\n",
      "[2/10]  train mae 1.2327 | mae val 1.0621\n",
      "     additional metrics:  [2/10]  train 3.2020 | val 2.1575\n",
      "\n",
      "[3/10]  train mae 1.2938 | mae val 1.0441\n",
      "     additional metrics:  [3/10]  train 3.2189 | val 2.1229\n",
      "\n",
      "[4/10]  train mae 1.2640 | mae val 1.0159\n",
      "     additional metrics:  [4/10]  train 3.1567 | val 2.0934\n",
      "\n",
      "[5/10]  train mae 1.2257 | mae val 1.0162\n",
      "     additional metrics:  [5/10]  train 3.1480 | val 2.1503\n",
      "\n",
      "[6/10]  train mae 1.2413 | mae val 1.0351\n",
      "     additional metrics:  [6/10]  train 3.2236 | val 2.2005\n",
      "\n",
      "[7/10]  train mae 1.2562 | mae val 1.0373\n",
      "     additional metrics:  [7/10]  train 3.2569 | val 2.1939\n",
      "\n",
      "[8/10]  train mae 1.2633 | mae val 1.0280\n",
      "     additional metrics:  [8/10]  train 3.2703 | val 2.1653\n",
      "\n",
      "[9/10]  train mae 1.2534 | mae val 1.0189\n",
      "     additional metrics:  [9/10]  train 3.2261 | val 2.1390\n",
      "\n",
      "[10/10]  train mae 1.2480 | mae val 1.0105\n",
      "     additional metrics:  [10/10]  train 3.2355 | val 2.1124\n",
      "\n",
      "8.121310949325562 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 1]  max|Δ|=8.941e-08\n",
      "[batch 0  rot 2]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 3]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 4]  max|Δ|=1.192e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.466e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.192e-07\n",
      "permK : 1.466e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.09251554310321808\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_014222\n",
      "params: 17697\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.8071 | mae val 1.1899\n",
      "     additional metrics:  [1/10]  train 5.4303 | val 2.6614\n",
      "\n",
      "[2/10]  train mae 1.3854 | mae val 1.0166\n",
      "     additional metrics:  [2/10]  train 3.7577 | val 2.1186\n",
      "\n",
      "[3/10]  train mae 1.2363 | mae val 1.0548\n",
      "     additional metrics:  [3/10]  train 3.1941 | val 2.1412\n",
      "\n",
      "[4/10]  train mae 1.2867 | mae val 1.0817\n",
      "     additional metrics:  [4/10]  train 3.1528 | val 2.1956\n",
      "\n",
      "[5/10]  train mae 1.3186 | mae val 1.0695\n",
      "     additional metrics:  [5/10]  train 3.2172 | val 2.1704\n",
      "\n",
      "[6/10]  train mae 1.3116 | mae val 1.0436\n",
      "     additional metrics:  [6/10]  train 3.2500 | val 2.1172\n",
      "\n",
      "[7/10]  train mae 1.2843 | mae val 1.0298\n",
      "     additional metrics:  [7/10]  train 3.1945 | val 2.0954\n",
      "\n",
      "[8/10]  train mae 1.2538 | mae val 1.0191\n",
      "     additional metrics:  [8/10]  train 3.1881 | val 2.0842\n",
      "\n",
      "[9/10]  train mae 1.2483 | mae val 1.0114\n",
      "     additional metrics:  [9/10]  train 3.1594 | val 2.0878\n",
      "\n",
      "[10/10]  train mae 1.2266 | mae val 1.0092\n",
      "     additional metrics:  [10/10]  train 3.1378 | val 2.1032\n",
      "\n",
      "8.705018758773804 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 1]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 2]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 3]  max|Δ|=1.192e-07\n",
      "[batch 0  rot 4]  max|Δ|=1.192e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.007e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.192e-07\n",
      "permK : 1.007e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.06460964679718018\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_014251\n",
      "params: 16369\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.3937 | mae val 1.0243\n",
      "     additional metrics:  [1/10]  train 3.8035 | val 2.1324\n",
      "\n",
      "[2/10]  train mae 1.2349 | mae val 1.0654\n",
      "     additional metrics:  [2/10]  train 3.2066 | val 2.1650\n",
      "\n",
      "[3/10]  train mae 1.2997 | mae val 1.0471\n",
      "     additional metrics:  [3/10]  train 3.2265 | val 2.1290\n",
      "\n",
      "[4/10]  train mae 1.2703 | mae val 1.0156\n",
      "     additional metrics:  [4/10]  train 3.1630 | val 2.0852\n",
      "\n",
      "[5/10]  train mae 1.2241 | mae val 1.0124\n",
      "     additional metrics:  [5/10]  train 3.1328 | val 2.1259\n",
      "\n",
      "[6/10]  train mae 1.2329 | mae val 1.0388\n",
      "     additional metrics:  [6/10]  train 3.1838 | val 2.2051\n",
      "\n",
      "[7/10]  train mae 1.2619 | mae val 1.0506\n",
      "     additional metrics:  [7/10]  train 3.2795 | val 2.2364\n",
      "\n",
      "[8/10]  train mae 1.2756 | mae val 1.0411\n",
      "     additional metrics:  [8/10]  train 3.3172 | val 2.2054\n",
      "\n",
      "[9/10]  train mae 1.2694 | mae val 1.0226\n",
      "     additional metrics:  [9/10]  train 3.2686 | val 2.1484\n",
      "\n",
      "[10/10]  train mae 1.2465 | mae val 1.0138\n",
      "     additional metrics:  [10/10]  train 3.2430 | val 2.1231\n",
      "\n",
      "8.632115840911865 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=5.811e-07\n",
      "[batch 0  rot 1]  max|Δ|=8.345e-07\n",
      "[batch 0  rot 2]  max|Δ|=3.874e-07\n",
      "[batch 0  rot 3]  max|Δ|=5.662e-07\n",
      "[batch 0  rot 4]  max|Δ|=3.278e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.291e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 8.345e-07\n",
      "permK : 1.291e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.13932392001152039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_014627\n",
      "params: 17697\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.8114 | mae val 1.1944\n",
      "     additional metrics:  [1/10]  train 5.4368 | val 2.6812\n",
      "\n",
      "[2/10]  train mae 1.3855 | mae val 1.0176\n",
      "     additional metrics:  [2/10]  train 3.7635 | val 2.1219\n",
      "\n",
      "[3/10]  train mae 1.2359 | mae val 1.0543\n",
      "     additional metrics:  [3/10]  train 3.1869 | val 2.1408\n",
      "\n",
      "[4/10]  train mae 1.2889 | mae val 1.0848\n",
      "     additional metrics:  [4/10]  train 3.1580 | val 2.2013\n",
      "\n",
      "[5/10]  train mae 1.3212 | mae val 1.0738\n",
      "     additional metrics:  [5/10]  train 3.2142 | val 2.1788\n",
      "\n",
      "[6/10]  train mae 1.3132 | mae val 1.0469\n",
      "     additional metrics:  [6/10]  train 3.2433 | val 2.1241\n",
      "\n",
      "[7/10]  train mae 1.2861 | mae val 1.0330\n",
      "     additional metrics:  [7/10]  train 3.1852 | val 2.1020\n",
      "\n",
      "[8/10]  train mae 1.2504 | mae val 1.0217\n",
      "     additional metrics:  [8/10]  train 3.1760 | val 2.0898\n",
      "\n",
      "[9/10]  train mae 1.2464 | mae val 1.0120\n",
      "     additional metrics:  [9/10]  train 3.1592 | val 2.0907\n",
      "\n",
      "[10/10]  train mae 1.2272 | mae val 1.0113\n",
      "     additional metrics:  [10/10]  train 3.1405 | val 2.1054\n",
      "\n",
      "8.385558605194092 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 1]  max|Δ|=4.768e-07\n",
      "[batch 0  rot 2]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 3]  max|Δ|=4.768e-07\n",
      "[batch 0  rot 4]  max|Δ|=2.384e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.205e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 4.768e-07\n",
      "permK : 1.205e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.08558619022369385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_014447\n",
      "params: 17705\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.3635 | mae val 1.0526\n",
      "     additional metrics:  [1/10]  train 3.6311 | val 2.2125\n",
      "\n",
      "[2/10]  train mae 1.2415 | mae val 1.0090\n",
      "     additional metrics:  [2/10]  train 3.2189 | val 2.0988\n",
      "\n",
      "[3/10]  train mae 1.2353 | mae val 1.0189\n",
      "     additional metrics:  [3/10]  train 3.1706 | val 2.0925\n",
      "\n",
      "[4/10]  train mae 1.2461 | mae val 1.0235\n",
      "     additional metrics:  [4/10]  train 3.1474 | val 2.0977\n",
      "\n",
      "[5/10]  train mae 1.2537 | mae val 1.0192\n",
      "     additional metrics:  [5/10]  train 3.1591 | val 2.0929\n",
      "\n",
      "[6/10]  train mae 1.2430 | mae val 1.0136\n",
      "     additional metrics:  [6/10]  train 3.1418 | val 2.0911\n",
      "\n",
      "[7/10]  train mae 1.2286 | mae val 1.0118\n",
      "     additional metrics:  [7/10]  train 3.1184 | val 2.0915\n",
      "\n",
      "[8/10]  train mae 1.2383 | mae val 1.0107\n",
      "     additional metrics:  [8/10]  train 3.1703 | val 2.0946\n",
      "\n",
      "[9/10]  train mae 1.2172 | mae val 1.0107\n",
      "     additional metrics:  [9/10]  train 3.0995 | val 2.0992\n",
      "\n",
      "[10/10]  train mae 1.2251 | mae val 1.0110\n",
      "     additional metrics:  [10/10]  train 3.1331 | val 2.1049\n",
      "\n",
      "8.994598627090454 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=2.086e-07\n",
      "[batch 0  rot 1]  max|Δ|=1.937e-07\n",
      "[batch 0  rot 2]  max|Δ|=2.086e-07\n",
      "[batch 0  rot 3]  max|Δ|=1.788e-07\n",
      "[batch 0  rot 4]  max|Δ|=2.235e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=4.703e-02\n",
      "[batch 0  perm‑R ]  max|Δ|=5.698e-02\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 2.235e-07\n",
      "permK : 4.703e-02\n",
      "permR : 5.698e-02\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.04330526292324066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_014558\n",
      "params: 17705\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.3635 | mae val 1.0526\n",
      "     additional metrics:  [1/10]  train 3.6311 | val 2.2125\n",
      "\n",
      "[2/10]  train mae 1.2415 | mae val 1.0090\n",
      "     additional metrics:  [2/10]  train 3.2189 | val 2.0988\n",
      "\n",
      "[3/10]  train mae 1.2353 | mae val 1.0189\n",
      "     additional metrics:  [3/10]  train 3.1706 | val 2.0925\n",
      "\n",
      "[4/10]  train mae 1.2461 | mae val 1.0235\n",
      "     additional metrics:  [4/10]  train 3.1474 | val 2.0977\n",
      "\n",
      "[5/10]  train mae 1.2537 | mae val 1.0192\n",
      "     additional metrics:  [5/10]  train 3.1591 | val 2.0929\n",
      "\n",
      "[6/10]  train mae 1.2430 | mae val 1.0136\n",
      "     additional metrics:  [6/10]  train 3.1418 | val 2.0911\n",
      "\n",
      "[7/10]  train mae 1.2286 | mae val 1.0118\n",
      "     additional metrics:  [7/10]  train 3.1184 | val 2.0915\n",
      "\n",
      "[8/10]  train mae 1.2383 | mae val 1.0107\n",
      "     additional metrics:  [8/10]  train 3.1703 | val 2.0946\n",
      "\n",
      "[9/10]  train mae 1.2172 | mae val 1.0107\n",
      "     additional metrics:  [9/10]  train 3.0995 | val 2.0992\n",
      "\n",
      "[10/10]  train mae 1.2251 | mae val 1.0110\n",
      "     additional metrics:  [10/10]  train 3.1331 | val 2.1049\n",
      "\n",
      "8.327433347702026 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=2.086e-07\n",
      "[batch 0  rot 1]  max|Δ|=1.937e-07\n",
      "[batch 0  rot 2]  max|Δ|=2.086e-07\n",
      "[batch 0  rot 3]  max|Δ|=1.788e-07\n",
      "[batch 0  rot 4]  max|Δ|=2.235e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=4.703e-02\n",
      "[batch 0  perm‑R ]  max|Δ|=5.698e-02\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 2.235e-07\n",
      "permK : 4.703e-02\n",
      "permR : 5.698e-02\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.04330526292324066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_021450\n",
      "params: 17705\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.3635 | mae val 1.0526\n",
      "     additional metrics:  [1/10]  train 3.6311 | val 2.2125\n",
      "\n",
      "[2/10]  train mae 1.2415 | mae val 1.0090\n",
      "     additional metrics:  [2/10]  train 3.2189 | val 2.0988\n",
      "\n",
      "[3/10]  train mae 1.2353 | mae val 1.0189\n",
      "     additional metrics:  [3/10]  train 3.1706 | val 2.0925\n",
      "\n",
      "[4/10]  train mae 1.2461 | mae val 1.0235\n",
      "     additional metrics:  [4/10]  train 3.1474 | val 2.0977\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4fee704257cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# ================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mva\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mva_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudy_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-4fee704257cb>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cfg, loader, train)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7b0da446c95a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# --- build token ----------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrbf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rbf\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhood_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# (R,C,N)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pkegnn/pKaSchNet/PK-EGNN/results/thinkpad_results/architecture.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# Identity passthrough — assume input is returned as output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pkegnn/pKaSchNet/PK-EGNN/results/thinkpad_results/architecture.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, centroids, coords)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcutoff\u001b[0m     \u001b[0;31m# [K]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0md\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# [B,N,N,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_014842\n",
      "params: 17705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.4138 | mae val 1.0971\n",
      "     additional metrics:  [1/10]  train 3.8091 | val 2.3469\n",
      "\n",
      "[2/10]  train mae 1.2822 | mae val 1.0316\n",
      "     additional metrics:  [2/10]  train 3.3433 | val 2.1616\n",
      "\n",
      "[3/10]  train mae 1.2361 | mae val 1.0102\n",
      "     additional metrics:  [3/10]  train 3.1918 | val 2.0975\n",
      "\n",
      "[4/10]  train mae 1.2304 | mae val 1.0139\n",
      "     additional metrics:  [4/10]  train 3.1469 | val 2.0943\n",
      "\n",
      "[5/10]  train mae 1.2397 | mae val 1.0137\n",
      "     additional metrics:  [5/10]  train 3.1532 | val 2.0958\n",
      "\n",
      "[6/10]  train mae 1.2378 | mae val 1.0125\n",
      "     additional metrics:  [6/10]  train 3.1455 | val 2.1010\n",
      "\n",
      "[7/10]  train mae 1.2327 | mae val 1.0133\n",
      "     additional metrics:  [7/10]  train 3.1303 | val 2.1114\n",
      "\n",
      "[8/10]  train mae 1.2354 | mae val 1.0144\n",
      "     additional metrics:  [8/10]  train 3.1471 | val 2.1183\n",
      "\n",
      "[9/10]  train mae 1.2249 | mae val 1.0170\n",
      "     additional metrics:  [9/10]  train 3.1272 | val 2.1264\n",
      "\n",
      "[10/10]  train mae 1.2271 | mae val 1.0205\n",
      "     additional metrics:  [10/10]  train 3.1388 | val 2.1349\n",
      "\n",
      "8.79986310005188 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=2.533e-07\n",
      "[batch 0  rot 1]  max|Δ|=2.533e-07\n",
      "[batch 0  rot 2]  max|Δ|=2.114e-07\n",
      "[batch 0  rot 3]  max|Δ|=3.129e-07\n",
      "[batch 0  rot 4]  max|Δ|=2.608e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=2.900e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=3.230e-01\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 3.129e-07\n",
      "permK : 2.900e-01\n",
      "permR : 3.230e-01\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.33655866980552673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: dont modify egnn at all, and just use as plannned, maybe even Conv after I study that more.\n",
    "but dont use nconv aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_021523\n",
      "params: 17697\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.8114 | mae val 1.1944\n",
      "     additional metrics:  [1/10]  train 5.4368 | val 2.6812\n",
      "\n",
      "[2/10]  train mae 1.3855 | mae val 1.0176\n",
      "     additional metrics:  [2/10]  train 3.7635 | val 2.1219\n",
      "\n",
      "[3/10]  train mae 1.2359 | mae val 1.0543\n",
      "     additional metrics:  [3/10]  train 3.1869 | val 2.1408\n",
      "\n",
      "[4/10]  train mae 1.2889 | mae val 1.0848\n",
      "     additional metrics:  [4/10]  train 3.1580 | val 2.2013\n",
      "\n",
      "[5/10]  train mae 1.3212 | mae val 1.0738\n",
      "     additional metrics:  [5/10]  train 3.2142 | val 2.1788\n",
      "\n",
      "[6/10]  train mae 1.3132 | mae val 1.0469\n",
      "     additional metrics:  [6/10]  train 3.2433 | val 2.1241\n",
      "\n",
      "[7/10]  train mae 1.2861 | mae val 1.0330\n",
      "     additional metrics:  [7/10]  train 3.1852 | val 2.1020\n",
      "\n",
      "[8/10]  train mae 1.2504 | mae val 1.0217\n",
      "     additional metrics:  [8/10]  train 3.1760 | val 2.0898\n",
      "\n",
      "[9/10]  train mae 1.2464 | mae val 1.0120\n",
      "     additional metrics:  [9/10]  train 3.1592 | val 2.0907\n",
      "\n",
      "[10/10]  train mae 1.2272 | mae val 1.0113\n",
      "     additional metrics:  [10/10]  train 3.1405 | val 2.1054\n",
      "\n",
      "8.27546763420105 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 1]  max|Δ|=4.768e-07\n",
      "[batch 0  rot 2]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 3]  max|Δ|=4.768e-07\n",
      "[batch 0  rot 4]  max|Δ|=2.384e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.205e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 4.768e-07\n",
      "permK : 1.205e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.08558619022369385\n"
     ]
    }
   ],
   "source": [
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_021540\n",
      "params: 17697\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.8114 | mae val 1.1944\n",
      "     additional metrics:  [1/10]  train 5.4368 | val 2.6812\n",
      "\n",
      "[2/10]  train mae 1.3855 | mae val 1.0176\n",
      "     additional metrics:  [2/10]  train 3.7635 | val 2.1219\n",
      "\n",
      "[3/10]  train mae 1.2359 | mae val 1.0543\n",
      "     additional metrics:  [3/10]  train 3.1869 | val 2.1408\n",
      "\n",
      "[4/10]  train mae 1.2889 | mae val 1.0848\n",
      "     additional metrics:  [4/10]  train 3.1580 | val 2.2013\n",
      "\n",
      "[5/10]  train mae 1.3212 | mae val 1.0738\n",
      "     additional metrics:  [5/10]  train 3.2142 | val 2.1788\n",
      "\n",
      "[6/10]  train mae 1.3132 | mae val 1.0469\n",
      "     additional metrics:  [6/10]  train 3.2433 | val 2.1241\n",
      "\n",
      "[7/10]  train mae 1.2861 | mae val 1.0330\n",
      "     additional metrics:  [7/10]  train 3.1852 | val 2.1020\n",
      "\n",
      "[8/10]  train mae 1.2504 | mae val 1.0217\n",
      "     additional metrics:  [8/10]  train 3.1760 | val 2.0898\n",
      "\n",
      "[9/10]  train mae 1.2464 | mae val 1.0120\n",
      "     additional metrics:  [9/10]  train 3.1592 | val 2.0907\n",
      "\n",
      "[10/10]  train mae 1.2272 | mae val 1.0113\n",
      "     additional metrics:  [10/10]  train 3.1405 | val 2.1054\n",
      "\n",
      "8.028393030166626 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 1]  max|Δ|=4.768e-07\n",
      "[batch 0  rot 2]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 3]  max|Δ|=4.768e-07\n",
      "[batch 0  rot 4]  max|Δ|=2.384e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.205e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 4.768e-07\n",
      "permK : 1.205e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.08558619022369385\n"
     ]
    }
   ],
   "source": [
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_021629\n",
      "params: 18897\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=200 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=200 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 2.1616 | mae val 1.5883\n",
      "     additional metrics:  [1/10]  train 6.9524 | val 3.9480\n",
      "\n",
      "[2/10]  train mae 1.7354 | mae val 1.1779\n",
      "     additional metrics:  [2/10]  train 5.0136 | val 2.6048\n",
      "\n",
      "[3/10]  train mae 1.4026 | mae val 1.0163\n",
      "     additional metrics:  [3/10]  train 3.8120 | val 2.1245\n",
      "\n",
      "[4/10]  train mae 1.2580 | mae val 1.0395\n",
      "     additional metrics:  [4/10]  train 3.2324 | val 2.1198\n",
      "\n",
      "[5/10]  train mae 1.2830 | mae val 1.0608\n",
      "     additional metrics:  [5/10]  train 3.1843 | val 2.1595\n",
      "\n",
      "[6/10]  train mae 1.3125 | mae val 1.0429\n",
      "     additional metrics:  [6/10]  train 3.2623 | val 2.1247\n",
      "\n",
      "[7/10]  train mae 1.2936 | mae val 1.0191\n",
      "     additional metrics:  [7/10]  train 3.2217 | val 2.0902\n",
      "\n",
      "[8/10]  train mae 1.2443 | mae val 1.0125\n",
      "     additional metrics:  [8/10]  train 3.1486 | val 2.0946\n",
      "\n",
      "[9/10]  train mae 1.2287 | mae val 1.0155\n",
      "     additional metrics:  [9/10]  train 3.1403 | val 2.1211\n",
      "\n",
      "[10/10]  train mae 1.2256 | mae val 1.0284\n",
      "     additional metrics:  [10/10]  train 3.1409 | val 2.1619\n",
      "\n",
      "21.427672147750854 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=5.364e-07\n",
      "[batch 0  rot 1]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 2]  max|Δ|=4.172e-07\n",
      "[batch 0  rot 3]  max|Δ|=4.172e-07\n",
      "[batch 0  rot 4]  max|Δ|=2.980e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=1.215e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 5.364e-07\n",
      "permK : 1.215e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 175 is out of bounds for dimension 0 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-108c0ae95517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m err = (model(z, x) -\n\u001b[0;32m--> 200\u001b[0;31m model(z[:, torch.randperm(cfg.hood_k)],\n\u001b[0m\u001b[1;32m    201\u001b[0m         x[:, torch.randperm(cfg.hood_k)])).abs().max()\n\u001b[1;32m    202\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"perm‑K  max|Δ| =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 175 is out of bounds for dimension 0 with size 100"
     ]
    }
   ],
   "source": [
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=200, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_021904\n",
      "params: 21021\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.2914 | mae val 1.0407\n",
      "     additional metrics:  [1/10]  train 3.3821 | val 2.1940\n",
      "\n",
      "[2/10]  train mae 1.2388 | mae val 1.0100\n",
      "     additional metrics:  [2/10]  train 3.1879 | val 2.0907\n",
      "\n",
      "[3/10]  train mae 1.2260 | mae val 1.0169\n",
      "     additional metrics:  [3/10]  train 3.1014 | val 2.0809\n",
      "\n",
      "[4/10]  train mae 1.2442 | mae val 1.0157\n",
      "     additional metrics:  [4/10]  train 3.1324 | val 2.0809\n",
      "\n",
      "[5/10]  train mae 1.2401 | mae val 1.0114\n",
      "     additional metrics:  [5/10]  train 3.1413 | val 2.0842\n",
      "\n",
      "[6/10]  train mae 1.2345 | mae val 1.0098\n",
      "     additional metrics:  [6/10]  train 3.1357 | val 2.0982\n",
      "\n",
      "[7/10]  train mae 1.2262 | mae val 1.0138\n",
      "     additional metrics:  [7/10]  train 3.1246 | val 2.1178\n",
      "\n",
      "[8/10]  train mae 1.2271 | mae val 1.0202\n",
      "     additional metrics:  [8/10]  train 3.1438 | val 2.1376\n",
      "\n",
      "[9/10]  train mae 1.2299 | mae val 1.0253\n",
      "     additional metrics:  [9/10]  train 3.1584 | val 2.1510\n",
      "\n",
      "[10/10]  train mae 1.2310 | mae val 1.0274\n",
      "     additional metrics:  [10/10]  train 3.1522 | val 2.1571\n",
      "\n",
      "10.571140766143799 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=3.576e-07\n",
      "[batch 0  rot 1]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 2]  max|Δ|=2.384e-07\n",
      "[batch 0  rot 3]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 4]  max|Δ|=4.172e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=7.437e-02\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 4.172e-07\n",
      "permK : 7.437e-02\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.04653486609458923\n"
     ]
    }
   ],
   "source": [
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=3, hidden_dim=3, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_021958\n",
      "params: 27783\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.2294 | mae val 1.0131\n",
      "     additional metrics:  [1/10]  train 3.1543 | val 2.0882\n",
      "\n",
      "[2/10]  train mae 1.2434 | mae val 1.0121\n",
      "     additional metrics:  [2/10]  train 3.1482 | val 2.1107\n",
      "\n",
      "[3/10]  train mae 1.2344 | mae val 1.0259\n",
      "     additional metrics:  [3/10]  train 3.1724 | val 2.1541\n",
      "\n",
      "[4/10]  train mae 1.2295 | mae val 1.0230\n",
      "     additional metrics:  [4/10]  train 3.1638 | val 2.1457\n",
      "\n",
      "[5/10]  train mae 1.2238 | mae val 1.0151\n",
      "     additional metrics:  [5/10]  train 3.1421 | val 2.1242\n",
      "\n",
      "[6/10]  train mae 1.2300 | mae val 1.0109\n",
      "     additional metrics:  [6/10]  train 3.1514 | val 2.1060\n",
      "\n",
      "[7/10]  train mae 1.2328 | mae val 1.0096\n",
      "     additional metrics:  [7/10]  train 3.1447 | val 2.0985\n",
      "\n",
      "[8/10]  train mae 1.2257 | mae val 1.0097\n",
      "     additional metrics:  [8/10]  train 3.1478 | val 2.0981\n",
      "\n",
      "[9/10]  train mae 1.2255 | mae val 1.0109\n",
      "     additional metrics:  [9/10]  train 3.1317 | val 2.1049\n",
      "\n",
      "[10/10]  train mae 1.2294 | mae val 1.0133\n",
      "     additional metrics:  [10/10]  train 3.1471 | val 2.1181\n",
      "\n",
      "11.466535329818726 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=4.172e-07\n",
      "[batch 0  rot 1]  max|Δ|=4.768e-07\n",
      "[batch 0  rot 2]  max|Δ|=4.172e-07\n",
      "[batch 0  rot 3]  max|Δ|=2.980e-07\n",
      "[batch 0  rot 4]  max|Δ|=2.980e-07\n",
      "[batch 0  perm‑K ]  max|Δ|=3.728e-02\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 4.768e-07\n",
      "permK : 3.728e-02\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.04388517141342163\n"
     ]
    }
   ],
   "source": [
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=3, hidden_dim=10, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250727_022044\n",
      "params: 52017\n",
      "std of all pos_emb weights: [0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train mae 1.4989 | mae val 1.0153\n",
      "     additional metrics:  [1/10]  train 3.6681 | val 2.1066\n",
      "\n",
      "[2/10]  train mae 1.2259 | mae val 1.1107\n",
      "     additional metrics:  [2/10]  train 3.1391 | val 2.4289\n",
      "\n",
      "[3/10]  train mae 1.2940 | mae val 1.1227\n",
      "     additional metrics:  [3/10]  train 3.4053 | val 2.4612\n",
      "\n",
      "[4/10]  train mae 1.2926 | mae val 1.0794\n",
      "     additional metrics:  [4/10]  train 3.3601 | val 2.3229\n",
      "\n",
      "[5/10]  train mae 1.2658 | mae val 1.0327\n",
      "     additional metrics:  [5/10]  train 3.2685 | val 2.1752\n",
      "\n",
      "[6/10]  train mae 1.2344 | mae val 1.0169\n",
      "     additional metrics:  [6/10]  train 3.2075 | val 2.1208\n",
      "\n",
      "[7/10]  train mae 1.2253 | mae val 1.0127\n",
      "     additional metrics:  [7/10]  train 3.1318 | val 2.0895\n",
      "\n",
      "[8/10]  train mae 1.2237 | mae val 1.0131\n",
      "     additional metrics:  [8/10]  train 3.1557 | val 2.0788\n",
      "\n",
      "[9/10]  train mae 1.2247 | mae val 1.0134\n",
      "     additional metrics:  [9/10]  train 3.1175 | val 2.0783\n",
      "\n",
      "[10/10]  train mae 1.2323 | mae val 1.0132\n",
      "     additional metrics:  [10/10]  train 3.1312 | val 2.0827\n",
      "\n",
      "37.143887996673584 sec\n",
      "\n",
      "================  INVARIANCE SUITE  ================\n",
      "\n",
      "[batch 0  rot 0]  max|Δ|=1.311e-06\n",
      "[batch 0  rot 1]  max|Δ|=1.609e-06\n",
      "[batch 0  rot 2]  max|Δ|=1.669e-06\n",
      "[batch 0  rot 3]  max|Δ|=1.967e-06\n",
      "[batch 0  rot 4]  max|Δ|=1.907e-06\n",
      "[batch 0  perm‑K ]  max|Δ|=2.180e-01\n",
      "[batch 0  perm‑R ]  max|Δ|=0.000e+00\n",
      "-------------------------------------------------------\n",
      "\n",
      "----------------  summary (max abs error) ----------------\n",
      "eqv   : 1.967e-06\n",
      "permK : 2.180e-01\n",
      "permR : 0.000e+00\n",
      "----------------------------------------------------------\n",
      "✔  thresholds used: atol=5.0e-04, rtol=5.0e-04\n",
      "==========================================================\n",
      "\n",
      "perm‑K  max|Δ| = 0.1827806830406189\n"
     ]
    }
   ],
   "source": [
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=10, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=1, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "    epochs=10,\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae',\n",
    "    study_metrics=True,\n",
    "    lr=5e-3, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=2, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "v_fn = nn.MSELoss() if cfg.loss_type=='mae' else nn.L1Loss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(cfg, loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0;oloss_sum=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        if cfg.study_metrics:\n",
    "            other_loss = v_fn(pred,y)\n",
    "            oloss_sum +=other_loss.item(); \n",
    "        loss_sum+=loss.item(); n+=1\n",
    "        \n",
    "    if not cfg.study_metrics:\n",
    "        return loss_sum/n\n",
    "    else:\n",
    "        return (loss_sum/n, oloss_sum/n)\n",
    "# run once, before the checker, to blank them all\n",
    "with torch.no_grad():\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"pos_emb\"):          # catches every EGNN_Network\n",
    "            m.pos_emb.weight.zero_()\n",
    "    print(\"std of all pos_emb weights:\",\n",
    "      [m.pos_emb.weight.std().item() for m in model.modules()\n",
    "       if hasattr(m,\"pos_emb\")])\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(cfg,tr_loader,True)\n",
    "    va=run(cfg,va_loader,False)\n",
    "    if not cfg.study_metrics:\n",
    "        sch.step(va)\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "    else:\n",
    "        sch.step(va[0])\n",
    "        print(f\"[{e+1}/{cfg.epochs}]  train {cfg.loss_type} {tr[0]:.4f} | {cfg.loss_type} val {va[0]:.4f}\")\n",
    "        print(\"     additional metrics: \",f\"[{e+1}/{cfg.epochs}]  train {tr[1]:.4f} | val {va[1]:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "print(time.time() - t0,\"sec\")\n",
    "\n",
    "# ================================================================\n",
    "# 7)  SE(3)‑equivariance & permutation‑invariance checker\n",
    "# ================================================================\n",
    "import torch, math, itertools, random\n",
    "from collections import defaultdict\n",
    "torch.set_printoptions(precision=3, sci_mode=True)\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def _random_rotation(device):\n",
    "    \"\"\"Draw a random 3×3 rotation matrix from a unit quaternion.\"\"\"\n",
    "    q = torch.randn(4, device=device); q /= q.norm()\n",
    "    w, x, y, z = q\n",
    "    return torch.tensor([[1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
    "                         [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
    "                         [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)]],\n",
    "                        device=device)\n",
    "\n",
    "def _prep_batch(batch, cfg):\n",
    "    \"\"\"Flatten (B,S,K,…) → (R,K,…) and drop padded rows.\"\"\"\n",
    "    z, x, _, m, *_ = batch\n",
    "    mask  = m.view(-1)                             # (R,)\n",
    "    z = z.view(-1, z.size(2))[mask].to(cfg.device)           # (R,K)\n",
    "    x = x.view(-1, x.size(2), 3)[mask].to(cfg.device)        # (R,K,3)\n",
    "    return z, x                                     # R = Σ valid residues\n",
    "\n",
    "# ---------- core checks ------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_invariance_suite(model, loader, cfg,\n",
    "                         max_batches=4, rot_trials=3,\n",
    "                         atol=5e-4, rtol=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    For `max_batches` mini‑batches:\n",
    "      • SE(3) equivariance   (rot + trans)\n",
    "      • neighbour perm‑inv   (perm K)\n",
    "      • residue  perm‑eqv    (perm R)\n",
    "    Returns dict with max abs‑errors.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(float)\n",
    "    model.eval()\n",
    "\n",
    "    for b_id, batch in enumerate(loader):\n",
    "        if b_id >= max_batches: break\n",
    "        z, x = _prep_batch(batch, cfg)             # (R,K), (R,K,3)\n",
    "\n",
    "        # --- baseline prediction -------------------------------------------------\n",
    "        base = model(z, x).flatten()               # (R,)\n",
    "\n",
    "        # -- 1) SE(3) equivariance -----------------------------------------------\n",
    "        for t in range(rot_trials):\n",
    "            R = _random_rotation(x.device)\n",
    "            tvec = torch.randn(1,1,3, device=x.device)\n",
    "            x_rt = (x @ R.T) + tvec\n",
    "            p_rt = model(z, x_rt).flatten()\n",
    "            err  = (base - p_rt).abs().max().item()\n",
    "            stats['eqv'] = max(stats['eqv'], err)\n",
    "            if verbose:\n",
    "                print(f\"[batch {b_id}  rot {t}]  max|Δ|={err:.3e}\")\n",
    "\n",
    "        # -- 2) neighbour‑perm invariance ----------------------------------------\n",
    "        K = z.size(1)\n",
    "        permK = torch.randperm(K, device=x.device)\n",
    "        pK = model(z[:, permK], x[:, permK]).flatten()\n",
    "        errK = (base - pK).abs().max().item()\n",
    "        stats['permK'] = max(stats['permK'], errK)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑K ]  max|Δ|={errK:.3e}\")\n",
    "\n",
    "        # -- 3) residue‑perm equivariance ----------------------------------------\n",
    "        Rn = z.size(0)\n",
    "        permR = torch.randperm(Rn, device=x.device)\n",
    "        zR, xR  = z[permR], x[permR]\n",
    "        pR = model(zR, xR).flatten()\n",
    "        # undo permutation on prediction\n",
    "        pR = pR[permR.argsort()]\n",
    "        errR = (base - pR).abs().max().item()\n",
    "        stats['permR'] = max(stats['permR'], errR)\n",
    "        if verbose:\n",
    "            print(f\"[batch {b_id}  perm‑R ]  max|Δ|={errR:.3e}\")\n",
    "            print(\"-\"*55)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ---------- run the suite on real inputs -------------------------------------\n",
    "print(\"\\n================  INVARIANCE SUITE  ================\\n\")\n",
    "stats = run_invariance_suite(model, tr_loader, cfg,\n",
    "                             max_batches=3, rot_trials=5,\n",
    "                             atol=5e-4, rtol=5e-4, verbose=True)\n",
    "\n",
    "print(\"\\n----------------  summary (max abs error) ----------------\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k:6}: {v:.3e}\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"✔  thresholds used: atol={:.1e}, rtol={:.1e}\".format(5e-4,5e-4))\n",
    "print(\"==========================================================\\n\")\n",
    "\n",
    "\n",
    "err = (model(z, x) -\n",
    "model(z[:, torch.randperm(cfg.hood_k)],\n",
    "        x[:, torch.randperm(cfg.hood_k)])).abs().max()\n",
    "print(\"perm‑K  max|Δ| =\", err.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
