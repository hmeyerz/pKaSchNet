{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ================================================================\n",
    "# 0) dashboard – flip anything here\n",
    "# ================================================================\n",
    "class Cfg(dict):\n",
    "    __getattr__ = dict.__getitem__; __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 1) reproducibility\n",
    "# ================================================================\n",
    "import random, os, numpy as np, torch, glob, datetime\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# 2) dataset helpers\n",
    "# ================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class HoodDS(Dataset):\n",
    "    def __init__(self, paths, k):\n",
    "        self.data=[]; self.ids=[]\n",
    "        nbr=NearestNeighbors(k,algorithm='brute')\n",
    "        for p in paths:\n",
    "            try:\n",
    "                d=np.load(p,allow_pickle=True)\n",
    "                if len(d['sites'])==0: continue\n",
    "                nbr.fit(d['pos']); idx=nbr.kneighbors(d['sites'],return_distance=False)\n",
    "                self.data.append((torch.from_numpy(d['z'][idx]),\n",
    "                                  torch.from_numpy(d['pos'][idx]),\n",
    "                                  torch.from_numpy(d['pks'])))\n",
    "                self.ids.append(os.path.splitext(os.path.basename(p))[0])\n",
    "            except Exception as e: print(\"skip\",p,e)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self,i):\n",
    "        z,p,y=self.data[i]; return z,p,y,self.ids[i]\n",
    "\n",
    "def pad(batch,k,device,ret_ids):\n",
    "    ids=[b[3] for b in batch] if ret_ids else None\n",
    "    B=len(batch); S=max(b[0].shape[0] for b in batch)\n",
    "    zt=torch.zeros(B,S,k,dtype=torch.int32,device=device)\n",
    "    pt=torch.zeros(B,S,k,3,dtype=torch.float32,device=device)\n",
    "    yt=torch.full((B,S),float('nan'),device=device); mt=torch.zeros(B,S,dtype=torch.bool,device=device)\n",
    "    for b,(z,p,y,_) in enumerate(batch):\n",
    "        s=z.shape[0]; zt[b,:s]=z; pt[b,:s]=p; yt[b,:s]=y; mt[b,:s]=True\n",
    "    return (zt,pt,yt,mt,ids) if ret_ids else (zt,pt,yt,mt)\n",
    "\n",
    "def split(paths):\n",
    "    if cfg.num_paths: paths=paths[:cfg.num_paths]\n",
    "    rng=np.random.RandomState(cfg.split_seed)\n",
    "    idx=rng.permutation(len(paths)); cut=int(len(paths)*cfg.split_ratio)\n",
    "    return [paths[i] for i in idx[:cut]], [paths[i] for i in idx[cut:]]\n",
    "\n",
    "# ================================================================\n",
    "# 3) model\n",
    "# ================================================================\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,c):\n",
    "        super().__init__(); self.c=c\n",
    "        C = c.dim + c.basis\n",
    "\n",
    "        self.egnn = StackedEGNN(c.dim,c.depth,c.hidden_dim,c.dropout,\n",
    "                                c.hood_k,98,c.num_neighbors,c.norm_coors).to(c.device)\n",
    "\n",
    "        self.rbf  = TunableBlock(LearnableRBF(c.basis,10.).to(c.device), c.use_rbf)\n",
    "        self.attn = TunableBlock(AttentionBlock(C,C,c.hidden_dim).to(c.device), c.use_attn)\n",
    "\n",
    "        if c.aggregator=='linear':\n",
    "            self.agg = nn.Linear(C,1).to(c.device)\n",
    "        elif c.aggregator=='nconv':\n",
    "            self.agg = nn.Conv1d(c.hood_k,1,kernel_size=C,padding=0).to(c.device)\n",
    "        elif c.aggregator=='pool':\n",
    "            self.agg = None\n",
    "        else: raise ValueError(\"aggregator must be 'linear' | 'nconv' | 'pool'\")\n",
    "\n",
    "        self.boost = nn.Linear(1,1).to(c.device) if c.use_boost else nn.Identity()\n",
    "        self.prot  = EGNN(dim=1,update_coors=True,num_nearest_neighbors=3).to(c.device) \\\n",
    "                     if c.use_prot else nn.Identity()\n",
    "        self.conv  = nn.Conv1d(1,1,c.conv_kernel,padding=c.conv_kernel//2).to(c.device) \\\n",
    "                     if c.use_conv else nn.Identity()\n",
    "\n",
    "    def forward(self,z,x):\n",
    "        h,coord=self.egnn(z,x); h=h[0]                # (R,N,dim)\n",
    "        cent=coord.mean(1,keepdim=True)               # (R,1,3)\n",
    "\n",
    "        # --- build token ----------------------------------------------------------------\n",
    "        r = self.rbf(cent,coord).transpose(1,2) if self.c.use_rbf else \\\n",
    "            h.new_zeros(h.size(0),self.c.basis,self.c.hood_k)\n",
    "        tok = torch.cat((r,h.transpose(1,2)),1)       # (R,C,N)\n",
    "\n",
    "        att = self.attn(tok.permute(2,0,1))\n",
    "        tok = att[0] if isinstance(att,(tuple,list)) else att\n",
    "        tok = tok.permute(1,0,2)                      # (R,N,C)\n",
    "\n",
    "        # --- aggregation ----------------------------------------------------------------\n",
    "        if self.c.aggregator=='linear':\n",
    "            preds = self.agg(tok) .max(1).values                # (R,1)\n",
    "        elif self.c.aggregator=='nconv':\n",
    "            preds = self.agg(tok).squeeze(-1)                   # (R,1)\n",
    "        else:   # pool\n",
    "            preds = tok.max(1).values.mean(1,keepdim=True)      # (R,1)\n",
    "\n",
    "        preds = self.boost(preds)\n",
    "\n",
    "        if self.c.use_prot:\n",
    "            preds = self.prot(preds.unsqueeze(0),\n",
    "                              cent.permute(1,0,2))[0].squeeze(0)\n",
    "\n",
    "        if self.c.use_conv:\n",
    "            preds = self.conv(preds.T.unsqueeze(0)).squeeze(0).T\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_225207\n",
      "params: 16348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 1.6758 | val 1.1467\n",
      "[2/10]  train 1.2742 | val 1.1439\n",
      "[3/10]  train 1.2657 | val 1.1457\n",
      "[4/10]  train 1.2656 | val 1.1468\n",
      "[5/10]  train 1.2655 | val 1.1471\n",
      "[6/10]  train 1.2655 | val 1.1479\n",
      "[7/10]  train 1.2654 | val 1.1481\n",
      "[8/10]  train 1.2654 | val 1.1481\n",
      "[9/10]  train 1.2654 | val 1.1482\n",
      "[10/10]  train 1.2655 | val 1.1481\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='pool',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lin agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_225444\n",
      "params: 16367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 1.3989 | val 1.1896\n",
      "[2/10]  train 1.2674 | val 1.1400\n",
      "[3/10]  train 1.2522 | val 1.1206\n",
      "[4/10]  train 1.2236 | val 1.1170\n",
      "[5/10]  train 1.2026 | val 1.1360\n",
      "[6/10]  train 1.1462 | val 1.0370\n",
      "[7/10]  train 1.1655 | val 1.1616\n",
      "[8/10]  train 1.1135 | val 1.0250\n",
      "[9/10]  train 1.0366 | val 0.9737\n",
      "[10/10]  train 0.9746 | val 0.9117\n",
      "98.28616762161255 sec\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='linear',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=False,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_225632\n",
      "params: 18149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 2.0834 | val 1.2063\n",
      "[2/10]  train 1.3065 | val 1.0911\n",
      "[3/10]  train 1.1154 | val 1.0493\n",
      "[4/10]  train 1.0105 | val 0.9790\n",
      "[5/10]  train 0.9371 | val 0.9596\n",
      "[6/10]  train 0.9248 | val 0.9660\n",
      "[7/10]  train 0.8935 | val 0.9430\n",
      "[8/10]  train 0.8496 | val 0.9694\n",
      "[9/10]  train 0.8412 | val 0.9084\n",
      "[10/10]  train 0.8207 | val 0.8903\n",
      "97.00188207626343 sec\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proceeding with nconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_230147\n",
      "params: 18149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 2.0965 | val 1.1284\n",
      "[2/10]  train 1.3527 | val 1.1105\n",
      "[3/10]  train 1.1306 | val 1.0777\n",
      "[4/10]  train 1.0158 | val 0.9621\n",
      "[5/10]  train 0.9308 | val 0.9360\n",
      "[6/10]  train 0.9074 | val 0.9313\n",
      "[7/10]  train 0.8699 | val 0.9447\n",
      "[8/10]  train 0.8347 | val 0.9875\n",
      "[9/10]  train 0.8336 | val 1.0118\n",
      "[10/10]  train 0.8179 | val 0.9079\n",
      "84.53549122810364 sec\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_231218\n",
      "params: 18149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 2.6957 | val 2.0016\n",
      "[2/10]  train 1.5322 | val 1.2235\n",
      "[3/10]  train 1.1391 | val 1.0274\n",
      "[4/10]  train 1.0450 | val 0.9733\n",
      "[5/10]  train 0.9588 | val 0.9335\n",
      "[6/10]  train 0.9158 | val 0.9207\n",
      "[7/10]  train 0.9300 | val 0.9829\n",
      "[8/10]  train 0.8637 | val 0.9226\n",
      "[9/10]  train 0.8437 | val 0.9758\n",
      "[10/10]  train 0.9055 | val 0.8894\n",
      "115.00042057037354 sec\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "from egnn_pytorch import EGNN\n",
    "from architecture import (StackedEGNN,\n",
    "                          LearnableRBF,\n",
    "                          AttentionBlock,\n",
    "                          TunableBlock)\n",
    "import time, datetime\n",
    "import glob\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "winner#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_231416\n",
      "params: 18151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 2.0807 | val 1.4433\n",
      "[2/10]  train 1.4117 | val 1.2751\n",
      "[3/10]  train 1.1584 | val 1.0363\n",
      "[4/10]  train 1.0491 | val 0.9936\n",
      "[5/10]  train 0.9829 | val 0.9851\n",
      "[6/10]  train 0.9159 | val 0.9126\n",
      "[7/10]  train 0.8692 | val 0.8687\n",
      "[8/10]  train 0.8351 | val 0.8687\n",
      "[9/10]  train 0.8161 | val 0.8547\n",
      "[10/10]  train 0.7876 | val 0.8465\n",
      "111.8393828868866 sec\n"
     ]
    }
   ],
   "source": [
    "#winner\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =False,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True,\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_231714\n",
      "params: 19477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 2.0621 | val 1.2336\n",
      "[2/10]  train 1.3531 | val 1.4853\n",
      "[3/10]  train 1.1949 | val 1.0629\n",
      "[4/10]  train 1.0942 | val 1.0373\n",
      "[5/10]  train 0.9542 | val 1.0138\n",
      "[6/10]  train 0.9741 | val 0.9294\n",
      "[7/10]  train 0.8765 | val 0.9586\n",
      "[8/10]  train 0.8452 | val 0.8553\n",
      "[9/10]  train 0.8117 | val 0.8640\n",
      "[10/10]  train 0.7772 | val 0.8403\n",
      "104.7829270362854 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True, #study metrics not working\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_231859\n",
      "params: 19479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 2.1856 | val 1.1215\n",
      "[2/10]  train 1.3608 | val 1.0839\n",
      "[3/10]  train 1.1348 | val 1.0261\n",
      "[4/10]  train 1.0440 | val 1.0091\n",
      "[5/10]  train 0.9778 | val 0.9580\n",
      "[6/10]  train 0.9328 | val 0.9290\n",
      "[7/10]  train 0.9102 | val 0.8918\n",
      "[8/10]  train 0.8508 | val 0.9068\n",
      "[9/10]  train 0.8458 | val 0.8788\n",
      "[10/10]  train 0.8131 | val 0.8615\n",
      "103.34276056289673 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True, #study metrics not working\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proceed w prot no boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_232329\n",
      "params: 19485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 1.7781 | val 1.2738\n",
      "[2/10]  train 1.2617 | val 1.1750\n",
      "[3/10]  train 1.1787 | val 1.1414\n",
      "[4/10]  train 1.1329 | val 1.1605\n",
      "[5/10]  train 1.1012 | val 1.1611\n",
      "[6/10]  train 1.0869 | val 1.1816\n",
      "[7/10]  train 1.0755 | val 1.1597\n",
      "[8/10]  train 1.0481 | val 1.1501\n",
      "[9/10]  train 1.0359 | val 1.1420\n",
      "[10/10]  train 1.0279 | val 1.1490\n",
      "106.9748044013977 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =True,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True, #study metrics not working\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-96963c89afcd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-96963c89afcd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [1/10]  train 2.0621 | val 1.2336\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[1/10]  train 2.0621 | val 1.2336\n",
    "[2/10]  train 1.3531 | val 1.4853\n",
    "[3/10]  train 1.1949 | val 1.0629\n",
    "[4/10]  train 1.0942 | val 1.0373\n",
    "[5/10]  train 0.9542 | val 1.0138\n",
    "[6/10]  train 0.9741 | val 0.9294\n",
    "[7/10]  train 0.8765 | val 0.9586\n",
    "[8/10]  train 0.8452 | val 0.8553\n",
    "[9/10]  train 0.8117 | val 0.8640\n",
    "[10/10]  train 0.7772 | val 0.8403\n",
    "104.7829270362854 sec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norbf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_232609\n",
      "params: 19477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 2.8559 | val 1.4831\n",
      "[2/10]  train 1.5866 | val 1.1893\n",
      "[3/10]  train 1.1755 | val 1.1914\n",
      "[4/10]  train 1.1214 | val 1.1487\n",
      "[5/10]  train 0.9889 | val 0.9604\n",
      "[6/10]  train 0.9210 | val 0.9354\n",
      "[7/10]  train 0.9036 | val 0.9515\n",
      "[8/10]  train 0.9083 | val 0.8885\n",
      "[9/10]  train 0.8451 | val 0.9038\n",
      "[10/10]  train 0.8726 | val 0.9128\n",
      "107.1022801399231 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =False,\n",
    "    use_attn     =True,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True, #study metrics not working\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prot rbf no attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_232925\n",
      "params: 19477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 2.2655 | val 1.7489\n",
      "[2/10]  train 1.4005 | val 1.0979\n",
      "[3/10]  train 1.1693 | val 1.0348\n",
      "[4/10]  train 1.0632 | val 0.9992\n",
      "[5/10]  train 0.9745 | val 1.0157\n",
      "[6/10]  train 0.9501 | val 0.9340\n",
      "[7/10]  train 0.8894 | val 1.0206\n",
      "[8/10]  train 0.8956 | val 1.0333\n",
      "[9/10]  train 0.8327 | val 0.8982\n",
      "[10/10]  train 0.7930 | val 0.8875\n",
      "84.47911500930786 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =False,\n",
    "    use_boost    =False,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True, #study metrics not working\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run‑ID: 20250726_233219\n",
      "params: 19479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/usr/lib64/python3.6/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass n_neighbors=100 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10]  train 1.8082 | val 1.6452\n",
      "[2/10]  train 1.3010 | val 1.1606\n",
      "[3/10]  train 1.0970 | val 1.0790\n",
      "[4/10]  train 1.0353 | val 1.0194\n",
      "[5/10]  train 0.9837 | val 0.9806\n",
      "[6/10]  train 0.9416 | val 0.9487\n",
      "[7/10]  train 0.9137 | val 0.9431\n",
      "[8/10]  train 0.8695 | val 0.9199\n",
      "[9/10]  train 0.8320 | val 0.9162\n",
      "[10/10]  train 0.8559 | val 0.9077\n",
      "79.04292941093445 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0=time.time()\n",
    "cfg = Cfg(\n",
    "    # backbone\n",
    "    dim=12, basis=6, depth=2, hidden_dim=4, dropout=0.02,\n",
    "    hood_k=100, num_neighbors=8, norm_coors=True,\n",
    "\n",
    "    # aggregation: 'linear' | 'nconv' | 'pool'\n",
    "    aggregator   ='nconv',\n",
    "\n",
    "    # block switches\n",
    "    use_rbf      =True,\n",
    "    use_attn     =False,\n",
    "    use_boost    =True,     # Linear(1→1) after aggregator\n",
    "    use_prot     =True,      # protein‑level EGNN\n",
    "    use_conv     =False,     # 1‑D conv after prot EGNN\n",
    "    conv_kernel  =7,\n",
    "\n",
    "    # training\n",
    "    loss_type='mae', study_metrics=True, #study metrics not working\n",
    "    lr=5e-3, epochs=10, batch_size=1, #batchsize not safw to inc\n",
    "\n",
    "    # misc\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    seed=0, analysis_mode=False,\n",
    "    num_paths=20, split_ratio=0.5, split_seed=0,\n",
    "    runid=datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    ")\n",
    "print(\"Run‑ID:\", cfg.runid)\n",
    "random.seed(cfg.seed); np.random.seed(cfg.seed); torch.manual_seed(cfg.seed)\n",
    "\n",
    "model=Model(cfg); \n",
    "print(\"params:\",sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# ================================================================\n",
    "# 4) loaders\n",
    "# ================================================================\n",
    "allp=glob.glob(\"../../../data/pkegnn_INS/inputs/*.npz\")\n",
    "tr,val=split(allp)\n",
    "train_ds=HoodDS(tr,cfg.hood_k); val_ds=HoodDS(val,cfg.hood_k)\n",
    "coll=lambda b: pad(b,cfg.hood_k,cfg.device,cfg.analysis_mode)\n",
    "tr_loader=DataLoader(train_ds,batch_size=cfg.batch_size,shuffle=True ,collate_fn=coll)\n",
    "va_loader=DataLoader(val_ds,batch_size=cfg.batch_size,shuffle=False,collate_fn=coll)\n",
    "\n",
    "# ================================================================\n",
    "# 5) training utils\n",
    "# ================================================================\n",
    "p_fn = nn.L1Loss() if cfg.loss_type=='mae' else nn.MSELoss()\n",
    "opt  = torch.optim.AdamW(model.parameters(),lr=cfg.lr)\n",
    "sch  = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,'min',0.5,3)\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler=GradScaler(enabled=(cfg.device=='cuda'))\n",
    "\n",
    "def run(loader,train):\n",
    "    model.train() if train else model.eval(); loss_sum=0;n=0\n",
    "    for z,x,y,m,*_ in loader:\n",
    "        v=m.view(-1); z=z.view(-1,z.size(2))[v].to(cfg.device)\n",
    "        x=x.view(-1,x.size(2),3)[v].to(cfg.device); y=y.view(-1)[v].to(cfg.device)\n",
    "        with autocast(enabled=(cfg.device=='cuda')):\n",
    "            pred=model(z,x).flatten(); loss=p_fn(pred,y)\n",
    "        if train:\n",
    "            opt.zero_grad(); scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        loss_sum+=loss.item(); n+=1\n",
    "    return loss_sum/n\n",
    "\n",
    "# ================================================================\n",
    "# 6) train\n",
    "# ================================================================\n",
    "for e in range(cfg.epochs):\n",
    "    tr=run(tr_loader,True)\n",
    "    va=run(va_loader,False); sch.step(va)\n",
    "    print(f\"[{e+1}/{cfg.epochs}]  train {tr:.4f} | val {va:.4f}\")\n",
    "print(time.time() - t0,\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
