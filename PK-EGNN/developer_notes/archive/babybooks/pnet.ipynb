{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "from egnn_pytorch import EGNN_Network\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "charges = {'HIS': 1, 'LYS': 1,\n",
    "           'ASP': -1, 'GLU': -1,  # carboxylate\n",
    "           'CYS': 0, 'CYS': 0,\n",
    "           'TYR': 0}   # thiol and phenol prot\n",
    "\n",
    "cations = {'HIS': (\"HD1\", \"HD2\", \"HE1\", \"HE2\"),\n",
    "        'ASP':None,\n",
    "        \"LYS\":(\"HZ1\", \"HZ2\",\"HZ3\"),\n",
    "        \"TYR\":\"HH\",\n",
    "        \"GLU\":None,\n",
    "        \"CYS\":'HG',\n",
    "        \"ARG\": (\"HE11\",\"HE12\", \"HE21\", \"HE22\"),\n",
    "        \"THR\":\"HG1\",\n",
    "        \"SER\":\"HG\",\n",
    "        \"TRP\":\"HE1\"}\n",
    "\n",
    "\n",
    "anions = {\"HIS\":(\"ND1\", \"ND2\"),\n",
    "          \"ASP\":(\"OD1\",\"OD2\"),\n",
    "          \"LYS\":\"NZ\",\n",
    "          \"TYR\":\"OH\",\n",
    "          \"GLU\":(\"OE1\", \"OE2\"),\n",
    "          \"CYS\":\"SG\",\n",
    "          \"ARG\": (\"NE1\",\"NE2\"),\n",
    "          \"THR\":\"OG1\",\n",
    "          \"SER\":\"OG\",\n",
    "          \"TRP\":\"NE1\"}\n",
    "\n",
    "\n",
    "def load_pdb(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        resis = {}\n",
    "        xs = []\n",
    "        counter=0\n",
    "        for line in f:\n",
    "            L = line.split()\n",
    "\n",
    "            if line.startswith(\"ATOM\"):\n",
    "                resname, atomname = L[3], L[2]\n",
    "                if resname in prot:\n",
    "                    resname = resname + str(L[5]) + L[4]\n",
    "\n",
    "                    if resname in resis:\n",
    "                        resis[resname][f\"{atomname} {counter}\"] = (float(L[6]), float(L[7]), float(L[8]))\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        resis[resname] = {f\"{atomname} {counter}\": (float(L[6]), float(L[7]), float(L[8]))}\n",
    "                        counter += 1\n",
    "\n",
    "                else:  # resname in noprot\n",
    "                    resname, atomname, resnum = L[3], L[2], L[5]\n",
    "                    xs.append((float(L[6]), float(L[7]), float(L[8])))\n",
    "                    counter += 1\n",
    "\n",
    "            elif line.startswith(\"HETATM\"):\n",
    "                resname = L[3] + str(L[5])\n",
    "                atomname = L[2]\n",
    "                if resname in solvents:\n",
    "                    solvents[resname][f\"{atomname} {str(counter)}\"] = (float(L[6]), float(L[7]), float(L[8]))\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    solvents[resname] = {f\"{atomname} {str(counter)}\": (float(L[6]), float(L[7]), float(L[8]))}\n",
    "                counter += 1\n",
    "    #print(resis)\n",
    "    return resis\n",
    "\n",
    "#load_pdb()\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "prot = charges.keys()\n",
    "\n",
    "Rs, solvents, counter = {}, {}, 0\n",
    "\n",
    "all_ppos, all_pspecies = {}, {}\n",
    "positions = []\n",
    "s={}\n",
    "all_ions={}\n",
    "\n",
    "\n",
    "directory = '/home/jrhoernschemeyer/Desktop/thesis/\"\n",
    "\n",
    "all_targets={}\n",
    "\n",
    "    \n",
    "data, pkpdb, pdbs = get_pdbs_labels(directory)\n",
    "\n",
    "for pdb in pdbs:\n",
    "    d = df[df.iloc[:, 0] == pdb].drop(columns = [\"idcode\"])\n",
    "    all_targets[pdb] = group_by_category(d.compute().to_dict(orient=\"records\"))\n",
    "\n",
    "def get_pdbs_labels(directory):\n",
    "    \"\"\"gets the data dictionary and [pdb string names] \n",
    "\n",
    "    inputs: path to directory where input structures are saved\"\"\"\n",
    "    pdbids, data =[],{}\n",
    "    \n",
    "    for file in glob.glob(directory + \"/PDB/*red.pdb\"):\n",
    "        pdbids = file[-12:-8]\n",
    "        data[pdb] = load_pdb(file)\n",
    "        pdbids.append(pdb)\n",
    "    \n",
    "   \n",
    "\n",
    "    return data, dd.read_csv(directory + \"/pkas.csv\", sep=';'), pdbids\n",
    "\n",
    "\n",
    "def group_by_category(data_list):\n",
    "    #targets = defaultdict(list)\n",
    "    targets={}\n",
    "    for item in data_list:\n",
    "        num, resi, chain, pk = item['residue_number'], item['residue_name'], item[\"chain\"], item[\"pk\"]\n",
    "        targets[resi+str(num)+chain]= pk \n",
    "    \n",
    "    return targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def boltzmax(x, temp):\n",
    "    \"\"\"\n",
    "    Computes Boltzmann probabilities (Euclidean Softmax) for a tensor of distances.\n",
    "    \n",
    "    Parameters:\n",
    "        distances (torch.Tensor): feature tensor\n",
    "        temperature (float): The temperature parameter to control the sharpness.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of notprobabilities \n",
    "    \"\"\"\n",
    "    # Scale the distances by temperature\n",
    "    scaled_distances = x / temp\n",
    "    \n",
    "    # Exponentiate the scaled distances\n",
    "    x = torch.exp(scaled_distances)\n",
    "    p = x / torch.max(x) #doing max instead of \n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "def contrastive_loss(latent_features, species):\n",
    "    \"\"\"seperates latent space by enforcing dissimilarity between negative and positive ions\"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    positive_pairs = [(i, j) for i, j in itertools.combinations(range(len(species)), 2) if species[i] == species[j]]\n",
    "    negative_pairs = [(i, j) for i, j in itertools.combinations(range(3), 2) if species[i] != species[j]]\n",
    "\n",
    "\n",
    "    # Positive pairs\n",
    "    for i, j in positive_pairs:\n",
    "        z_i, z_j = latent_features[i], latent_features[j]\n",
    "        dist = torch.norm(z_i - z_j, p=2)  \n",
    "        loss += dist ** 2  \n",
    "\n",
    "    # Negative pairs\n",
    "    for i, j in negative_pairs:\n",
    "        z_i, z_j = latent_features[i], latent_features[j]\n",
    "        dist = torch.norm(z_i - z_j, p=2) \n",
    "        loss += torch.clamp(dist, min=0) ** 2 #min 0 was the margin enforcement\n",
    "\n",
    "\n",
    "\n",
    "    return loss / (len(positive_pairs) + len(negative_pairs))\n",
    "\n",
    "\n",
    "\n",
    "def model(num_nodes, dim, depth, lr, weight_decay):\n",
    "    net = EGNN_Network(\n",
    "        num_tokens = 100, #vocabulary siye, \"number of unique species\"\n",
    "        num_positions = num_nodes, # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "        dim = dim,\n",
    "        num_nearest_neighbors = 1, \n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return net, optimizer\n",
    "\n",
    "\n",
    "\n",
    "def loop(nepochs, coors, Hs, ion_labels, model, optimizer, negative_slope):\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x, _ = model(Hs.unsqueeze(0), coors.unsqueeze(0)) #can turn off return po\n",
    "        rep = boltzmax((nn.LeakyReLU(negative_slope=negative_slope)(x)), 15)[0] #apply activation (output raw numbers), and softmax (output nonnormalized probabilities)  \n",
    "        L = contrastive_loss(rep, ion_labels)\n",
    "        print(\"loss\",L)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "    return rep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom egnn_pytorch import EGNN_Network\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import dask.dataframe as dd\n",
    "\n",
    "charges = {'HIS': 1, 'LYS': 1,\n",
    "           'ASP': -1, 'GLU': -1,  # carboxylate\n",
    "           'CYS': 0,\n",
    "           'TYR': 0}   # thiol and phenol prot\n",
    "\n",
    "cations = {'HIS': (\"HD1\", \"HD2\", \"HE1\", \"HE2\"),\n",
    "        'ASP':None,\n",
    "        \"LYS\":(\"HZ1\", \"HZ2\",\"HZ3\"),\n",
    "        \"TYR\":\"HH\",\n",
    "        \"GLU\":None,\n",
    "        \"CYS\":'HG',\n",
    "        \"ARG\": (\"HE11\",\"HE12\", \"HE21\", \"HE22\"),\n",
    "        \"THR\":\"HG1\",\n",
    "        \"SER\":\"HG\",\n",
    "        \"TRP\":\"HE1\"}\n",
    "\n",
    "\n",
    "anions = {\"HIS\":(\"ND1\", \"ND2\"),\n",
    "          \"ASP\":(\"OD1\",\"OD2\"),\n",
    "          \"LYS\":\"NZ\",\n",
    "          \"TYR\":\"OH\",\n",
    "          \"GLU\":(\"OE1\", \"OE2\"),\n",
    "          \"CYS\":\"SG\",\n",
    "          \"ARG\": (\"NE1\",\"NE2\"),\n",
    "          \"THR\":\"OG1\",\n",
    "          \"SER\":\"OG\",\n",
    "          \"TRP\":\"NE1\"}\n",
    "\n",
    "prot = charges.keys()\n",
    "\n",
    "Rs, solvents, counter = {}, {}, 0\n",
    "\n",
    "all_ppos, all_pspecies = {}, {}\n",
    "positions = []\n",
    "s={}\n",
    "all_ions={}\n",
    "all_targets={}\n",
    "\n",
    "\n",
    "directory = \"/home/jrhoernschemeyer/Desktop/thesis/\"\n",
    "\n",
    "\n",
    "def load_pdb(path):\n",
    "    solvents, counter = {}, 0\n",
    "    with open(path, \"r\") as f:\n",
    "        resis, xs, counter = {}, [], 0\n",
    "        for line in f:\n",
    "            L = line.split()\n",
    "\n",
    "            if line.startswith(\"ATOM\"):\n",
    "                resname, atomname = L[3], L[2]\n",
    "                if resname in charges.keys():\n",
    "                    resname = resname + str(L[5]) + L[4]\n",
    "\n",
    "                    if resname in resis:\n",
    "                        resis[resname][f\"{atomname} {counter}\"] = (float(L[6]), float(L[7]), float(L[8]))\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        resis[resname] = {f\"{atomname} {counter}\": (float(L[6]), float(L[7]), float(L[8]))}\n",
    "                        counter += 1\n",
    "\n",
    "                else:  # resname in noprot\n",
    "                    resname, atomname = L[3], L[2]\n",
    "                    xs.append((float(L[6]), float(L[7]), float(L[8])))\n",
    "                    counter += 1\n",
    "\n",
    "            elif line.startswith(\"HETATM\"):\n",
    "                resname = L[3] + str(L[5])\n",
    "                atomname = L[2]\n",
    "                if resname in solvents:\n",
    "                    solvents[resname][f\"{atomname} {str(counter)}\"] = (float(L[6]), float(L[7]), float(L[8]))\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    solvents[resname] = {f\"{atomname} {str(counter)}\": (float(L[6]), float(L[7]), float(L[8]))}\n",
    "                counter += 1\n",
    "    \n",
    "    return resis\n",
    "\n",
    "\n",
    "def get_pdbs_labels(directory):\n",
    "    \"\"\"gets the data dictionary and [pdb string names] \n",
    "\n",
    "    inputs: path to directory where input structures are saved\"\"\"\n",
    "    pdbids, data =[],{}\n",
    "    \n",
    "    for file in glob.glob(directory + \"/PDB/*red.pdb\"):\n",
    "        pdbids = file[-12:-8]\n",
    "        data[pdb] = load_pdb(file)\n",
    "        pdbids.append(pdb)\n",
    "    \n",
    "   \n",
    "\n",
    "    return data, dd.read_csv(directory + \"/pkas.csv\", sep=';'), pdbids\n",
    "\n",
    "\n",
    "def get_targets(df):\n",
    "    #targets = defaultdict(list)\n",
    "    targets={}\n",
    "    for item in df:\n",
    "        num, resi, chain, pk = item['residue_number'], item['residue_name'], item[\"chain\"], item[\"pk\"]\n",
    "        targets[resi+str(num)+chain]= pk \n",
    "\n",
    "        \n",
    "    \n",
    "    return targets\n",
    "\n",
    "\n",
    "\n",
    "def boltzmax(x, temp):\n",
    "    \"\"\"\n",
    "    Computes Boltzmann probabilities (Euclidean Softmax) for a tensor of distances.\n",
    "    \n",
    "    Parameters:\n",
    "        distances (torch.Tensor): feature tensor\n",
    "        temperature (float): The temperature parameter to control the sharpness.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of notprobabilities \n",
    "    \"\"\"\n",
    "    # Scale the distances by temperature\n",
    "    scaled_distances = x / temp\n",
    "    \n",
    "    # Exponentiate the scaled distances\n",
    "    x = torch.exp(scaled_distances)\n",
    "    p = x / torch.max(x) #doing max instead of \n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "def contrastive_loss(latent_features, species):\n",
    "    \"\"\"seperates latent space by enforcing dissimilarity between negative and positive ions\"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    positive_pairs = [(i, j) for i, j in itertools.combinations(range(len(species)), 2) if species[i] == species[j]]\n",
    "    negative_pairs = [(i, j) for i, j in itertools.combinations(range(3), 2) if species[i] != species[j]]\n",
    "\n",
    "\n",
    "    # Positive pairs\n",
    "    for i, j in positive_pairs:\n",
    "        z_i, z_j = latent_features[i], latent_features[j]\n",
    "        dist = torch.norm(z_i - z_j, p=2)  \n",
    "        loss += dist ** 2  \n",
    "\n",
    "    # Negative pairs\n",
    "    for i, j in negative_pairs:\n",
    "        z_i, z_j = latent_features[i], latent_features[j]\n",
    "        dist = torch.norm(z_i - z_j, p=2) \n",
    "        loss += torch.clamp(dist, min=0) ** 2 #min 0 was the margin enforcement\n",
    "\n",
    "\n",
    "\n",
    "    return loss / (len(positive_pairs) + len(negative_pairs))\n",
    "\n",
    "\n",
    "\n",
    "def model1(num_nodes, dim, depth, lr, weight_decay):\n",
    "    net = EGNN_Network(\n",
    "        num_tokens = 100, #vocabulary siye, \"number of unique species\"\n",
    "        num_positions = num_nodes, # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "        dim = dim,\n",
    "        num_nearest_neighbors = 1, \n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    return net, optimizer\n",
    "\n",
    "\n",
    "\n",
    "def loop(nepochs, coors, Hs, ion_labels, model, optimizer, negative_slope):\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x, _ = model(Hs.unsqueeze(0), coors.unsqueeze(0)) #can turn off return po\n",
    "        rep = boltzmax((nn.LeakyReLU(negative_slope=negative_slope)(x)), 15)[0] #apply activation (output raw numbers), and softmax (output nonnormalized probabilities)  \n",
    "        L = contrastive_loss(rep, ion_labels)\n",
    "        print(\"loss\",L)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "       \n",
    "    return rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10gs\n",
      "TYR3A\n",
      "TYR7A\n",
      "CYS14A\n",
      "ASP23A\n",
      "LYS29A\n",
      "GLU30A\n",
      "GLU31A\n",
      "GLU36A\n",
      "GLU40A\n",
      "LYS44A\n",
      "CYS47A\n",
      "TYR49A\n",
      "LYS54A\n",
      "ASP57A\n",
      "ASP59A\n",
      "TYR63A\n",
      "HIS71A\n",
      "TYR79A\n",
      "LYS81A\n",
      "ASP82A\n",
      "GLU85A\n",
      "ASP90A\n",
      "ASP94A\n",
      "GLU97A\n",
      "ASP98A\n",
      "CYS101A\n",
      "LYS102A\n",
      "TYR103A\n",
      "TYR108A\n",
      "TYR111A\n",
      "GLU112A\n",
      "LYS115A\n",
      "ASP116A\n",
      "ASP117A\n",
      "TYR118A\n",
      "LYS120A\n",
      "LYS127A\n",
      "GLU130A\n",
      "LYS140A\n",
      "ASP146A\n",
      "ASP152A\n",
      "TYR153A\n",
      "ASP157A\n",
      "HIS162A\n",
      "GLU163A\n",
      "CYS169A\n",
      "ASP171A\n",
      "TYR179A\n",
      "LYS188A\n",
      "LYS190A\n",
      "GLU197A\n",
      "TYR198A\n",
      "LYS208A\n",
      "TYR3B\n",
      "TYR7B\n",
      "CYS14B\n",
      "ASP23B\n",
      "LYS29B\n",
      "GLU30B\n",
      "GLU31B\n",
      "GLU36B\n",
      "GLU40B\n",
      "LYS44B\n",
      "CYS47B\n",
      "TYR49B\n",
      "LYS54B\n",
      "ASP57B\n",
      "ASP59B\n",
      "TYR63B\n",
      "HIS71B\n",
      "TYR79B\n",
      "LYS81B\n",
      "ASP82B\n",
      "GLU85B\n",
      "ASP90B\n",
      "ASP94B\n",
      "GLU97B\n",
      "ASP98B\n",
      "CYS101B\n",
      "LYS102B\n",
      "TYR103B\n",
      "TYR108B\n",
      "TYR111B\n",
      "GLU112B\n",
      "LYS115B\n",
      "ASP116B\n",
      "ASP117B\n",
      "TYR118B\n",
      "LYS120B\n",
      "LYS127B\n",
      "GLU130B\n",
      "LYS140B\n",
      "ASP146B\n",
      "ASP152B\n",
      "TYR153B\n",
      "ASP157B\n",
      "HIS162B\n",
      "GLU163B\n",
      "CYS169B\n",
      "ASP171B\n",
      "TYR179B\n",
      "LYS188B\n",
      "LYS190B\n",
      "GLU197B\n",
      "TYR198B\n",
      "LYS208B\n",
      "109l\n",
      "GLU5A\n",
      "ASP10A\n",
      "GLU11A\n",
      "LYS16A\n",
      "TYR18A\n",
      "LYS19A\n",
      "ASP20A\n",
      "GLU22A\n",
      "TYR24A\n",
      "TYR25A\n",
      "HIS31A\n",
      "LYS35A\n",
      "LYS43A\n",
      "LYS44A\n",
      "GLU45A\n",
      "ASP47A\n",
      "LYS48A\n",
      "LYS60A\n",
      "ASP61A\n",
      "GLU62A\n",
      "GLU64A\n",
      "LYS65A\n",
      "ASP70A\n",
      "ASP72A\n",
      "LYS83A\n",
      "LYS85A\n",
      "TYR88A\n",
      "ASP89A\n",
      "ASP92A\n",
      "GLU108A\n",
      "LYS124A\n",
      "ASP127A\n",
      "GLU128A\n",
      "LYS135A\n",
      "TYR139A\n",
      "LYS147A\n",
      "ASP159A\n",
      "TYR161A\n",
      "LYS162A\n",
      "111m\n",
      "GLU5A\n",
      "GLU7A\n",
      "HIS13A\n",
      "LYS17A\n",
      "GLU19A\n",
      "ASP21A\n",
      "HIS25A\n",
      "ASP28A\n",
      "LYS35A\n",
      "HIS37A\n",
      "GLU39A\n",
      "GLU42A\n",
      "LYS43A\n",
      "ASP45A\n",
      "LYS48A\n",
      "HIS49A\n",
      "LYS51A\n",
      "GLU53A\n",
      "GLU55A\n",
      "LYS57A\n",
      "GLU60A\n",
      "ASP61A\n",
      "LYS63A\n",
      "LYS64A\n",
      "HIS65A\n",
      "LYS78A\n",
      "LYS79A\n",
      "LYS80A\n",
      "HIS82A\n",
      "HIS83A\n",
      "GLU84A\n",
      "GLU86A\n",
      "LYS88A\n",
      "HIS94A\n",
      "LYS97A\n",
      "HIS98A\n",
      "LYS99A\n",
      "LYS103A\n",
      "TYR104A\n",
      "GLU106A\n",
      "GLU110A\n",
      "HIS114A\n",
      "HIS117A\n",
      "HIS120A\n",
      "ASP127A\n",
      "LYS134A\n",
      "GLU137A\n",
      "LYS141A\n",
      "ASP142A\n",
      "LYS146A\n",
      "TYR147A\n",
      "LYS148A\n",
      "GLU149A\n",
      "TYR152A\n",
      "110m\n",
      "GLU5A\n",
      "GLU7A\n",
      "HIS13A\n",
      "LYS17A\n",
      "GLU19A\n",
      "ASP21A\n",
      "HIS25A\n",
      "ASP28A\n",
      "LYS35A\n",
      "HIS37A\n",
      "GLU39A\n",
      "GLU42A\n",
      "LYS43A\n",
      "ASP45A\n",
      "LYS48A\n",
      "HIS49A\n",
      "LYS51A\n",
      "GLU53A\n",
      "GLU55A\n",
      "LYS57A\n",
      "GLU60A\n",
      "ASP61A\n",
      "LYS63A\n",
      "LYS64A\n",
      "HIS65A\n",
      "LYS78A\n",
      "LYS79A\n",
      "LYS80A\n",
      "HIS82A\n",
      "HIS83A\n",
      "GLU84A\n",
      "GLU86A\n",
      "LYS88A\n",
      "HIS94A\n",
      "LYS97A\n",
      "HIS98A\n",
      "LYS99A\n",
      "LYS103A\n",
      "TYR104A\n",
      "GLU106A\n",
      "GLU110A\n",
      "HIS114A\n",
      "HIS117A\n",
      "HIS120A\n",
      "ASP127A\n",
      "LYS134A\n",
      "GLU137A\n",
      "LYS141A\n",
      "ASP142A\n",
      "LYS146A\n",
      "TYR147A\n",
      "LYS148A\n",
      "GLU149A\n",
      "TYR152A\n",
      "108m\n",
      "GLU5A\n",
      "GLU7A\n",
      "HIS13A\n",
      "LYS17A\n",
      "GLU19A\n",
      "ASP21A\n",
      "HIS25A\n",
      "ASP28A\n",
      "LYS35A\n",
      "HIS37A\n",
      "GLU39A\n",
      "GLU42A\n",
      "LYS43A\n",
      "ASP45A\n",
      "LYS48A\n",
      "HIS49A\n",
      "LYS51A\n",
      "GLU53A\n",
      "GLU55A\n",
      "LYS57A\n",
      "GLU60A\n",
      "ASP61A\n",
      "LYS63A\n",
      "LYS64A\n",
      "HIS65A\n",
      "LYS78A\n",
      "LYS79A\n",
      "LYS80A\n",
      "HIS82A\n",
      "HIS83A\n",
      "GLU84A\n",
      "GLU86A\n",
      "LYS88A\n",
      "HIS94A\n",
      "LYS97A\n",
      "HIS98A\n",
      "LYS99A\n",
      "LYS103A\n",
      "TYR104A\n",
      "GLU106A\n",
      "GLU110A\n",
      "HIS114A\n",
      "HIS117A\n",
      "HIS120A\n",
      "ASP127A\n",
      "LYS134A\n",
      "GLU137A\n",
      "LYS141A\n",
      "ASP142A\n",
      "LYS146A\n",
      "TYR147A\n",
      "LYS148A\n",
      "GLU149A\n",
      "TYR152A\n",
      "110l\n",
      "GLU5A\n",
      "ASP10A\n",
      "GLU11A\n",
      "LYS16A\n",
      "TYR18A\n",
      "LYS19A\n",
      "ASP20A\n",
      "GLU22A\n",
      "TYR24A\n",
      "TYR25A\n",
      "HIS31A\n",
      "LYS35A\n",
      "LYS43A\n",
      "GLU45A\n",
      "ASP47A\n",
      "LYS48A\n",
      "LYS60A\n",
      "ASP61A\n",
      "GLU62A\n",
      "GLU64A\n",
      "LYS65A\n",
      "ASP70A\n",
      "ASP72A\n",
      "LYS83A\n",
      "LYS85A\n",
      "TYR88A\n",
      "ASP89A\n",
      "ASP92A\n",
      "GLU108A\n",
      "LYS124A\n",
      "ASP127A\n",
      "GLU128A\n",
      "LYS135A\n",
      "TYR139A\n",
      "LYS147A\n",
      "ASP159A\n",
      "TYR161A\n",
      "LYS162A\n",
      "109m\n",
      "GLU5A\n",
      "GLU7A\n",
      "HIS13A\n",
      "LYS17A\n",
      "GLU19A\n",
      "ASP21A\n",
      "HIS25A\n",
      "ASP28A\n",
      "LYS35A\n",
      "HIS37A\n",
      "GLU39A\n",
      "GLU42A\n",
      "LYS43A\n",
      "ASP45A\n",
      "LYS48A\n",
      "HIS49A\n",
      "LYS51A\n",
      "GLU53A\n",
      "GLU55A\n",
      "LYS57A\n",
      "GLU60A\n",
      "ASP61A\n",
      "LYS63A\n",
      "LYS64A\n",
      "HIS65A\n",
      "LYS78A\n",
      "LYS79A\n",
      "LYS80A\n",
      "HIS82A\n",
      "HIS83A\n",
      "GLU84A\n",
      "GLU86A\n",
      "LYS88A\n",
      "HIS94A\n",
      "LYS97A\n",
      "HIS98A\n",
      "LYS99A\n",
      "LYS103A\n",
      "TYR104A\n",
      "GLU106A\n",
      "GLU110A\n",
      "HIS114A\n",
      "HIS117A\n",
      "HIS120A\n",
      "ASP127A\n",
      "LYS134A\n",
      "GLU137A\n",
      "LYS141A\n",
      "ASP142A\n",
      "LYS146A\n",
      "TYR147A\n",
      "LYS148A\n",
      "GLU149A\n",
      "TYR152A\n",
      "111l\n",
      "GLU5A\n",
      "ASP10A\n",
      "GLU11A\n",
      "LYS16A\n",
      "TYR18A\n",
      "LYS19A\n",
      "ASP20A\n",
      "GLU22A\n",
      "TYR24A\n",
      "TYR25A\n",
      "HIS31A\n",
      "LYS35A\n",
      "LYS43A\n",
      "GLU45A\n",
      "ASP47A\n",
      "LYS48A\n",
      "LYS60A\n",
      "ASP61A\n",
      "GLU62A\n",
      "GLU64A\n",
      "LYS65A\n",
      "ASP70A\n",
      "ASP72A\n",
      "LYS83A\n",
      "LYS85A\n",
      "TYR88A\n",
      "ASP89A\n",
      "ASP92A\n",
      "GLU108A\n",
      "LYS124A\n",
      "ASP127A\n",
      "GLU128A\n",
      "LYS135A\n",
      "TYR139A\n",
      "LYS147A\n",
      "ASP159A\n",
      "TYR161A\n",
      "LYS162A\n",
      "108l\n",
      "GLU5A\n",
      "ASP10A\n",
      "GLU11A\n",
      "LYS16A\n",
      "TYR18A\n",
      "LYS19A\n",
      "ASP20A\n",
      "GLU22A\n",
      "TYR24A\n",
      "TYR25A\n",
      "HIS31A\n",
      "LYS35A\n",
      "LYS43A\n",
      "GLU45A\n",
      "ASP47A\n",
      "LYS48A\n",
      "LYS60A\n",
      "ASP61A\n",
      "GLU62A\n",
      "GLU64A\n",
      "LYS65A\n",
      "ASP70A\n",
      "ASP72A\n",
      "LYS83A\n",
      "LYS85A\n",
      "TYR88A\n",
      "ASP89A\n",
      "ASP92A\n",
      "GLU108A\n",
      "LYS124A\n",
      "ASP127A\n",
      "GLU128A\n",
      "LYS135A\n",
      "TYR139A\n",
      "LYS147A\n",
      "ASP159A\n",
      "TYR161A\n",
      "LYS162A\n",
      "104m\n",
      "GLU4A\n",
      "GLU6A\n",
      "HIS12A\n",
      "LYS16A\n",
      "GLU18A\n",
      "ASP20A\n",
      "HIS24A\n",
      "ASP27A\n",
      "LYS34A\n",
      "HIS36A\n",
      "GLU38A\n",
      "GLU41A\n",
      "LYS42A\n",
      "ASP44A\n",
      "LYS47A\n",
      "HIS48A\n",
      "LYS50A\n",
      "GLU52A\n",
      "GLU54A\n",
      "LYS56A\n",
      "GLU59A\n",
      "ASP60A\n",
      "LYS62A\n",
      "LYS63A\n",
      "HIS64A\n",
      "LYS77A\n",
      "LYS78A\n",
      "LYS79A\n",
      "HIS81A\n",
      "HIS82A\n",
      "GLU83A\n",
      "GLU85A\n",
      "LYS87A\n",
      "HIS93A\n",
      "LYS96A\n",
      "HIS97A\n",
      "LYS98A\n",
      "LYS102A\n",
      "TYR103A\n",
      "GLU105A\n",
      "GLU109A\n",
      "HIS113A\n",
      "HIS116A\n",
      "HIS119A\n",
      "ASP122A\n",
      "ASP126A\n",
      "LYS133A\n",
      "GLU136A\n",
      "LYS140A\n",
      "ASP141A\n",
      "LYS145A\n",
      "TYR146A\n",
      "LYS147A\n",
      "GLU148A\n",
      "TYR151A\n",
      "106m\n",
      "GLU5A\n",
      "GLU7A\n",
      "HIS13A\n",
      "LYS17A\n",
      "GLU19A\n",
      "ASP21A\n",
      "HIS25A\n",
      "ASP28A\n",
      "LYS35A\n",
      "HIS37A\n",
      "GLU39A\n",
      "GLU42A\n",
      "LYS43A\n",
      "ASP45A\n",
      "LYS48A\n",
      "HIS49A\n",
      "LYS51A\n",
      "GLU53A\n",
      "GLU55A\n",
      "LYS57A\n",
      "GLU60A\n",
      "ASP61A\n",
      "LYS63A\n",
      "LYS64A\n",
      "HIS65A\n",
      "LYS78A\n",
      "LYS79A\n",
      "LYS80A\n",
      "HIS82A\n",
      "HIS83A\n",
      "GLU84A\n",
      "GLU86A\n",
      "LYS88A\n",
      "HIS94A\n",
      "LYS97A\n",
      "HIS98A\n",
      "LYS99A\n",
      "LYS103A\n",
      "TYR104A\n",
      "GLU106A\n",
      "GLU110A\n",
      "HIS114A\n",
      "HIS117A\n",
      "HIS120A\n",
      "ASP127A\n",
      "LYS134A\n",
      "GLU137A\n",
      "LYS141A\n",
      "ASP142A\n",
      "LYS146A\n",
      "TYR147A\n",
      "LYS148A\n",
      "GLU149A\n",
      "TYR152A\n",
      "105m\n",
      "GLU4A\n",
      "GLU6A\n",
      "HIS12A\n",
      "LYS16A\n",
      "GLU18A\n",
      "ASP20A\n",
      "HIS24A\n",
      "ASP27A\n",
      "LYS34A\n",
      "HIS36A\n",
      "GLU38A\n",
      "GLU41A\n",
      "LYS42A\n",
      "ASP44A\n",
      "LYS47A\n",
      "HIS48A\n",
      "LYS50A\n",
      "GLU52A\n",
      "GLU54A\n",
      "LYS56A\n",
      "GLU59A\n",
      "ASP60A\n",
      "LYS62A\n",
      "LYS63A\n",
      "HIS64A\n",
      "LYS77A\n",
      "LYS78A\n",
      "LYS79A\n",
      "HIS81A\n",
      "HIS82A\n",
      "GLU83A\n",
      "GLU85A\n",
      "LYS87A\n",
      "HIS93A\n",
      "LYS96A\n",
      "HIS97A\n",
      "LYS98A\n",
      "LYS102A\n",
      "TYR103A\n",
      "GLU105A\n",
      "GLU109A\n",
      "HIS113A\n",
      "HIS116A\n",
      "HIS119A\n",
      "ASP122A\n",
      "ASP126A\n",
      "LYS133A\n",
      "GLU136A\n",
      "LYS140A\n",
      "ASP141A\n",
      "LYS145A\n",
      "TYR146A\n",
      "LYS147A\n",
      "GLU148A\n",
      "TYR151A\n",
      "103m\n",
      "GLU5A\n",
      "GLU7A\n",
      "HIS13A\n",
      "LYS17A\n",
      "GLU19A\n",
      "ASP21A\n",
      "HIS25A\n",
      "ASP28A\n",
      "LYS35A\n",
      "HIS37A\n",
      "GLU39A\n",
      "GLU42A\n",
      "LYS43A\n",
      "ASP45A\n",
      "LYS48A\n",
      "HIS49A\n",
      "LYS51A\n",
      "GLU53A\n",
      "GLU55A\n",
      "LYS57A\n",
      "GLU60A\n",
      "ASP61A\n",
      "LYS63A\n",
      "LYS64A\n",
      "LYS78A\n",
      "LYS79A\n",
      "LYS80A\n",
      "HIS82A\n",
      "HIS83A\n",
      "GLU84A\n",
      "GLU86A\n",
      "LYS88A\n",
      "HIS94A\n",
      "LYS97A\n",
      "HIS98A\n",
      "LYS99A\n",
      "LYS103A\n",
      "TYR104A\n",
      "GLU106A\n",
      "GLU110A\n",
      "HIS114A\n",
      "HIS117A\n",
      "HIS120A\n",
      "ASP127A\n",
      "LYS134A\n",
      "GLU137A\n",
      "LYS141A\n",
      "ASP142A\n",
      "LYS146A\n",
      "TYR147A\n",
      "LYS148A\n",
      "GLU149A\n",
      "TYR152A\n",
      "107m\n",
      "GLU5A\n",
      "GLU7A\n",
      "HIS13A\n",
      "LYS17A\n",
      "GLU19A\n",
      "ASP21A\n",
      "HIS25A\n",
      "ASP28A\n",
      "LYS35A\n",
      "HIS37A\n",
      "GLU39A\n",
      "GLU42A\n",
      "LYS43A\n",
      "ASP45A\n",
      "LYS48A\n",
      "HIS49A\n",
      "LYS51A\n",
      "GLU53A\n",
      "GLU55A\n",
      "LYS57A\n",
      "GLU60A\n",
      "ASP61A\n",
      "LYS63A\n",
      "LYS64A\n",
      "HIS65A\n",
      "LYS78A\n",
      "LYS79A\n",
      "LYS80A\n",
      "HIS82A\n",
      "HIS83A\n",
      "GLU84A\n",
      "GLU86A\n",
      "LYS88A\n",
      "HIS94A\n",
      "LYS97A\n",
      "HIS98A\n",
      "LYS99A\n",
      "LYS103A\n",
      "TYR104A\n",
      "GLU106A\n",
      "GLU110A\n",
      "HIS114A\n",
      "HIS117A\n",
      "HIS120A\n",
      "ASP127A\n",
      "LYS134A\n",
      "GLU137A\n",
      "LYS141A\n",
      "ASP142A\n",
      "LYS146A\n",
      "TYR147A\n",
      "LYS148A\n",
      "GLU149A\n",
      "TYR152A\n",
      "107l\n",
      "GLU5A\n",
      "ASP10A\n",
      "GLU11A\n",
      "LYS16A\n",
      "TYR18A\n",
      "LYS19A\n",
      "ASP20A\n",
      "GLU22A\n",
      "TYR24A\n",
      "TYR25A\n",
      "HIS31A\n",
      "LYS35A\n",
      "LYS43A\n",
      "GLU45A\n",
      "ASP47A\n",
      "LYS48A\n",
      "LYS60A\n",
      "ASP61A\n",
      "GLU62A\n",
      "GLU64A\n",
      "LYS65A\n",
      "ASP70A\n",
      "ASP72A\n",
      "LYS83A\n",
      "LYS85A\n",
      "TYR88A\n",
      "ASP89A\n",
      "ASP92A\n",
      "GLU108A\n",
      "LYS124A\n",
      "ASP127A\n",
      "GLU128A\n",
      "LYS135A\n",
      "TYR139A\n",
      "LYS147A\n",
      "ASP159A\n",
      "TYR161A\n",
      "LYS162A\n",
      "103l\n",
      "GLU5A\n",
      "ASP10A\n",
      "GLU11A\n",
      "LYS16A\n",
      "TYR18A\n",
      "LYS19A\n",
      "ASP20A\n",
      "GLU22A\n",
      "TYR24A\n",
      "TYR25A\n",
      "HIS31A\n",
      "LYS35A\n",
      "ASP43CA\n",
      "LYS46A\n",
      "GLU48A\n",
      "ASP50A\n",
      "LYS51A\n",
      "LYS63A\n",
      "ASP64A\n",
      "GLU65A\n",
      "GLU67A\n",
      "LYS68A\n",
      "ASP73A\n",
      "ASP75A\n",
      "LYS86A\n",
      "LYS88A\n",
      "TYR91A\n",
      "ASP92A\n",
      "ASP95A\n",
      "GLU111A\n",
      "LYS127A\n",
      "ASP130A\n",
      "GLU131A\n",
      "LYS138A\n",
      "TYR142A\n",
      "LYS150A\n",
      "ASP162A\n",
      "TYR164A\n",
      "LYS165A\n",
      "102m\n",
      "GLU5A\n",
      "GLU7A\n",
      "HIS13A\n",
      "LYS17A\n",
      "GLU19A\n",
      "ASP21A\n",
      "HIS25A\n",
      "ASP28A\n",
      "LYS35A\n",
      "HIS37A\n",
      "GLU39A\n",
      "GLU42A\n",
      "LYS43A\n",
      "ASP45A\n",
      "LYS48A\n",
      "HIS49A\n",
      "LYS51A\n",
      "GLU53A\n",
      "GLU55A\n",
      "LYS57A\n",
      "GLU60A\n",
      "ASP61A\n",
      "LYS63A\n",
      "LYS64A\n",
      "LYS78A\n",
      "LYS79A\n",
      "LYS80A\n",
      "HIS82A\n",
      "HIS83A\n",
      "GLU84A\n",
      "GLU86A\n",
      "LYS88A\n",
      "HIS94A\n",
      "LYS97A\n",
      "HIS98A\n",
      "LYS99A\n",
      "LYS103A\n",
      "TYR104A\n",
      "GLU106A\n",
      "GLU110A\n",
      "HIS114A\n",
      "HIS117A\n",
      "HIS120A\n",
      "ASP127A\n",
      "LYS134A\n",
      "GLU137A\n",
      "LYS141A\n",
      "ASP142A\n",
      "LYS146A\n",
      "TYR147A\n",
      "LYS148A\n",
      "GLU149A\n",
      "TYR152A\n",
      "102l\n",
      "GLU5A\n",
      "ASP10A\n",
      "GLU11A\n",
      "LYS16A\n",
      "TYR18A\n",
      "LYS19A\n",
      "ASP20A\n",
      "GLU22A\n",
      "TYR24A\n",
      "TYR25A\n",
      "HIS31A\n",
      "LYS35A\n",
      "LYS44A\n",
      "GLU46A\n",
      "ASP48A\n",
      "LYS49A\n",
      "LYS61A\n",
      "ASP62A\n",
      "GLU63A\n",
      "GLU65A\n",
      "LYS66A\n",
      "ASP71A\n",
      "ASP73A\n",
      "LYS84A\n",
      "LYS86A\n",
      "TYR89A\n",
      "ASP90A\n",
      "ASP93A\n",
      "GLU109A\n",
      "LYS125A\n",
      "ASP128A\n",
      "GLU129A\n",
      "LYS136A\n",
      "TYR140A\n",
      "LYS148A\n",
      "ASP160A\n",
      "TYR162A\n",
      "LYS163A\n",
      "101m\n",
      "GLU5A\n",
      "GLU7A\n",
      "HIS13A\n",
      "LYS17A\n",
      "GLU19A\n",
      "ASP21A\n",
      "HIS25A\n",
      "ASP28A\n",
      "LYS35A\n",
      "HIS37A\n",
      "GLU39A\n",
      "GLU42A\n",
      "LYS43A\n",
      "ASP45A\n",
      "LYS48A\n",
      "HIS49A\n",
      "LYS51A\n",
      "GLU53A\n",
      "GLU55A\n",
      "LYS57A\n",
      "GLU60A\n",
      "ASP61A\n",
      "LYS63A\n",
      "LYS64A\n",
      "HIS65A\n",
      "LYS78A\n",
      "LYS79A\n",
      "LYS80A\n",
      "HIS82A\n",
      "HIS83A\n",
      "GLU84A\n",
      "GLU86A\n",
      "LYS88A\n",
      "HIS94A\n",
      "LYS97A\n",
      "HIS98A\n",
      "LYS99A\n",
      "LYS103A\n",
      "TYR104A\n",
      "GLU106A\n",
      "GLU110A\n",
      "HIS114A\n",
      "HIS117A\n",
      "HIS120A\n",
      "ASP127A\n",
      "LYS134A\n",
      "GLU137A\n",
      "LYS141A\n",
      "ASP142A\n",
      "LYS146A\n",
      "TYR147A\n",
      "LYS148A\n",
      "GLU149A\n",
      "TYR152A\n",
      "104l\n",
      "GLU5A\n",
      "ASP10A\n",
      "GLU11A\n",
      "LYS16A\n",
      "TYR18A\n",
      "LYS19A\n",
      "ASP20A\n",
      "GLU22A\n",
      "TYR24A\n",
      "TYR25A\n",
      "HIS31A\n",
      "LYS35A\n",
      "LYS43A\n",
      "GLU47A\n",
      "ASP49A\n",
      "LYS50A\n",
      "LYS62A\n",
      "ASP63A\n",
      "GLU64A\n",
      "GLU66A\n",
      "LYS67A\n",
      "ASP72A\n",
      "ASP74A\n",
      "LYS85A\n",
      "LYS87A\n",
      "TYR90A\n",
      "ASP91A\n",
      "ASP94A\n",
      "GLU110A\n",
      "LYS126A\n",
      "ASP129A\n",
      "GLU130A\n",
      "LYS137A\n",
      "TYR141A\n",
      "LYS149A\n",
      "ASP161A\n",
      "TYR163A\n",
      "LYS164A\n",
      "GLU5B\n",
      "ASP10B\n",
      "GLU11B\n",
      "LYS16B\n",
      "TYR18B\n",
      "LYS19B\n",
      "ASP20B\n",
      "GLU22B\n",
      "TYR24B\n",
      "TYR25B\n",
      "HIS31B\n",
      "LYS35B\n",
      "LYS43B\n",
      "GLU47B\n",
      "ASP49B\n",
      "LYS50B\n",
      "LYS62B\n",
      "ASP63B\n",
      "GLU64B\n",
      "GLU66B\n",
      "LYS67B\n",
      "ASP72B\n",
      "ASP74B\n",
      "LYS85B\n",
      "LYS87B\n",
      "TYR90B\n",
      "ASP91B\n",
      "ASP94B\n",
      "GLU110B\n",
      "LYS126B\n",
      "ASP129B\n",
      "GLU130B\n",
      "LYS137B\n",
      "TYR141B\n",
      "LYS149B\n",
      "ASP161B\n",
      "TYR163B\n",
      "LYS164B\n"
     ]
    }
   ],
   "source": [
    "def get_data(pdbs, df):\n",
    "    \"\"\"returns dictionary of data with pdbids as keys and residueid indexed positions, atomic node and species information, input features (hydrogen indices), and the number of atoms in each sample (residue).\n",
    "    all_species: {199l: {ASP1A : {CA:1} , ..., {TYR100 : {OXT:1032} } \"\"\"\n",
    "    \n",
    "    all_pos, all_species, all_lengths, all_feats = {},{},{},{}\n",
    "\n",
    "    counter=0\n",
    "\n",
    "\n",
    "    for pdb in pdbs:\n",
    "        resis = data[pdb] #for one protein\n",
    "        lengths, species, feats, R ={}, {}, {}, {}\n",
    "        \n",
    "        for resid, vals in resis.items():\n",
    "            hindices = []\n",
    "            counter=0\n",
    "            resi_pos, resi_species ={},{}\n",
    "            \n",
    "            for atom, pos in vals.items():\n",
    "                node = atom.split()  # name, pos\n",
    "                name, nodei =node[0], int(node[1])\n",
    "\n",
    "                if name[0] == 'H':\n",
    "                    hindices.append(counter)\n",
    "\n",
    "                resi_pos[counter] = pos\n",
    "                resi_species[name] = counter\n",
    "\n",
    "                counter +=1\n",
    "            \n",
    "\n",
    "            f = torch.zeros(counter)        \n",
    "            f[hindices] = 1\n",
    "            feats[resid] = f #hydrogen indices\n",
    "\n",
    "            species[resid] = resi_species \n",
    "            R[resid] = resi_pos\n",
    "\n",
    "            lengths[resid] = counter #num atoms of a sample (resi)\n",
    "\n",
    "        all_pos[pdb]= R\n",
    "        all_species[pdb] = species\n",
    "        all_lengths[pdb] = lengths\n",
    "        all_feats[pdb] = feats\n",
    "\n",
    "        #get labels\n",
    "        d = df[df.iloc[:, 0] == pdb].drop(columns = [\"idcode\"])\n",
    "        all_targets[pdb] = get_targets(d.compute().to_dict(orient=\"records\"))\n",
    "\n",
    "        def ionic_features(pdb, all_ions):\n",
    "\n",
    "            pdb_ions={}\n",
    "            Ls = all_lengths[pdb]\n",
    "            \n",
    "            for resi in data[pdb].keys():\n",
    "                resi_species = (all_species[pdb])[resi]\n",
    "                ion = torch.zeros(Ls[resi])\n",
    "                resname = resi[:3]\n",
    "                idxs = [resi_species.get(a) for a in anions[resname]] \n",
    "                ion[idxs]= -1\n",
    "                \n",
    "                cats = cations[resname]\n",
    "                if cats:\n",
    "                    cs = [c for c in cats if c is not None]\n",
    "                    cats =[resi_species.get(a) for a in cs]\n",
    "                    cs = [c for c in cats if c is not None]\n",
    "                    ion[cs] = 1\n",
    "                pdb_ions[resi] = ion\n",
    "                \n",
    "            return all_ions\n",
    "        \n",
    "        all_ions[pdb] = ionic_features(pdbs, {})\n",
    "\n",
    "    dicts = (all_ions, all_feats, all_pos, all_lengths, all_targets)\n",
    "    titles = [\"ions\", \"Hs\", \"pos\", \"L\", \"targets\"]\n",
    "\n",
    "# Initialize a defaultdict of dicts\n",
    "    all_data = defaultdict(dict)\n",
    "\n",
    "# Merge dictionaries with titles\n",
    "    for title, d in zip(titles, dicts):\n",
    "        for key, value in d.items():\n",
    "            all_data[key][title] = value\n",
    "\n",
    "    return all_data\n",
    "\n",
    "all_data = get_data(pdbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10gs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109l\n",
      "111m\n",
      "110m\n",
      "108m\n",
      "110l\n",
      "109m\n",
      "111l\n",
      "108l\n",
      "104m\n",
      "106m\n",
      "105m\n",
      "103m\n",
      "107m\n",
      "107l\n",
      "103l\n",
      "102m\n",
      "102l\n",
      "101m\n",
      "104l\n"
     ]
    }
   ],
   "source": [
    "#r = sample[:3]\n",
    "            #out = torch.mean(x, dim=1)\n",
    "\n",
    "#test on failed to load cuz pypka error \n",
    "Ds = {}\n",
    "fails=[]\n",
    "for pdb in pdbs:\n",
    "    ds={}\n",
    "    try:\n",
    "        \n",
    "        maxes, Ns, scs =[],[],[]\n",
    "\n",
    "        d = all_data[pdb]\n",
    "        \n",
    "        pos, hs, ions = d[\"pos\"], d[\"Hs\"], d[\"ions\"]#, d[\"targets\"]\n",
    "        \n",
    "        \n",
    "        for sample in pos.keys():\n",
    "            label=all_targets[pdb][sample]\n",
    "            coords = torch.tensor(tuple(pos[sample].values()))\n",
    "            L = coords.shape[0]\n",
    "            \n",
    "            x, loss = loop(1, coords, torch.tensor(hs[sample], dtype=int), ions[sample], net, optimizer, 1) #margin, negative slope\n",
    "            if loss > .1:\n",
    "                print(loss, pdb)\n",
    "            y=torch.mean(x, dim=1).unsqueeze(0).detach().numpy()*coords.detach().numpy().T\n",
    "            \n",
    "            ds[sample] = np.gradient(np.gradient(np.column_stack(y).flatten())).reshape(-1,3)\n",
    "        \n",
    "        Ds[pdb] = ds\n",
    "            #loss = torch.nn.HuberLoss()\n",
    "    except:\n",
    "        fails.append(pdb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    #losses[pdb] = loss2\n",
    "\n",
    "    #outs2[pdb] = outie\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GLU5A': 3.796177,\n",
       " 'ASP10A': 3.0374708,\n",
       " 'GLU11A': 3.7354913,\n",
       " 'LYS16A': 11.058256,\n",
       " 'TYR18A': 9.971584,\n",
       " 'LYS19A': 10.785202,\n",
       " 'ASP20A': 3.0870056,\n",
       " 'GLU22A': 3.7509766,\n",
       " 'TYR24A': 11.501759,\n",
       " 'TYR25A': 10.8952675,\n",
       " 'HIS31A': 7.0591774,\n",
       " 'LYS35A': 7.9153605,\n",
       " 'LYS43A': 10.04669,\n",
       " 'GLU47A': 3.180633,\n",
       " 'ASP49A': 3.0032482,\n",
       " 'LYS50A': 11.529646,\n",
       " 'LYS62A': 11.032026,\n",
       " 'ASP63A': 3.2223406,\n",
       " 'GLU64A': 3.6213398,\n",
       " 'GLU66A': 3.7888856,\n",
       " 'LYS67A': 10.947372,\n",
       " 'ASP72A': 3.0073657,\n",
       " 'ASP74A': 3.1027818,\n",
       " 'LYS85A': 11.062357,\n",
       " 'LYS87A': 11.098835,\n",
       " 'TYR90A': 11.106178,\n",
       " 'ASP91A': 2.9063745,\n",
       " 'ASP94A': 2.9202952,\n",
       " 'GLU110A': 3.6656814,\n",
       " 'LYS126A': 10.059591,\n",
       " 'ASP129A': 3.1636667,\n",
       " 'GLU130A': 3.7978287,\n",
       " 'LYS137A': 9.785675,\n",
       " 'TYR141A': 9.576412,\n",
       " 'LYS149A': 10.9088955,\n",
       " 'ASP161A': 3.199821,\n",
       " 'TYR163A': 10.891399,\n",
       " 'LYS164A': 9.979883,\n",
       " 'GLU5B': 3.5775719,\n",
       " 'ASP10B': 3.1565375,\n",
       " 'GLU11B': 3.0792613,\n",
       " 'LYS16B': 11.0568695,\n",
       " 'TYR18B': 10.101749,\n",
       " 'LYS19B': 10.418552,\n",
       " 'ASP20B': 3.0978599,\n",
       " 'GLU22B': 3.7889485,\n",
       " 'TYR24B': 10.939865,\n",
       " 'TYR25B': 10.5221195,\n",
       " 'HIS31B': 7.222171,\n",
       " 'LYS35B': 10.681959,\n",
       " 'LYS43B': 11.052635,\n",
       " 'GLU47B': 3.5748267,\n",
       " 'ASP49B': 3.1679816,\n",
       " 'LYS50B': 11.065203,\n",
       " 'LYS62B': 10.539764,\n",
       " 'ASP63B': 3.0148754,\n",
       " 'GLU64B': 3.5392504,\n",
       " 'GLU66B': 3.163186,\n",
       " 'LYS67B': 10.630552,\n",
       " 'ASP72B': 3.1853962,\n",
       " 'ASP74B': 3.1797214,\n",
       " 'LYS85B': 10.559164,\n",
       " 'LYS87B': 11.532242,\n",
       " 'TYR90B': 10.827587,\n",
       " 'ASP91B': 3.1088653,\n",
       " 'ASP94B': 3.13349,\n",
       " 'GLU110B': 3.7797604,\n",
       " 'LYS126B': 10.565582,\n",
       " 'ASP129B': 2.9024067,\n",
       " 'GLU130B': 3.638999,\n",
       " 'LYS137B': 10.567177,\n",
       " 'TYR141B': 10.891006,\n",
       " 'LYS149B': 10.822418,\n",
       " 'ASP161B': 3.0541983,\n",
       " 'TYR163B': 11.062122,\n",
       " 'LYS164B': 10.835701}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs_val[\"104l\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = EGNN_Network(\n",
    "        num_tokens = 6, #vocabulary siye, number of unique species\n",
    "        num_positions = 23,  #number of nodes         # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "        dim = 2,# #internal rep size. c has square dependence. richer rep but overfitting for small d.s.\n",
    "        depth = 2, #number of layers #deeper need more memort to store intermediate reps\n",
    "        num_nearest_neighbors = 2, #number of nearest neighbors to consider #make this the max hood size\n",
    "        dropout=.03,\n",
    "        m_pool_method='mean')\n",
    "    \n",
    "#net, optimizer = model(23, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "\n",
    "optimizer3 = torch.optim.Adam(pnet.parameters(), lr=.1, weight_decay=.1)\n",
    "\n",
    "pnet = EGNN_Network(\n",
    "        num_tokens = 6, #vocabulary siye, number of unique species\n",
    "        num_positions = 23,  #number of nodes         # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "        dim = 2,# #internal rep size. c has square dependence. richer rep but overfitting for small d.s.\n",
    "        depth = 2, #number of layers #deeper need more memort to store intermediate reps\n",
    "        num_nearest_neighbors = 2, #number of nearest neighbors to consider #make this the max hood size\n",
    "        dropout=.0,\n",
    "        m_pool_method='mean')\n",
    "    \n",
    "#net, optimizer = model(23, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "\n",
    "optimizer3 = torch.optim.Adam(pnet.parameters(), lr=.01, weight_decay=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs2={}\n",
    "losses={}\n",
    "#\n",
    " def run_m2(pdbs, all_data, ):\n",
    "\n",
    "for i in range(50):\n",
    "    print(f\"starting loop {i}\")\n",
    "    \n",
    "    for pdb in pdbs: #Ds.keys():\n",
    "        if pdb not in fails:\n",
    "        \n",
    "            outie = {}\n",
    "            #print(pdb)\n",
    "            #Ddict = Ds[pdb]\n",
    "        \n",
    "\n",
    "            d = all_data[pdb]\n",
    "            pos, hs, ions, T = d[\"pos\"], d[\"Hs\"], d[\"ions\"], d[\"targets\"]\n",
    "\n",
    "            \n",
    "            for sample in pos.keys():\n",
    "\n",
    "                coords = torch.tensor(np.gradient(np.gradient(torch.tensor(tuple(pos[sample].values())))))\n",
    "                #species = torch.tensor(hs[sample], dtype=int) #todo\n",
    "                L = coords.shape[0]\n",
    "                #net, optimizer = model(L, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "                #x = loop(10, coords, torch.tensor(hs[sample], dtype=int), ions[sample], net, optimizer, 1) #margin, negative slope\n",
    "                #out = torch.mean(x, dim=1)\n",
    "                #print(sample)\n",
    "                #D=Ddict[sample]\n",
    "                \n",
    "                #f = torch.mean(x, dim=1).unsqueeze(0).detach().numpy()*coords.detach().numpy().T\n",
    "                #combined = np.column_stack(f).flatten()\n",
    "                \n",
    "                #D=np.gradient(np.gradient(np.column_stack(f).flatten())).reshape(-1,3)\n",
    "                #plot(x[0], D)\n",
    "                def loop2(D, sample):\n",
    "                    #maxes=[]#,[],[]\n",
    "                    #print(sample, pdb)\n",
    "                    #r = sample[:3]\n",
    "                    #out = torch.mean(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "                    loss = torch.nn.HuberLoss()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "                    ins = torch.ones(L)\n",
    "                    ins[0]=0\n",
    "                    ins[-1] = 2#first index 0, all other atoms have index 1, terminal side chain atom has index 2\n",
    "\n",
    "                    label=T[sample]\n",
    "        \n",
    "                    \n",
    "                    for epoch in range(4):\n",
    "\n",
    "                        optimizer3.zero_grad()\n",
    "                        y=torch.mean(pnet(torch.tensor(ins, dtype=int).unsqueeze(0), torch.tensor(D).unsqueeze(0))[0],dim=-1)\n",
    "                    \n",
    "                        \n",
    "                        loss2=loss(max(y[0][1::]), torch.tensor(label))\n",
    "                        \n",
    "                        if epoch==0:\n",
    "                            print(loss2.detach().item(), \"!!epoch0\", )\n",
    "                        #elif epoch==up - 1:\n",
    "                            #print(up, loss2.detach().item())\n",
    "                        loss2.backward()\n",
    "                        #print(loss2)\n",
    "                        \n",
    "                        optimizer3.step()\n",
    "                    print(loss2.detach().item(), \"finished 4 epochs\", sample, pdb)\n",
    "                    print(\"\")\n",
    "                    return max(y[0].detach().numpy()[1::])\n",
    "                #print(\"finished 50 epochs\", loss2)\n",
    "                        \n",
    "                    \n",
    "                    #a=y[0].detach().numpy()\n",
    "                    #maxes.append((np.where(a == max(a[1::]))[0], max(a[1::])))\n",
    "                    #maxes.append(max(a[1::]))\n",
    "                    #Ns.append(a[0])\n",
    "                    #scs.append(a[-1])\n",
    "                    #print(loss2.item())\n",
    "\n",
    "                    #print(sp, loss2)\n",
    "                    #losses[sample] = loss2\n",
    "                    #print(sample, loss2.detach(), pdb)\n",
    "                    #print(loss2,sample)\n",
    "                            # maxes#, Ns, scs\n",
    "                \n",
    "                \n",
    "                #m = loop2(D, sample)\n",
    "                #print(sample)\n",
    "                #for i in range(59):\n",
    "                #print(\"finished\", sample, loss2)\n",
    "                outie[sample] = loop2(D, sample)\n",
    "            \n",
    "            #losses[pdb] = loss2\n",
    "\n",
    "            outs2[pdb] = outie\n",
    "            #losses[pdb] = loss2\n",
    "                #coords = torch.tensor()\n",
    "            #x[\"ions\"], x[\"\"]\n",
    "            #n#et, optimizer = model(x[\"lengths, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "\n",
    "        print(\"finished one round\")\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#notFAKE\n",
    "################################################\n",
    "# 2) Dataset\n",
    "################################################\n",
    "class PDBDataset(Dataset):\n",
    "    def __init__(self, pdb_ids, outs, all_targets):\n",
    "        self.samples = []\n",
    "\n",
    "        for pdb_id in pdb_ids:\n",
    "            feats_list = []\n",
    "            targets_list = []\n",
    "            # gather per-residue feature & target\n",
    "            for res_id, feat_val in outs[pdb_id].items():\n",
    "                if res_id not in all_targets[pdb_id]:\n",
    "                    continue\n",
    "                t_val = all_targets[pdb_id][res_id]\n",
    "\n",
    "                # shape (1,)\n",
    "                f = torch.tensor([feat_val], dtype=torch.float32)\n",
    "                t = torch.tensor([t_val],   dtype=torch.float32)\n",
    "\n",
    "                feats_list.append(f)\n",
    "                targets_list.append(t)\n",
    "\n",
    "            # skip if none\n",
    "            if len(feats_list) == 0:\n",
    "                continue\n",
    "\n",
    "            # store it\n",
    "            self.samples.append((feats_list, targets_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of (feats_list, targets_list)\n",
    "    all_feats, all_targs = [], []\n",
    "    for feats_list, t_list in batch:\n",
    "        all_feats.append(feats_list)\n",
    "        all_targs.append(t_list)\n",
    "    return all_feats, all_targs\n",
    "\n",
    "\n",
    "#ti = [0,4,9,6,11,12,14,15,16,17,18]\n",
    "#tv = [1,2,3,5,7,8,10,13,19]\n",
    "#pdbs_train = [pdbs[ti] for ti in tv]\n",
    "#pdbs_val   = [pdbs[tv] for tv in ti]\n",
    "\n",
    "#print(pdbs_train)\n",
    "#train_dataset = PDBDataset(pdbs_train, outs, all_targets)\n",
    "#val_dataset   = PDBDataset(pdbs_val,   outs, all_targets)\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "#val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6907, 0.9137, 0.7695], grad_fn=<SelectBackward0>)\n",
      "3.6614752 GLU5A 111m\n",
      "\n",
      "tensor([0.7443, 0.6814, 0.7650], grad_fn=<SelectBackward0>)\n",
      "3.7015295 GLU7A 111m\n",
      "\n",
      "tensor([0.8362, 0.8259, 0.8591], grad_fn=<SelectBackward0>)\n",
      "6.904503 HIS13A 111m\n",
      "\n",
      "tensor([0.7027, 0.8739, 0.7311], grad_fn=<SelectBackward0>)\n",
      "11.53557 LYS17A 111m\n",
      "\n",
      "tensor([0.8346, 0.7066, 0.7743], grad_fn=<SelectBackward0>)\n",
      "3.6397069 GLU19A 111m\n",
      "\n",
      "tensor([0.9181, 0.9696, 0.8781], grad_fn=<SelectBackward0>)\n",
      "2.8822436 ASP21A 111m\n",
      "\n",
      "tensor([0.7370, 0.8374, 0.8209], grad_fn=<SelectBackward0>)\n",
      "7.0057755 HIS25A 111m\n",
      "\n",
      "tensor([0.9582, 0.9043, 0.8386], grad_fn=<SelectBackward0>)\n",
      "3.2122722 ASP28A 111m\n",
      "\n",
      "tensor([0.8369, 0.9631, 0.9233], grad_fn=<SelectBackward0>)\n",
      "11.100159 LYS35A 111m\n",
      "\n",
      "tensor([0.7083, 0.7837, 0.7766], grad_fn=<SelectBackward0>)\n",
      "5.821362 HIS37A 111m\n",
      "\n",
      "tensor([0.9082, 0.8407, 0.9064], grad_fn=<SelectBackward0>)\n",
      "3.6445308 GLU39A 111m\n",
      "\n",
      "tensor([0.8467, 0.6841, 0.9685], grad_fn=<SelectBackward0>)\n",
      "3.7574067 GLU42A 111m\n",
      "\n",
      "tensor([0.8486, 0.8615, 0.9373], grad_fn=<SelectBackward0>)\n",
      "9.915466 LYS43A 111m\n",
      "\n",
      "tensor([0.8855, 0.7985, 0.7831], grad_fn=<SelectBackward0>)\n",
      "2.9819636 ASP45A 111m\n",
      "\n",
      "tensor([0.7516, 0.6493, 0.7771], grad_fn=<SelectBackward0>)\n",
      "10.527309 LYS48A 111m\n",
      "\n",
      "tensor([0.7767, 0.8633, 0.7696], grad_fn=<SelectBackward0>)\n",
      "7.0563107 HIS49A 111m\n",
      "\n",
      "tensor([0.8434, 0.7865, 0.6670], grad_fn=<SelectBackward0>)\n",
      "11.056099 LYS51A 111m\n",
      "\n",
      "tensor([0.7540, 0.6543, 1.0000], grad_fn=<SelectBackward0>)\n",
      "3.0339026 GLU53A 111m\n",
      "\n",
      "tensor([0.7859, 0.7903, 0.8391], grad_fn=<SelectBackward0>)\n",
      "3.7634654 GLU55A 111m\n",
      "\n",
      "tensor([0.6959, 0.8824, 0.7751], grad_fn=<SelectBackward0>)\n",
      "10.831911 LYS57A 111m\n",
      "\n",
      "tensor([0.8479, 0.8110, 0.9321], grad_fn=<SelectBackward0>)\n",
      "3.544126 GLU60A 111m\n",
      "\n",
      "tensor([0.8581, 0.8173, 0.8791], grad_fn=<SelectBackward0>)\n",
      "3.2108169 ASP61A 111m\n",
      "\n",
      "tensor([0.8231, 0.9039, 0.6717], grad_fn=<SelectBackward0>)\n",
      "10.398195 LYS63A 111m\n",
      "\n",
      "tensor([0.6965, 0.8012, 0.6939], grad_fn=<SelectBackward0>)\n",
      "10.975306 LYS64A 111m\n",
      "\n",
      "tensor([0.9438, 0.7930, 0.9265], grad_fn=<SelectBackward0>)\n",
      "7.0646095 HIS65A 111m\n",
      "\n",
      "tensor([0.6790, 0.9410, 0.7511], grad_fn=<SelectBackward0>)\n",
      "10.92285 LYS78A 111m\n",
      "\n",
      "tensor([0.7163, 0.9325, 0.6922], grad_fn=<SelectBackward0>)\n",
      "10.614771 LYS79A 111m\n",
      "\n",
      "tensor([0.8765, 0.8162, 0.8700], grad_fn=<SelectBackward0>)\n",
      "11.062555 LYS80A 111m\n",
      "\n",
      "tensor([0.9063, 0.8153, 0.9051], grad_fn=<SelectBackward0>)\n",
      "6.8014517 HIS82A 111m\n",
      "\n",
      "tensor([0.8428, 0.8920, 0.7900], grad_fn=<SelectBackward0>)\n",
      "7.035763 HIS83A 111m\n",
      "\n",
      "tensor([0.8138, 0.8717, 0.9714], grad_fn=<SelectBackward0>)\n",
      "3.553093 GLU84A 111m\n",
      "\n",
      "tensor([0.8195, 0.9112, 0.7881], grad_fn=<SelectBackward0>)\n",
      "3.698472 GLU86A 111m\n",
      "\n",
      "tensor([0.7755, 0.9560, 0.8045], grad_fn=<SelectBackward0>)\n",
      "11.112825 LYS88A 111m\n",
      "\n",
      "tensor([0.7172, 0.7842, 0.7593], grad_fn=<SelectBackward0>)\n",
      "7.268468 HIS94A 111m\n",
      "\n",
      "tensor([0.7811, 0.8283, 0.8023], grad_fn=<SelectBackward0>)\n",
      "10.560747 LYS97A 111m\n",
      "\n",
      "tensor([0.8589, 0.7624, 0.7862], grad_fn=<SelectBackward0>)\n",
      "7.0616617 HIS98A 111m\n",
      "\n",
      "tensor([0.9399, 0.7274, 0.8014], grad_fn=<SelectBackward0>)\n",
      "11.102276 LYS99A 111m\n",
      "\n",
      "tensor([0.8005, 0.8496, 0.7217], grad_fn=<SelectBackward0>)\n",
      "11.146502 LYS103A 111m\n",
      "\n",
      "tensor([0.9915, 0.7475, 0.7785], grad_fn=<SelectBackward0>)\n",
      "10.839798 TYR104A 111m\n",
      "\n",
      "tensor([0.8096, 0.9160, 0.8031], grad_fn=<SelectBackward0>)\n",
      "3.7775664 GLU106A 111m\n",
      "\n",
      "tensor([0.6827, 0.8063, 0.7083], grad_fn=<SelectBackward0>)\n",
      "3.568153 GLU110A 111m\n",
      "\n",
      "tensor([0.6865, 0.7201, 0.7303], grad_fn=<SelectBackward0>)\n",
      "6.85989 HIS114A 111m\n",
      "\n",
      "tensor([0.9050, 0.7568, 0.7120], grad_fn=<SelectBackward0>)\n",
      "6.8740807 HIS117A 111m\n",
      "\n",
      "tensor([0.9020, 0.7468, 0.9119], grad_fn=<SelectBackward0>)\n",
      "7.26425 HIS120A 111m\n",
      "\n",
      "tensor([0.8781, 0.7004, 0.6857], grad_fn=<SelectBackward0>)\n",
      "2.8723912 ASP127A 111m\n",
      "\n",
      "tensor([0.8747, 0.8176, 0.9114], grad_fn=<SelectBackward0>)\n",
      "10.913795 LYS134A 111m\n",
      "\n",
      "tensor([1.0000, 0.9372, 0.7891], grad_fn=<SelectBackward0>)\n",
      "3.8068194 GLU137A 111m\n",
      "\n",
      "tensor([0.6254, 0.7842, 0.7215], grad_fn=<SelectBackward0>)\n",
      "11.148605 LYS141A 111m\n",
      "\n",
      "tensor([0.8595, 0.7492, 0.8262], grad_fn=<SelectBackward0>)\n",
      "3.1685667 ASP142A 111m\n",
      "\n",
      "tensor([0.9315, 0.7858, 0.9175], grad_fn=<SelectBackward0>)\n",
      "10.76099 LYS146A 111m\n",
      "\n",
      "tensor([0.8672, 0.8935, 0.8709], grad_fn=<SelectBackward0>)\n",
      "10.444563 TYR147A 111m\n",
      "\n",
      "tensor([0.8060, 0.7125, 0.8320], grad_fn=<SelectBackward0>)\n",
      "10.596519 LYS148A 111m\n",
      "\n",
      "tensor([0.8243, 0.7947, 0.7396], grad_fn=<SelectBackward0>)\n",
      "3.825955 GLU149A 111m\n",
      "\n",
      "tensor([0.8378, 0.9540, 0.7228], grad_fn=<SelectBackward0>)\n",
      "11.529188 TYR152A 111m\n",
      "\n",
      "tensor([1.0000, 0.9004, 0.9210], grad_fn=<SelectBackward0>)\n",
      "3.7603664 GLU5A 110m\n",
      "\n",
      "tensor([0.8180, 0.7907, 0.8530], grad_fn=<SelectBackward0>)\n",
      "3.4697394 GLU7A 110m\n",
      "\n",
      "tensor([0.8435, 0.8050, 0.7230], grad_fn=<SelectBackward0>)\n",
      "6.6555824 HIS13A 110m\n",
      "\n",
      "tensor([0.9126, 0.7585, 0.8169], grad_fn=<SelectBackward0>)\n",
      "11.089525 LYS17A 110m\n",
      "\n",
      "tensor([0.7435, 0.8204, 0.8343], grad_fn=<SelectBackward0>)\n",
      "3.7254019 GLU19A 110m\n",
      "\n",
      "tensor([0.8491, 0.6267, 0.7408], grad_fn=<SelectBackward0>)\n",
      "3.1550546 ASP21A 110m\n",
      "\n",
      "tensor([0.9039, 0.9144, 0.8383], grad_fn=<SelectBackward0>)\n",
      "7.237774 HIS25A 110m\n",
      "\n",
      "tensor([0.8325, 0.9164, 0.8832], grad_fn=<SelectBackward0>)\n",
      "3.1187954 ASP28A 110m\n",
      "\n",
      "tensor([0.9557, 0.7157, 0.9277], grad_fn=<SelectBackward0>)\n",
      "11.04542 LYS35A 110m\n",
      "\n",
      "tensor([0.7739, 0.7777, 0.7055], grad_fn=<SelectBackward0>)\n",
      "6.817896 HIS37A 110m\n",
      "\n",
      "tensor([0.7770, 1.0000, 0.6724], grad_fn=<SelectBackward0>)\n",
      "3.5071077 GLU39A 110m\n",
      "\n",
      "tensor([0.9167, 0.9286, 0.8572], grad_fn=<SelectBackward0>)\n",
      "3.6179585 GLU42A 110m\n",
      "\n",
      "tensor([0.8181, 0.7235, 0.7806], grad_fn=<SelectBackward0>)\n",
      "10.546507 LYS43A 110m\n",
      "\n",
      "tensor([0.8579, 0.8783, 0.7415], grad_fn=<SelectBackward0>)\n",
      "3.1671767 ASP45A 110m\n",
      "\n",
      "tensor([0.6913, 0.6975, 0.7936], grad_fn=<SelectBackward0>)\n",
      "10.865444 LYS48A 110m\n",
      "\n",
      "tensor([0.8160, 0.6953, 0.7790], grad_fn=<SelectBackward0>)\n",
      "5.2389116 HIS49A 110m\n",
      "\n",
      "tensor([0.8847, 0.8124, 0.8755], grad_fn=<SelectBackward0>)\n",
      "10.845619 LYS51A 110m\n",
      "\n",
      "tensor([0.8397, 0.9805, 0.7386], grad_fn=<SelectBackward0>)\n",
      "3.6382923 GLU53A 110m\n",
      "\n",
      "tensor([0.9784, 0.8710, 0.8821], grad_fn=<SelectBackward0>)\n",
      "3.8231518 GLU55A 110m\n",
      "\n",
      "tensor([0.8202, 0.8501, 0.9253], grad_fn=<SelectBackward0>)\n",
      "10.860956 LYS57A 110m\n",
      "\n",
      "tensor([0.8401, 0.9979, 0.7630], grad_fn=<SelectBackward0>)\n",
      "3.7680068 GLU60A 110m\n",
      "\n",
      "tensor([0.9472, 0.8623, 0.9162], grad_fn=<SelectBackward0>)\n",
      "2.8981743 ASP61A 110m\n",
      "\n",
      "tensor([0.7872, 0.8785, 0.8372], grad_fn=<SelectBackward0>)\n",
      "11.07126 LYS63A 110m\n",
      "\n",
      "tensor([0.7899, 0.6289, 0.8139], grad_fn=<SelectBackward0>)\n",
      "11.029961 LYS64A 110m\n",
      "\n",
      "tensor([0.9187, 0.8594, 0.8604], grad_fn=<SelectBackward0>)\n",
      "6.922825 HIS65A 110m\n",
      "\n",
      "tensor([0.8426, 0.8719, 0.8801], grad_fn=<SelectBackward0>)\n",
      "11.074564 LYS78A 110m\n",
      "\n",
      "tensor([0.9682, 0.8270, 0.9163], grad_fn=<SelectBackward0>)\n",
      "10.58214 LYS79A 110m\n",
      "\n",
      "tensor([1.0000, 0.7683, 0.7993], grad_fn=<SelectBackward0>)\n",
      "10.050915 LYS80A 110m\n",
      "\n",
      "tensor([0.8344, 0.7924, 0.9835], grad_fn=<SelectBackward0>)\n",
      "7.268813 HIS82A 110m\n",
      "\n",
      "tensor([0.7703, 0.7957, 0.6956], grad_fn=<SelectBackward0>)\n",
      "7.057869 HIS83A 110m\n",
      "\n",
      "tensor([0.8725, 0.7842, 0.7848], grad_fn=<SelectBackward0>)\n",
      "3.4642859 GLU84A 110m\n",
      "\n",
      "tensor([0.9769, 0.7487, 0.6797], grad_fn=<SelectBackward0>)\n",
      "3.7665467 GLU86A 110m\n",
      "\n",
      "tensor([0.7946, 0.8752, 0.8847], grad_fn=<SelectBackward0>)\n",
      "9.386646 LYS88A 110m\n",
      "\n",
      "tensor([0.7786, 0.9344, 0.8874], grad_fn=<SelectBackward0>)\n",
      "5.9169984 HIS94A 110m\n",
      "\n",
      "tensor([1.0000, 0.7666, 0.7415], grad_fn=<SelectBackward0>)\n",
      "11.485002 LYS97A 110m\n",
      "\n",
      "tensor([0.8970, 0.8157, 0.8147], grad_fn=<SelectBackward0>)\n",
      "6.4511757 HIS98A 110m\n",
      "\n",
      "tensor([0.8837, 1.0000, 0.9406], grad_fn=<SelectBackward0>)\n",
      "11.527746 LYS99A 110m\n",
      "\n",
      "tensor([0.8572, 0.8742, 0.8075], grad_fn=<SelectBackward0>)\n",
      "11.132584 LYS103A 110m\n",
      "\n",
      "tensor([0.8060, 0.9337, 0.6727], grad_fn=<SelectBackward0>)\n",
      "10.659401 TYR104A 110m\n",
      "\n",
      "tensor([0.6812, 0.7723, 0.6704], grad_fn=<SelectBackward0>)\n",
      "3.788856 GLU106A 110m\n",
      "\n",
      "tensor([0.7456, 0.8910, 0.6926], grad_fn=<SelectBackward0>)\n",
      "3.5877771 GLU110A 110m\n",
      "\n",
      "tensor([0.9119, 0.8749, 1.0000], grad_fn=<SelectBackward0>)\n",
      "5.236329 HIS114A 110m\n",
      "\n",
      "tensor([0.7897, 0.9669, 0.8321], grad_fn=<SelectBackward0>)\n",
      "7.2655163 HIS117A 110m\n",
      "\n",
      "tensor([0.6705, 0.7793, 0.9006], grad_fn=<SelectBackward0>)\n",
      "7.2170296 HIS120A 110m\n",
      "\n",
      "tensor([0.8018, 0.8949, 0.7161], grad_fn=<SelectBackward0>)\n",
      "3.100315 ASP127A 110m\n",
      "\n",
      "tensor([0.8422, 0.7839, 0.9013], grad_fn=<SelectBackward0>)\n",
      "11.170919 LYS134A 110m\n",
      "\n",
      "tensor([0.9658, 1.0000, 0.8855], grad_fn=<SelectBackward0>)\n",
      "3.6331363 GLU137A 110m\n",
      "\n",
      "tensor([0.8395, 0.8459, 0.7751], grad_fn=<SelectBackward0>)\n",
      "11.144684 LYS141A 110m\n",
      "\n",
      "tensor([0.8038, 0.8324, 0.9285], grad_fn=<SelectBackward0>)\n",
      "3.1659484 ASP142A 110m\n",
      "\n",
      "tensor([0.7703, 0.6924, 0.7449], grad_fn=<SelectBackward0>)\n",
      "11.041063 LYS146A 110m\n",
      "\n",
      "tensor([0.7541, 0.8734, 0.9574], grad_fn=<SelectBackward0>)\n",
      "10.411631 TYR147A 110m\n",
      "\n",
      "tensor([0.7299, 0.7532, 0.8278], grad_fn=<SelectBackward0>)\n",
      "10.455433 LYS148A 110m\n",
      "\n",
      "tensor([0.9414, 0.8455, 0.7424], grad_fn=<SelectBackward0>)\n",
      "3.567802 GLU149A 110m\n",
      "\n",
      "tensor([0.9476, 0.8279, 0.9249], grad_fn=<SelectBackward0>)\n",
      "11.537949 TYR152A 110m\n",
      "\n",
      "tensor([0.8226, 0.6852, 0.8197], grad_fn=<SelectBackward0>)\n",
      "3.7470083 GLU5A 108m\n",
      "\n",
      "tensor([1.0000, 0.5888, 0.7470], grad_fn=<SelectBackward0>)\n",
      "3.1159039 GLU7A 108m\n",
      "\n",
      "tensor([0.7718, 0.8510, 0.8475], grad_fn=<SelectBackward0>)\n",
      "6.9364147 HIS13A 108m\n",
      "\n",
      "tensor([0.9446, 0.7390, 0.7097], grad_fn=<SelectBackward0>)\n",
      "10.5862465 LYS17A 108m\n",
      "\n",
      "tensor([0.8451, 0.7487, 0.9219], grad_fn=<SelectBackward0>)\n",
      "3.6549163 GLU19A 108m\n",
      "\n",
      "tensor([0.6614, 0.6232, 0.5771], grad_fn=<SelectBackward0>)\n",
      "2.857121 ASP21A 108m\n",
      "\n",
      "tensor([0.7138, 0.7334, 0.9578], grad_fn=<SelectBackward0>)\n",
      "6.8121853 HIS25A 108m\n",
      "\n",
      "tensor([0.8590, 0.8360, 0.8035], grad_fn=<SelectBackward0>)\n",
      "2.6130955 ASP28A 108m\n",
      "\n",
      "tensor([0.7290, 0.7856, 0.8658], grad_fn=<SelectBackward0>)\n",
      "11.028023 LYS35A 108m\n",
      "\n",
      "tensor([0.9185, 0.8558, 0.7521], grad_fn=<SelectBackward0>)\n",
      "6.993988 HIS37A 108m\n",
      "\n",
      "tensor([0.7887, 0.7969, 0.8628], grad_fn=<SelectBackward0>)\n",
      "3.5102572 GLU39A 108m\n",
      "\n",
      "tensor([0.8473, 0.9352, 0.7633], grad_fn=<SelectBackward0>)\n",
      "3.6432252 GLU42A 108m\n",
      "\n",
      "tensor([0.7845, 0.9179, 0.8641], grad_fn=<SelectBackward0>)\n",
      "10.519066 LYS43A 108m\n",
      "\n",
      "tensor([0.7612, 0.9439, 0.7840], grad_fn=<SelectBackward0>)\n",
      "3.1758642 ASP45A 108m\n",
      "\n",
      "tensor([0.6453, 0.9332, 0.8299], grad_fn=<SelectBackward0>)\n",
      "11.05966 LYS48A 108m\n",
      "\n",
      "tensor([0.6691, 0.7318, 0.7455], grad_fn=<SelectBackward0>)\n",
      "6.879632 HIS49A 108m\n",
      "\n",
      "tensor([0.9285, 0.7799, 0.8186], grad_fn=<SelectBackward0>)\n",
      "11.54229 LYS51A 108m\n",
      "\n",
      "tensor([0.7675, 0.7875, 0.7493], grad_fn=<SelectBackward0>)\n",
      "3.1423426 GLU53A 108m\n",
      "\n",
      "tensor([0.7923, 0.9177, 0.7151], grad_fn=<SelectBackward0>)\n",
      "3.8204675 GLU55A 108m\n",
      "\n",
      "tensor([0.7624, 0.8013, 0.7346], grad_fn=<SelectBackward0>)\n",
      "9.772831 LYS57A 108m\n",
      "\n",
      "tensor([0.8523, 0.9764, 0.7359], grad_fn=<SelectBackward0>)\n",
      "3.1592083 GLU60A 108m\n",
      "\n",
      "tensor([0.8699, 0.7787, 0.9594], grad_fn=<SelectBackward0>)\n",
      "3.1962352 ASP61A 108m\n",
      "\n",
      "tensor([0.6823, 0.8659, 0.7918], grad_fn=<SelectBackward0>)\n",
      "11.061279 LYS63A 108m\n",
      "\n",
      "tensor([0.8923, 0.7767, 0.7543], grad_fn=<SelectBackward0>)\n",
      "10.829159 LYS64A 108m\n",
      "\n",
      "tensor([0.6500, 0.7053, 0.7977], grad_fn=<SelectBackward0>)\n",
      "6.9147997 HIS65A 108m\n",
      "\n",
      "tensor([0.8053, 0.7950, 0.7371], grad_fn=<SelectBackward0>)\n",
      "10.598719 LYS78A 108m\n",
      "\n",
      "tensor([0.8480, 0.8636, 0.8162], grad_fn=<SelectBackward0>)\n",
      "11.059128 LYS79A 108m\n",
      "\n",
      "tensor([0.9189, 0.7797, 0.7822], grad_fn=<SelectBackward0>)\n",
      "11.136755 LYS80A 108m\n",
      "\n",
      "tensor([0.8644, 0.9016, 0.7090], grad_fn=<SelectBackward0>)\n",
      "6.701088 HIS82A 108m\n",
      "\n",
      "tensor([0.8505, 0.9895, 0.7907], grad_fn=<SelectBackward0>)\n",
      "6.845832 HIS83A 108m\n",
      "\n",
      "tensor([0.8155, 0.8038, 1.0000], grad_fn=<SelectBackward0>)\n",
      "3.1894536 GLU84A 108m\n",
      "\n",
      "tensor([0.7674, 0.8798, 0.7816], grad_fn=<SelectBackward0>)\n",
      "3.3631878 GLU86A 108m\n",
      "\n",
      "tensor([0.8922, 0.9070, 0.8066], grad_fn=<SelectBackward0>)\n",
      "10.606636 LYS88A 108m\n",
      "\n",
      "tensor([0.8321, 0.7915, 0.7737], grad_fn=<SelectBackward0>)\n",
      "6.8567114 HIS94A 108m\n",
      "\n",
      "tensor([0.7251, 0.8101, 0.9252], grad_fn=<SelectBackward0>)\n",
      "10.840237 LYS97A 108m\n",
      "\n",
      "tensor([0.8271, 0.6797, 0.6216], grad_fn=<SelectBackward0>)\n",
      "5.363247 HIS98A 108m\n",
      "\n",
      "tensor([0.7759, 0.7268, 0.9656], grad_fn=<SelectBackward0>)\n",
      "10.753426 LYS99A 108m\n",
      "\n",
      "tensor([0.6926, 0.9950, 0.9479], grad_fn=<SelectBackward0>)\n",
      "11.063929 LYS103A 108m\n",
      "\n",
      "tensor([0.6871, 0.9337, 0.8754], grad_fn=<SelectBackward0>)\n",
      "11.517361 TYR104A 108m\n",
      "\n",
      "tensor([0.7950, 0.7822, 0.7573], grad_fn=<SelectBackward0>)\n",
      "3.70405 GLU106A 108m\n",
      "\n",
      "tensor([0.9326, 0.8362, 0.8903], grad_fn=<SelectBackward0>)\n",
      "3.6337276 GLU110A 108m\n",
      "\n",
      "tensor([0.6673, 0.6877, 0.9079], grad_fn=<SelectBackward0>)\n",
      "6.857296 HIS114A 108m\n",
      "\n",
      "tensor([1.0000, 0.9419, 0.8477], grad_fn=<SelectBackward0>)\n",
      "7.2894382 HIS117A 108m\n",
      "\n",
      "tensor([0.8355, 0.9673, 0.8231], grad_fn=<SelectBackward0>)\n",
      "7.2562056 HIS120A 108m\n",
      "\n",
      "tensor([0.7935, 0.9705, 0.9625], grad_fn=<SelectBackward0>)\n",
      "3.1297522 ASP127A 108m\n",
      "\n",
      "tensor([0.7740, 0.7771, 0.7726], grad_fn=<SelectBackward0>)\n",
      "10.375254 LYS134A 108m\n",
      "\n",
      "tensor([1.0000, 0.8016, 0.8800], grad_fn=<SelectBackward0>)\n",
      "3.866766 GLU137A 108m\n",
      "\n",
      "tensor([0.8328, 0.9709, 0.8229], grad_fn=<SelectBackward0>)\n",
      "10.570536 LYS141A 108m\n",
      "\n",
      "tensor([0.8520, 0.9018, 0.7341], grad_fn=<SelectBackward0>)\n",
      "3.1415062 ASP142A 108m\n",
      "\n",
      "tensor([0.6157, 0.8021, 0.8377], grad_fn=<SelectBackward0>)\n",
      "10.826462 LYS146A 108m\n",
      "\n",
      "tensor([0.7686, 0.9120, 0.7520], grad_fn=<SelectBackward0>)\n",
      "6.8177395 TYR147A 108m\n",
      "\n",
      "tensor([0.8300, 0.8450, 0.7773], grad_fn=<SelectBackward0>)\n",
      "10.704538 LYS148A 108m\n",
      "\n",
      "tensor([0.7652, 0.8007, 0.7968], grad_fn=<SelectBackward0>)\n",
      "3.831386 GLU149A 108m\n",
      "\n",
      "tensor([0.7337, 0.7220, 0.8398], grad_fn=<SelectBackward0>)\n",
      "9.427689 TYR152A 108m\n",
      "\n",
      "tensor([0.8444, 0.9130, 0.7473], grad_fn=<SelectBackward0>)\n",
      "3.596292 GLU5A 109m\n",
      "\n",
      "tensor([0.6934, 0.7846, 0.7431], grad_fn=<SelectBackward0>)\n",
      "3.7988644 GLU7A 109m\n",
      "\n",
      "tensor([0.9408, 0.7374, 0.7647], grad_fn=<SelectBackward0>)\n",
      "6.9695234 HIS13A 109m\n",
      "\n",
      "tensor([0.6504, 0.7591, 0.6548], grad_fn=<SelectBackward0>)\n",
      "10.810984 LYS17A 109m\n",
      "\n",
      "tensor([0.7201, 0.6271, 0.7788], grad_fn=<SelectBackward0>)\n",
      "3.844462 GLU19A 109m\n",
      "\n",
      "tensor([0.8777, 0.8895, 0.8849], grad_fn=<SelectBackward0>)\n",
      "3.0392714 ASP21A 109m\n",
      "\n",
      "tensor([0.8723, 0.9292, 0.6607], grad_fn=<SelectBackward0>)\n",
      "6.8640203 HIS25A 109m\n",
      "\n",
      "tensor([0.9198, 0.9342, 1.0000], grad_fn=<SelectBackward0>)\n",
      "3.0088935 ASP28A 109m\n",
      "\n",
      "tensor([0.7718, 0.7783, 0.7798], grad_fn=<SelectBackward0>)\n",
      "11.010178 LYS35A 109m\n",
      "\n",
      "tensor([0.8131, 0.8378, 0.9789], grad_fn=<SelectBackward0>)\n",
      "5.2788525 HIS37A 109m\n",
      "\n",
      "tensor([0.7985, 0.7763, 0.7823], grad_fn=<SelectBackward0>)\n",
      "3.3571515 GLU39A 109m\n",
      "\n",
      "tensor([0.8235, 0.9520, 0.7372], grad_fn=<SelectBackward0>)\n",
      "3.8409104 GLU42A 109m\n",
      "\n",
      "tensor([0.9571, 0.8002, 0.6849], grad_fn=<SelectBackward0>)\n",
      "11.525496 LYS43A 109m\n",
      "\n",
      "tensor([0.8775, 0.8787, 0.9180], grad_fn=<SelectBackward0>)\n",
      "3.034336 ASP45A 109m\n",
      "\n",
      "tensor([0.8286, 0.8235, 0.6983], grad_fn=<SelectBackward0>)\n",
      "11.060585 LYS48A 109m\n",
      "\n",
      "tensor([0.8468, 0.8090, 0.9251], grad_fn=<SelectBackward0>)\n",
      "7.0370817 HIS49A 109m\n",
      "\n",
      "tensor([0.8486, 0.8282, 0.8981], grad_fn=<SelectBackward0>)\n",
      "11.5331 LYS51A 109m\n",
      "\n",
      "tensor([0.8606, 0.7133, 0.7655], grad_fn=<SelectBackward0>)\n",
      "2.979683 GLU53A 109m\n",
      "\n",
      "tensor([0.7416, 0.7756, 0.7953], grad_fn=<SelectBackward0>)\n",
      "3.6367078 GLU55A 109m\n",
      "\n",
      "tensor([0.9586, 0.8794, 0.9616], grad_fn=<SelectBackward0>)\n",
      "11.105986 LYS57A 109m\n",
      "\n",
      "tensor([0.8039, 0.8047, 0.7441], grad_fn=<SelectBackward0>)\n",
      "3.4463215 GLU60A 109m\n",
      "\n",
      "tensor([0.7966, 0.9102, 0.8675], grad_fn=<SelectBackward0>)\n",
      "3.0787172 ASP61A 109m\n",
      "\n",
      "tensor([0.6836, 0.7327, 0.7205], grad_fn=<SelectBackward0>)\n",
      "10.50543 LYS63A 109m\n",
      "\n",
      "tensor([0.9030, 0.8836, 0.6873], grad_fn=<SelectBackward0>)\n",
      "11.140703 LYS64A 109m\n",
      "\n",
      "tensor([0.8381, 0.7455, 0.9524], grad_fn=<SelectBackward0>)\n",
      "6.9413204 HIS65A 109m\n",
      "\n",
      "tensor([0.8906, 0.8499, 0.8291], grad_fn=<SelectBackward0>)\n",
      "11.534773 LYS78A 109m\n",
      "\n",
      "tensor([0.8329, 0.8926, 0.7488], grad_fn=<SelectBackward0>)\n",
      "11.075775 LYS79A 109m\n",
      "\n",
      "tensor([0.7414, 0.7785, 0.8231], grad_fn=<SelectBackward0>)\n",
      "9.913013 LYS80A 109m\n",
      "\n",
      "tensor([0.7357, 0.7160, 0.8324], grad_fn=<SelectBackward0>)\n",
      "6.9161844 HIS82A 109m\n",
      "\n",
      "tensor([0.8163, 0.7861, 0.8105], grad_fn=<SelectBackward0>)\n",
      "6.754388 HIS83A 109m\n",
      "\n",
      "tensor([0.6451, 0.8858, 0.7936], grad_fn=<SelectBackward0>)\n",
      "3.3035922 GLU84A 109m\n",
      "\n",
      "tensor([0.9583, 0.9535, 0.8003], grad_fn=<SelectBackward0>)\n",
      "3.0998826 GLU86A 109m\n",
      "\n",
      "tensor([0.7162, 0.8238, 0.9298], grad_fn=<SelectBackward0>)\n",
      "11.064159 LYS88A 109m\n",
      "\n",
      "tensor([0.8149, 0.8416, 0.8686], grad_fn=<SelectBackward0>)\n",
      "7.2867823 HIS94A 109m\n",
      "\n",
      "tensor([0.9716, 0.8099, 0.8802], grad_fn=<SelectBackward0>)\n",
      "11.143366 LYS97A 109m\n",
      "\n",
      "tensor([0.7953, 0.8261, 0.6705], grad_fn=<SelectBackward0>)\n",
      "7.2875757 HIS98A 109m\n",
      "\n",
      "tensor([0.7851, 0.9119, 0.8858], grad_fn=<SelectBackward0>)\n",
      "11.511934 LYS99A 109m\n",
      "\n",
      "tensor([0.8865, 0.8332, 0.7852], grad_fn=<SelectBackward0>)\n",
      "11.033747 LYS103A 109m\n",
      "\n",
      "tensor([0.8397, 0.7569, 0.8278], grad_fn=<SelectBackward0>)\n",
      "10.119999 TYR104A 109m\n",
      "\n",
      "tensor([0.7753, 0.8651, 0.9367], grad_fn=<SelectBackward0>)\n",
      "3.7744489 GLU106A 109m\n",
      "\n",
      "tensor([0.7528, 0.7269, 0.8134], grad_fn=<SelectBackward0>)\n",
      "3.0406017 GLU110A 109m\n",
      "\n",
      "tensor([0.6746, 0.7701, 0.6915], grad_fn=<SelectBackward0>)\n",
      "6.851609 HIS114A 109m\n",
      "\n",
      "tensor([0.7399, 0.8740, 0.7435], grad_fn=<SelectBackward0>)\n",
      "6.8224983 HIS117A 109m\n",
      "\n",
      "tensor([0.9037, 0.7731, 0.8228], grad_fn=<SelectBackward0>)\n",
      "6.8365316 HIS120A 109m\n",
      "\n",
      "tensor([0.8637, 0.8205, 0.8709], grad_fn=<SelectBackward0>)\n",
      "3.0220852 ASP127A 109m\n",
      "\n",
      "tensor([0.8626, 0.9996, 0.7896], grad_fn=<SelectBackward0>)\n",
      "10.525936 LYS134A 109m\n",
      "\n",
      "tensor([0.9225, 0.7978, 0.8781], grad_fn=<SelectBackward0>)\n",
      "3.5953565 GLU137A 109m\n",
      "\n",
      "tensor([0.7963, 0.6750, 0.6394], grad_fn=<SelectBackward0>)\n",
      "10.844465 LYS141A 109m\n",
      "\n",
      "tensor([0.9223, 0.8221, 0.7245], grad_fn=<SelectBackward0>)\n",
      "3.0339565 ASP142A 109m\n",
      "\n",
      "tensor([0.8110, 0.7953, 0.7346], grad_fn=<SelectBackward0>)\n",
      "11.542109 LYS146A 109m\n",
      "\n",
      "tensor([0.7735, 0.7921, 0.8972], grad_fn=<SelectBackward0>)\n",
      "9.557791 TYR147A 109m\n",
      "\n",
      "tensor([0.5990, 0.7254, 0.6983], grad_fn=<SelectBackward0>)\n",
      "10.417921 LYS148A 109m\n",
      "\n",
      "tensor([0.7503, 0.9175, 0.9547], grad_fn=<SelectBackward0>)\n",
      "3.6438217 GLU149A 109m\n",
      "\n",
      "tensor([0.8469, 0.8816, 0.7798], grad_fn=<SelectBackward0>)\n",
      "11.51399 TYR152A 109m\n",
      "\n",
      "tensor([0.6759, 0.8908, 0.8703], grad_fn=<SelectBackward0>)\n",
      "3.826851 GLU5A 106m\n",
      "\n",
      "tensor([0.8793, 0.7317, 0.8497], grad_fn=<SelectBackward0>)\n",
      "3.4233408 GLU7A 106m\n",
      "\n",
      "tensor([0.7439, 0.8451, 0.7482], grad_fn=<SelectBackward0>)\n",
      "6.71383 HIS13A 106m\n",
      "\n",
      "tensor([0.9157, 0.6850, 0.7615], grad_fn=<SelectBackward0>)\n",
      "10.476164 LYS17A 106m\n",
      "\n",
      "tensor([0.8645, 0.7436, 0.8672], grad_fn=<SelectBackward0>)\n",
      "3.152008 GLU19A 106m\n",
      "\n",
      "tensor([0.7674, 0.8001, 0.8793], grad_fn=<SelectBackward0>)\n",
      "3.2154722 ASP21A 106m\n",
      "\n",
      "tensor([0.8838, 0.9788, 0.7911], grad_fn=<SelectBackward0>)\n",
      "7.2875395 HIS25A 106m\n",
      "\n",
      "tensor([0.8521, 0.8860, 0.8949], grad_fn=<SelectBackward0>)\n",
      "2.9784732 ASP28A 106m\n",
      "\n",
      "tensor([0.7397, 0.8536, 0.8078], grad_fn=<SelectBackward0>)\n",
      "10.830503 LYS35A 106m\n",
      "\n",
      "tensor([0.7977, 0.8151, 0.8005], grad_fn=<SelectBackward0>)\n",
      "6.830143 HIS37A 106m\n",
      "\n",
      "tensor([0.7804, 0.8255, 0.8673], grad_fn=<SelectBackward0>)\n",
      "3.1644716 GLU39A 106m\n",
      "\n",
      "tensor([0.8080, 0.7784, 0.8995], grad_fn=<SelectBackward0>)\n",
      "3.3380833 GLU42A 106m\n",
      "\n",
      "tensor([1.0000, 0.7986, 0.7794], grad_fn=<SelectBackward0>)\n",
      "7.431266 LYS43A 106m\n",
      "\n",
      "tensor([0.7262, 0.8717, 0.8446], grad_fn=<SelectBackward0>)\n",
      "3.1740255 ASP45A 106m\n",
      "\n",
      "tensor([0.7252, 0.7448, 0.9006], grad_fn=<SelectBackward0>)\n",
      "11.037166 LYS48A 106m\n",
      "\n",
      "tensor([0.7827, 0.7050, 0.8000], grad_fn=<SelectBackward0>)\n",
      "6.878932 HIS49A 106m\n",
      "\n",
      "tensor([0.8466, 0.8080, 0.7946], grad_fn=<SelectBackward0>)\n",
      "11.072512 LYS51A 106m\n",
      "\n",
      "tensor([0.8061, 0.8460, 0.8094], grad_fn=<SelectBackward0>)\n",
      "3.5372076 GLU53A 106m\n",
      "\n",
      "tensor([0.9051, 0.9872, 0.8231], grad_fn=<SelectBackward0>)\n",
      "3.6231184 GLU55A 106m\n",
      "\n",
      "tensor([0.7523, 0.8709, 0.9283], grad_fn=<SelectBackward0>)\n",
      "11.496202 LYS57A 106m\n",
      "\n",
      "tensor([0.8273, 0.9025, 0.7745], grad_fn=<SelectBackward0>)\n",
      "3.7887192 GLU60A 106m\n",
      "\n",
      "tensor([0.8140, 0.7576, 0.7966], grad_fn=<SelectBackward0>)\n",
      "2.9747567 ASP61A 106m\n",
      "\n",
      "tensor([0.8066, 0.8085, 0.8427], grad_fn=<SelectBackward0>)\n",
      "10.5664215 LYS63A 106m\n",
      "\n",
      "tensor([0.8612, 0.7827, 0.9531], grad_fn=<SelectBackward0>)\n",
      "11.117708 LYS64A 106m\n",
      "\n",
      "tensor([0.8749, 0.9229, 0.7548], grad_fn=<SelectBackward0>)\n",
      "7.276803 HIS65A 106m\n",
      "\n",
      "tensor([0.9777, 0.9096, 0.9872], grad_fn=<SelectBackward0>)\n",
      "11.129894 LYS78A 106m\n",
      "\n",
      "tensor([0.8659, 1.0000, 0.7918], grad_fn=<SelectBackward0>)\n",
      "9.383402 LYS79A 106m\n",
      "\n",
      "tensor([0.8131, 0.7821, 0.8388], grad_fn=<SelectBackward0>)\n",
      "11.010262 LYS80A 106m\n",
      "\n",
      "tensor([0.7533, 0.9129, 0.6756], grad_fn=<SelectBackward0>)\n",
      "7.2940006 HIS82A 106m\n",
      "\n",
      "tensor([0.7284, 0.9409, 0.7280], grad_fn=<SelectBackward0>)\n",
      "6.729087 HIS83A 106m\n",
      "\n",
      "tensor([0.7897, 0.8291, 0.8527], grad_fn=<SelectBackward0>)\n",
      "3.6451993 GLU84A 106m\n",
      "\n",
      "tensor([0.7514, 0.7953, 0.9029], grad_fn=<SelectBackward0>)\n",
      "3.5886889 GLU86A 106m\n",
      "\n",
      "tensor([0.8572, 0.7722, 0.9431], grad_fn=<SelectBackward0>)\n",
      "10.612068 LYS88A 106m\n",
      "\n",
      "tensor([0.8100, 0.7861, 0.8079], grad_fn=<SelectBackward0>)\n",
      "7.27106 HIS94A 106m\n",
      "\n",
      "tensor([0.6613, 0.6863, 0.7856], grad_fn=<SelectBackward0>)\n",
      "9.554411 LYS97A 106m\n",
      "\n",
      "tensor([0.8962, 0.7634, 0.8658], grad_fn=<SelectBackward0>)\n",
      "4.18716 HIS98A 106m\n",
      "\n",
      "tensor([0.8698, 0.6970, 0.9094], grad_fn=<SelectBackward0>)\n",
      "10.935232 LYS99A 106m\n",
      "\n",
      "tensor([0.8660, 0.9403, 0.8621], grad_fn=<SelectBackward0>)\n",
      "11.088589 LYS103A 106m\n",
      "\n",
      "tensor([0.7190, 0.8276, 0.9126], grad_fn=<SelectBackward0>)\n",
      "11.132122 TYR104A 106m\n",
      "\n",
      "tensor([0.7446, 0.9112, 0.9138], grad_fn=<SelectBackward0>)\n",
      "3.7722359 GLU106A 106m\n",
      "\n",
      "tensor([0.8661, 0.8397, 0.7790], grad_fn=<SelectBackward0>)\n",
      "3.4867496 GLU110A 106m\n",
      "\n",
      "tensor([0.9373, 0.8263, 0.8652], grad_fn=<SelectBackward0>)\n",
      "6.7159367 HIS114A 106m\n",
      "\n",
      "tensor([0.7099, 0.8183, 0.8116], grad_fn=<SelectBackward0>)\n",
      "7.2386656 HIS117A 106m\n",
      "\n",
      "tensor([0.7411, 0.7267, 0.8701], grad_fn=<SelectBackward0>)\n",
      "7.025485 HIS120A 106m\n",
      "\n",
      "tensor([0.8343, 0.7473, 0.7771], grad_fn=<SelectBackward0>)\n",
      "3.0085835 ASP127A 106m\n",
      "\n",
      "tensor([0.9103, 0.8027, 0.8053], grad_fn=<SelectBackward0>)\n",
      "11.086517 LYS134A 106m\n",
      "\n",
      "tensor([0.7369, 0.6930, 0.7247], grad_fn=<SelectBackward0>)\n",
      "3.7228107 GLU137A 106m\n",
      "\n",
      "tensor([0.8073, 0.7180, 0.8261], grad_fn=<SelectBackward0>)\n",
      "11.538071 LYS141A 106m\n",
      "\n",
      "tensor([0.8423, 0.7702, 0.8741], grad_fn=<SelectBackward0>)\n",
      "2.9925327 ASP142A 106m\n",
      "\n",
      "tensor([0.8748, 0.8859, 0.7406], grad_fn=<SelectBackward0>)\n",
      "11.109703 LYS146A 106m\n",
      "\n",
      "tensor([0.8328, 0.7752, 0.8347], grad_fn=<SelectBackward0>)\n",
      "9.537434 TYR147A 106m\n",
      "\n",
      "tensor([0.7548, 0.7411, 0.9032], grad_fn=<SelectBackward0>)\n",
      "10.719271 LYS148A 106m\n",
      "\n",
      "tensor([0.8584, 1.0000, 0.7893], grad_fn=<SelectBackward0>)\n",
      "3.5485926 GLU149A 106m\n",
      "\n",
      "tensor([0.7442, 0.8622, 0.8326], grad_fn=<SelectBackward0>)\n",
      "11.534281 TYR152A 106m\n",
      "\n",
      "tensor([0.9237, 0.8224, 1.0000], grad_fn=<SelectBackward0>)\n",
      "3.7143803 GLU5A 103m\n",
      "\n",
      "tensor([0.8783, 0.8476, 0.8935], grad_fn=<SelectBackward0>)\n",
      "3.7721887 GLU7A 103m\n",
      "\n",
      "tensor([0.8249, 0.7861, 0.7077], grad_fn=<SelectBackward0>)\n",
      "7.0519733 HIS13A 103m\n",
      "\n",
      "tensor([0.8058, 0.7070, 0.7767], grad_fn=<SelectBackward0>)\n",
      "11.14341 LYS17A 103m\n",
      "\n",
      "tensor([0.7805, 1.0000, 0.7992], grad_fn=<SelectBackward0>)\n",
      "3.745436 GLU19A 103m\n",
      "\n",
      "tensor([0.7697, 0.9221, 0.8589], grad_fn=<SelectBackward0>)\n",
      "3.0960941 ASP21A 103m\n",
      "\n",
      "tensor([0.8744, 0.8132, 0.7572], grad_fn=<SelectBackward0>)\n",
      "6.9458075 HIS25A 103m\n",
      "\n",
      "tensor([0.7920, 0.7393, 0.7806], grad_fn=<SelectBackward0>)\n",
      "2.9262505 ASP28A 103m\n",
      "\n",
      "tensor([0.8067, 0.7930, 0.7096], grad_fn=<SelectBackward0>)\n",
      "11.067547 LYS35A 103m\n",
      "\n",
      "tensor([0.8940, 0.9210, 0.7858], grad_fn=<SelectBackward0>)\n",
      "5.2819676 HIS37A 103m\n",
      "\n",
      "tensor([0.7256, 0.7596, 0.9757], grad_fn=<SelectBackward0>)\n",
      "3.815134 GLU39A 103m\n",
      "\n",
      "tensor([0.7386, 0.6663, 1.0000], grad_fn=<SelectBackward0>)\n",
      "3.7251363 GLU42A 103m\n",
      "\n",
      "tensor([0.7824, 0.8724, 0.7551], grad_fn=<SelectBackward0>)\n",
      "10.849716 LYS43A 103m\n",
      "\n",
      "tensor([0.8525, 0.9632, 0.9270], grad_fn=<SelectBackward0>)\n",
      "2.900919 ASP45A 103m\n",
      "\n",
      "tensor([0.7088, 0.8354, 0.7393], grad_fn=<SelectBackward0>)\n",
      "11.098206 LYS48A 103m\n",
      "\n",
      "tensor([0.7877, 0.7931, 0.9014], grad_fn=<SelectBackward0>)\n",
      "6.7382803 HIS49A 103m\n",
      "\n",
      "tensor([0.7472, 0.8309, 0.8558], grad_fn=<SelectBackward0>)\n",
      "11.04669 LYS51A 103m\n",
      "\n",
      "tensor([0.8147, 0.8904, 0.8644], grad_fn=<SelectBackward0>)\n",
      "3.1309319 GLU53A 103m\n",
      "\n",
      "tensor([0.8938, 0.8312, 0.9559], grad_fn=<SelectBackward0>)\n",
      "3.5880408 GLU55A 103m\n",
      "\n",
      "tensor([0.7609, 0.8538, 0.8095], grad_fn=<SelectBackward0>)\n",
      "11.488529 LYS57A 103m\n",
      "\n",
      "tensor([0.8504, 0.8406, 0.7912], grad_fn=<SelectBackward0>)\n",
      "3.5898595 GLU60A 103m\n",
      "\n",
      "tensor([0.8471, 1.0000, 0.8152], grad_fn=<SelectBackward0>)\n",
      "3.0927715 ASP61A 103m\n",
      "\n",
      "tensor([0.8515, 0.8120, 0.8390], grad_fn=<SelectBackward0>)\n",
      "11.149088 LYS63A 103m\n",
      "\n",
      "tensor([0.7499, 0.9575, 0.7706], grad_fn=<SelectBackward0>)\n",
      "10.812645 LYS64A 103m\n",
      "\n",
      "tensor([0.8659, 0.8191, 0.7082], grad_fn=<SelectBackward0>)\n",
      "10.07234 LYS78A 103m\n",
      "\n",
      "tensor([0.6748, 0.8207, 0.6834], grad_fn=<SelectBackward0>)\n",
      "10.413042 LYS79A 103m\n",
      "\n",
      "tensor([0.6873, 0.7535, 0.8040], grad_fn=<SelectBackward0>)\n",
      "10.820854 LYS80A 103m\n",
      "\n",
      "tensor([0.7408, 0.7652, 0.8235], grad_fn=<SelectBackward0>)\n",
      "5.2051744 HIS82A 103m\n",
      "\n",
      "tensor([0.6323, 0.7521, 0.6245], grad_fn=<SelectBackward0>)\n",
      "7.0448084 HIS83A 103m\n",
      "\n",
      "tensor([0.7610, 0.8406, 0.9207], grad_fn=<SelectBackward0>)\n",
      "3.0860538 GLU84A 103m\n",
      "\n",
      "tensor([0.8087, 0.7674, 0.7443], grad_fn=<SelectBackward0>)\n",
      "3.8186426 GLU86A 103m\n",
      "\n",
      "tensor([0.9370, 0.7215, 0.9448], grad_fn=<SelectBackward0>)\n",
      "10.309826 LYS88A 103m\n",
      "\n",
      "tensor([0.8435, 0.7877, 0.7457], grad_fn=<SelectBackward0>)\n",
      "6.907363 HIS94A 103m\n",
      "\n",
      "tensor([0.9440, 0.9290, 0.9610], grad_fn=<SelectBackward0>)\n",
      "11.533304 LYS97A 103m\n",
      "\n",
      "tensor([0.9065, 0.7788, 0.7715], grad_fn=<SelectBackward0>)\n",
      "7.0523186 HIS98A 103m\n",
      "\n",
      "tensor([0.7863, 0.7891, 0.8129], grad_fn=<SelectBackward0>)\n",
      "10.53747 LYS99A 103m\n",
      "\n",
      "tensor([0.7854, 0.8055, 0.8652], grad_fn=<SelectBackward0>)\n",
      "10.050272 LYS103A 103m\n",
      "\n",
      "tensor([0.7579, 0.8219, 0.8038], grad_fn=<SelectBackward0>)\n",
      "10.099958 TYR104A 103m\n",
      "\n",
      "tensor([0.8837, 0.8873, 0.8399], grad_fn=<SelectBackward0>)\n",
      "3.7596107 GLU106A 103m\n",
      "\n",
      "tensor([0.7863, 0.7794, 0.7952], grad_fn=<SelectBackward0>)\n",
      "3.5935678 GLU110A 103m\n",
      "\n",
      "tensor([1.0000, 0.8914, 0.8516], grad_fn=<SelectBackward0>)\n",
      "7.2665453 HIS114A 103m\n",
      "\n",
      "tensor([0.9951, 0.8044, 0.8389], grad_fn=<SelectBackward0>)\n",
      "7.2631483 HIS117A 103m\n",
      "\n",
      "tensor([0.6142, 0.9921, 0.7155], grad_fn=<SelectBackward0>)\n",
      "7.2693844 HIS120A 103m\n",
      "\n",
      "tensor([0.8261, 0.9056, 0.8268], grad_fn=<SelectBackward0>)\n",
      "2.9550042 ASP127A 103m\n",
      "\n",
      "tensor([0.7483, 0.8873, 0.7627], grad_fn=<SelectBackward0>)\n",
      "10.815258 LYS134A 103m\n",
      "\n",
      "tensor([0.8665, 0.7726, 0.6993], grad_fn=<SelectBackward0>)\n",
      "3.0298944 GLU137A 103m\n",
      "\n",
      "tensor([0.7574, 0.9298, 0.8855], grad_fn=<SelectBackward0>)\n",
      "10.128735 LYS141A 103m\n",
      "\n",
      "tensor([0.8034, 0.7663, 0.7592], grad_fn=<SelectBackward0>)\n",
      "3.1517978 ASP142A 103m\n",
      "\n",
      "tensor([0.6718, 0.7345, 0.8001], grad_fn=<SelectBackward0>)\n",
      "10.855991 LYS146A 103m\n",
      "\n",
      "tensor([0.7147, 0.7467, 0.8705], grad_fn=<SelectBackward0>)\n",
      "10.543814 TYR147A 103m\n",
      "\n",
      "tensor([0.7887, 0.6916, 0.7551], grad_fn=<SelectBackward0>)\n",
      "10.537895 LYS148A 103m\n",
      "\n",
      "tensor([0.6385, 0.7768, 0.8157], grad_fn=<SelectBackward0>)\n",
      "3.8440285 GLU149A 103m\n",
      "\n",
      "tensor([0.6663, 0.7181, 0.8044], grad_fn=<SelectBackward0>)\n",
      "11.54124 TYR152A 103m\n",
      "\n",
      "tensor([0.7609, 0.7278, 0.8029], grad_fn=<SelectBackward0>)\n",
      "3.7500973 GLU5A 107m\n",
      "\n",
      "tensor([1.0000, 0.9111, 0.7680], grad_fn=<SelectBackward0>)\n",
      "3.5574427 GLU7A 107m\n",
      "\n",
      "tensor([0.8178, 0.7841, 0.8795], grad_fn=<SelectBackward0>)\n",
      "5.0301104 HIS13A 107m\n",
      "\n",
      "tensor([0.8736, 0.7834, 0.9707], grad_fn=<SelectBackward0>)\n",
      "11.05756 LYS17A 107m\n",
      "\n",
      "tensor([0.8845, 0.8517, 0.8474], grad_fn=<SelectBackward0>)\n",
      "3.8289022 GLU19A 107m\n",
      "\n",
      "tensor([0.7808, 0.8049, 0.8755], grad_fn=<SelectBackward0>)\n",
      "2.9853354 ASP21A 107m\n",
      "\n",
      "tensor([0.7901, 0.8557, 0.7359], grad_fn=<SelectBackward0>)\n",
      "7.014166 HIS25A 107m\n",
      "\n",
      "tensor([0.8855, 0.8091, 0.8661], grad_fn=<SelectBackward0>)\n",
      "3.0171652 ASP28A 107m\n",
      "\n",
      "tensor([0.8500, 0.7409, 0.8767], grad_fn=<SelectBackward0>)\n",
      "10.031162 LYS35A 107m\n",
      "\n",
      "tensor([0.7732, 0.7847, 0.8764], grad_fn=<SelectBackward0>)\n",
      "6.976845 HIS37A 107m\n",
      "\n",
      "tensor([0.8565, 0.8951, 0.8467], grad_fn=<SelectBackward0>)\n",
      "3.5126138 GLU39A 107m\n",
      "\n",
      "tensor([0.8694, 1.0000, 0.7414], grad_fn=<SelectBackward0>)\n",
      "3.3187623 GLU42A 107m\n",
      "\n",
      "tensor([0.7508, 0.8311, 0.7022], grad_fn=<SelectBackward0>)\n",
      "11.0648575 LYS43A 107m\n",
      "\n",
      "tensor([0.8762, 0.7527, 0.7667], grad_fn=<SelectBackward0>)\n",
      "2.902503 ASP45A 107m\n",
      "\n",
      "tensor([0.8725, 0.8499, 0.8674], grad_fn=<SelectBackward0>)\n",
      "11.509851 LYS48A 107m\n",
      "\n",
      "tensor([0.9780, 0.8737, 0.8338], grad_fn=<SelectBackward0>)\n",
      "6.9516077 HIS49A 107m\n",
      "\n",
      "tensor([0.8050, 0.8397, 0.8997], grad_fn=<SelectBackward0>)\n",
      "10.831831 LYS51A 107m\n",
      "\n",
      "tensor([0.9819, 0.8688, 0.8684], grad_fn=<SelectBackward0>)\n",
      "3.735598 GLU53A 107m\n",
      "\n",
      "tensor([0.8235, 0.8489, 0.7235], grad_fn=<SelectBackward0>)\n",
      "3.8295584 GLU55A 107m\n",
      "\n",
      "tensor([1.0000, 0.7247, 0.7812], grad_fn=<SelectBackward0>)\n",
      "11.496176 LYS57A 107m\n",
      "\n",
      "tensor([0.7348, 0.9105, 0.8046], grad_fn=<SelectBackward0>)\n",
      "3.5425606 GLU60A 107m\n",
      "\n",
      "tensor([0.8516, 0.9008, 0.7621], grad_fn=<SelectBackward0>)\n",
      "2.855379 ASP61A 107m\n",
      "\n",
      "tensor([0.6984, 1.0000, 0.7921], grad_fn=<SelectBackward0>)\n",
      "11.542818 LYS63A 107m\n",
      "\n",
      "tensor([0.7539, 0.8096, 0.7043], grad_fn=<SelectBackward0>)\n",
      "11.097534 LYS64A 107m\n",
      "\n",
      "tensor([0.7452, 0.7286, 0.9252], grad_fn=<SelectBackward0>)\n",
      "6.864333 HIS65A 107m\n",
      "\n",
      "tensor([0.9390, 0.8267, 0.8539], grad_fn=<SelectBackward0>)\n",
      "9.538746 LYS78A 107m\n",
      "\n",
      "tensor([0.8035, 0.7822, 0.8889], grad_fn=<SelectBackward0>)\n",
      "11.509186 LYS79A 107m\n",
      "\n",
      "tensor([0.8701, 0.9354, 0.8497], grad_fn=<SelectBackward0>)\n",
      "10.826206 LYS80A 107m\n",
      "\n",
      "tensor([0.7452, 0.7576, 0.8255], grad_fn=<SelectBackward0>)\n",
      "6.883231 HIS82A 107m\n",
      "\n",
      "tensor([0.9439, 0.8579, 0.8369], grad_fn=<SelectBackward0>)\n",
      "6.890538 HIS83A 107m\n",
      "\n",
      "tensor([0.7318, 0.7077, 0.7842], grad_fn=<SelectBackward0>)\n",
      "3.3946543 GLU84A 107m\n",
      "\n",
      "tensor([0.8740, 0.6552, 0.8728], grad_fn=<SelectBackward0>)\n",
      "3.5717654 GLU86A 107m\n",
      "\n",
      "tensor([0.7883, 0.8842, 0.8749], grad_fn=<SelectBackward0>)\n",
      "10.617229 LYS88A 107m\n",
      "\n",
      "tensor([0.7667, 0.7407, 0.8124], grad_fn=<SelectBackward0>)\n",
      "7.015088 HIS94A 107m\n",
      "\n",
      "tensor([0.7673, 0.8021, 0.8076], grad_fn=<SelectBackward0>)\n",
      "11.030714 LYS97A 107m\n",
      "\n",
      "tensor([0.9540, 0.8282, 0.6645], grad_fn=<SelectBackward0>)\n",
      "6.53646 HIS98A 107m\n",
      "\n",
      "tensor([0.7715, 0.8806, 0.8143], grad_fn=<SelectBackward0>)\n",
      "11.52688 LYS99A 107m\n",
      "\n",
      "tensor([0.7424, 0.9116, 0.9342], grad_fn=<SelectBackward0>)\n",
      "11.064394 LYS103A 107m\n",
      "\n",
      "tensor([0.7598, 0.8883, 0.7852], grad_fn=<SelectBackward0>)\n",
      "10.863873 TYR104A 107m\n",
      "\n",
      "tensor([0.7366, 0.8721, 0.8254], grad_fn=<SelectBackward0>)\n",
      "3.6320167 GLU106A 107m\n",
      "\n",
      "tensor([0.9149, 0.8385, 0.8421], grad_fn=<SelectBackward0>)\n",
      "3.2142415 GLU110A 107m\n",
      "\n",
      "tensor([0.8671, 1.0000, 0.6928], grad_fn=<SelectBackward0>)\n",
      "5.169111 HIS114A 107m\n",
      "\n",
      "tensor([0.8498, 1.0000, 0.7625], grad_fn=<SelectBackward0>)\n",
      "5.923809 HIS117A 107m\n",
      "\n",
      "tensor([0.8455, 0.6627, 0.9918], grad_fn=<SelectBackward0>)\n",
      "7.276535 HIS120A 107m\n",
      "\n",
      "tensor([0.8571, 0.7783, 0.7649], grad_fn=<SelectBackward0>)\n",
      "3.176929 ASP127A 107m\n",
      "\n",
      "tensor([0.8989, 0.8244, 0.7445], grad_fn=<SelectBackward0>)\n",
      "11.514965 LYS134A 107m\n",
      "\n",
      "tensor([0.7955, 0.7963, 0.7077], grad_fn=<SelectBackward0>)\n",
      "3.5841331 GLU137A 107m\n",
      "\n",
      "tensor([0.9035, 0.7734, 0.8580], grad_fn=<SelectBackward0>)\n",
      "11.064512 LYS141A 107m\n",
      "\n",
      "tensor([0.8281, 0.7667, 0.8396], grad_fn=<SelectBackward0>)\n",
      "3.1618323 ASP142A 107m\n",
      "\n",
      "tensor([0.8036, 0.9048, 0.7982], grad_fn=<SelectBackward0>)\n",
      "11.521181 LYS146A 107m\n",
      "\n",
      "tensor([0.7907, 0.7432, 0.6883], grad_fn=<SelectBackward0>)\n",
      "10.540857 TYR147A 107m\n",
      "\n",
      "tensor([0.8440, 0.8717, 0.7550], grad_fn=<SelectBackward0>)\n",
      "10.587133 LYS148A 107m\n",
      "\n",
      "tensor([0.9169, 0.7701, 0.7709], grad_fn=<SelectBackward0>)\n",
      "3.5894756 GLU149A 107m\n",
      "\n",
      "tensor([0.9133, 0.8143, 0.8030], grad_fn=<SelectBackward0>)\n",
      "11.530329 TYR152A 107m\n",
      "\n",
      "tensor([0.8967, 0.9379, 0.6950], grad_fn=<SelectBackward0>)\n",
      "3.1584778 GLU5A 103l\n",
      "\n",
      "tensor([0.6223, 0.8046, 0.7924], grad_fn=<SelectBackward0>)\n",
      "3.0815673 ASP10A 103l\n",
      "\n",
      "tensor([0.8670, 0.8926, 0.9269], grad_fn=<SelectBackward0>)\n",
      "3.7371397 GLU11A 103l\n",
      "\n",
      "tensor([0.8393, 0.8201, 0.7331], grad_fn=<SelectBackward0>)\n",
      "11.51543 LYS16A 103l\n",
      "\n",
      "tensor([0.7452, 0.8849, 0.8998], grad_fn=<SelectBackward0>)\n",
      "10.442263 TYR18A 103l\n",
      "\n",
      "tensor([0.8935, 0.7337, 0.6355], grad_fn=<SelectBackward0>)\n",
      "11.123653 LYS19A 103l\n",
      "\n",
      "tensor([0.8118, 0.8215, 0.9303], grad_fn=<SelectBackward0>)\n",
      "2.9282193 ASP20A 103l\n",
      "\n",
      "tensor([0.9166, 0.8365, 0.7981], grad_fn=<SelectBackward0>)\n",
      "3.6379242 GLU22A 103l\n",
      "\n",
      "tensor([0.6820, 0.8658, 0.8218], grad_fn=<SelectBackward0>)\n",
      "11.492832 TYR24A 103l\n",
      "\n",
      "tensor([0.8025, 0.9831, 0.8060], grad_fn=<SelectBackward0>)\n",
      "10.870676 TYR25A 103l\n",
      "\n",
      "tensor([0.7589, 0.8367, 0.6712], grad_fn=<SelectBackward0>)\n",
      "6.7069902 HIS31A 103l\n",
      "\n",
      "tensor([0.8220, 0.8132, 0.7930], grad_fn=<SelectBackward0>)\n",
      "10.554617 LYS35A 103l\n",
      "\n",
      "tensor([0.8916, 0.7642, 0.7409], grad_fn=<SelectBackward0>)\n",
      "3.1545272 ASP43CA 103l\n",
      "\n",
      "tensor([0.8194, 0.8884, 0.8127], grad_fn=<SelectBackward0>)\n",
      "10.851801 LYS46A 103l\n",
      "\n",
      "tensor([0.7969, 0.7331, 0.8155], grad_fn=<SelectBackward0>)\n",
      "3.6752715 GLU48A 103l\n",
      "\n",
      "tensor([0.8282, 0.7790, 0.7892], grad_fn=<SelectBackward0>)\n",
      "3.196638 ASP50A 103l\n",
      "\n",
      "tensor([0.8878, 0.9228, 0.8627], grad_fn=<SelectBackward0>)\n",
      "10.818626 LYS51A 103l\n",
      "\n",
      "tensor([0.8103, 0.8831, 0.8360], grad_fn=<SelectBackward0>)\n",
      "11.022018 LYS63A 103l\n",
      "\n",
      "tensor([0.7378, 0.9902, 0.7414], grad_fn=<SelectBackward0>)\n",
      "3.1584187 ASP64A 103l\n",
      "\n",
      "tensor([0.7315, 0.7068, 0.7004], grad_fn=<SelectBackward0>)\n",
      "3.5939314 GLU65A 103l\n",
      "\n",
      "tensor([0.7885, 0.6692, 0.8358], grad_fn=<SelectBackward0>)\n",
      "3.6272812 GLU67A 103l\n",
      "\n",
      "tensor([0.7756, 0.8175, 0.7307], grad_fn=<SelectBackward0>)\n",
      "11.132722 LYS68A 103l\n",
      "\n",
      "tensor([0.7690, 0.7655, 0.8282], grad_fn=<SelectBackward0>)\n",
      "2.9805613 ASP73A 103l\n",
      "\n",
      "tensor([0.8063, 0.8243, 0.7521], grad_fn=<SelectBackward0>)\n",
      "3.0262547 ASP75A 103l\n",
      "\n",
      "tensor([0.7992, 0.8691, 0.7367], grad_fn=<SelectBackward0>)\n",
      "6.945857 LYS86A 103l\n",
      "\n",
      "tensor([0.7296, 0.7141, 0.6071], grad_fn=<SelectBackward0>)\n",
      "10.142826 LYS88A 103l\n",
      "\n",
      "tensor([0.8372, 0.7740, 0.8057], grad_fn=<SelectBackward0>)\n",
      "10.843548 TYR91A 103l\n",
      "\n",
      "tensor([0.9076, 0.9297, 0.9316], grad_fn=<SelectBackward0>)\n",
      "3.133501 ASP92A 103l\n",
      "\n",
      "tensor([0.7907, 0.7903, 0.8740], grad_fn=<SelectBackward0>)\n",
      "3.1829338 ASP95A 103l\n",
      "\n",
      "tensor([1.0000, 0.8160, 0.7704], grad_fn=<SelectBackward0>)\n",
      "3.8034115 GLU111A 103l\n",
      "\n",
      "tensor([0.9134, 0.9792, 0.8273], grad_fn=<SelectBackward0>)\n",
      "10.864815 LYS127A 103l\n",
      "\n",
      "tensor([0.9660, 0.9320, 0.8363], grad_fn=<SelectBackward0>)\n",
      "3.137196 ASP130A 103l\n",
      "\n",
      "tensor([0.9453, 0.8947, 0.8397], grad_fn=<SelectBackward0>)\n",
      "3.7090192 GLU131A 103l\n",
      "\n",
      "tensor([0.9074, 1.0000, 0.8560], grad_fn=<SelectBackward0>)\n",
      "11.093437 LYS138A 103l\n",
      "\n",
      "tensor([0.9032, 0.8561, 0.8902], grad_fn=<SelectBackward0>)\n",
      "10.833409 TYR142A 103l\n",
      "\n",
      "tensor([0.9207, 0.8227, 0.6400], grad_fn=<SelectBackward0>)\n",
      "11.035444 LYS150A 103l\n",
      "\n",
      "tensor([0.7007, 0.7842, 0.9347], grad_fn=<SelectBackward0>)\n",
      "3.1345348 ASP162A 103l\n",
      "\n",
      "tensor([0.8016, 0.8398, 0.7580], grad_fn=<SelectBackward0>)\n",
      "10.562914 TYR164A 103l\n",
      "\n",
      "tensor([0.6782, 0.7078, 0.9728], grad_fn=<SelectBackward0>)\n",
      "9.424999 LYS165A 103l\n",
      "\n",
      "tensor([0.6707, 0.7522, 0.8487], grad_fn=<SelectBackward0>)\n",
      "3.689021 GLU5A 102m\n",
      "\n",
      "tensor([0.8762, 0.7678, 0.9774], grad_fn=<SelectBackward0>)\n",
      "3.5832658 GLU7A 102m\n",
      "\n",
      "tensor([0.7280, 0.7925, 0.7421], grad_fn=<SelectBackward0>)\n",
      "6.8963356 HIS13A 102m\n",
      "\n",
      "tensor([0.7411, 0.8286, 0.6731], grad_fn=<SelectBackward0>)\n",
      "9.837536 LYS17A 102m\n",
      "\n",
      "tensor([0.9224, 0.8425, 0.8097], grad_fn=<SelectBackward0>)\n",
      "3.7754622 GLU19A 102m\n",
      "\n",
      "tensor([0.7828, 0.7303, 0.8546], grad_fn=<SelectBackward0>)\n",
      "3.1128426 ASP21A 102m\n",
      "\n",
      "tensor([0.8842, 0.8962, 0.7954], grad_fn=<SelectBackward0>)\n",
      "6.9972105 HIS25A 102m\n",
      "\n",
      "tensor([0.8831, 0.7498, 0.8702], grad_fn=<SelectBackward0>)\n",
      "3.011239 ASP28A 102m\n",
      "\n",
      "tensor([0.8048, 0.8411, 0.8985], grad_fn=<SelectBackward0>)\n",
      "11.5392685 LYS35A 102m\n",
      "\n",
      "tensor([0.9832, 0.9171, 0.9352], grad_fn=<SelectBackward0>)\n",
      "6.872407 HIS37A 102m\n",
      "\n",
      "tensor([0.9353, 0.8612, 0.8091], grad_fn=<SelectBackward0>)\n",
      "3.3274221 GLU39A 102m\n",
      "\n",
      "tensor([0.7607, 0.7778, 0.8713], grad_fn=<SelectBackward0>)\n",
      "3.2118745 GLU42A 102m\n",
      "\n",
      "tensor([0.7683, 0.7887, 0.7463], grad_fn=<SelectBackward0>)\n",
      "11.066639 LYS43A 102m\n",
      "\n",
      "tensor([0.7312, 0.9139, 1.0000], grad_fn=<SelectBackward0>)\n",
      "3.0362139 ASP45A 102m\n",
      "\n",
      "tensor([0.9324, 0.9339, 0.8294], grad_fn=<SelectBackward0>)\n",
      "11.060719 LYS48A 102m\n",
      "\n",
      "tensor([0.8093, 0.9151, 0.7104], grad_fn=<SelectBackward0>)\n",
      "7.139683 HIS49A 102m\n",
      "\n",
      "tensor([0.7989, 0.8244, 0.9822], grad_fn=<SelectBackward0>)\n",
      "11.514101 LYS51A 102m\n",
      "\n",
      "tensor([0.9515, 0.8186, 0.8372], grad_fn=<SelectBackward0>)\n",
      "3.5081472 GLU53A 102m\n",
      "\n",
      "tensor([0.7606, 0.7550, 0.7544], grad_fn=<SelectBackward0>)\n",
      "3.5516384 GLU55A 102m\n",
      "\n",
      "tensor([0.7993, 0.8383, 0.8982], grad_fn=<SelectBackward0>)\n",
      "11.097723 LYS57A 102m\n",
      "\n",
      "tensor([0.7430, 0.6773, 0.6980], grad_fn=<SelectBackward0>)\n",
      "3.6355124 GLU60A 102m\n",
      "\n",
      "tensor([0.8534, 0.9520, 0.8573], grad_fn=<SelectBackward0>)\n",
      "2.999414 ASP61A 102m\n",
      "\n",
      "tensor([0.7885, 0.9416, 0.9909], grad_fn=<SelectBackward0>)\n",
      "10.869415 LYS63A 102m\n",
      "\n",
      "tensor([0.6833, 0.7854, 0.7408], grad_fn=<SelectBackward0>)\n",
      "10.710293 LYS64A 102m\n",
      "\n",
      "tensor([0.6330, 0.7433, 0.7574], grad_fn=<SelectBackward0>)\n",
      "9.415943 LYS78A 102m\n",
      "\n",
      "tensor([0.9761, 0.8135, 0.7543], grad_fn=<SelectBackward0>)\n",
      "10.844267 LYS79A 102m\n",
      "\n",
      "tensor([0.8541, 0.8022, 0.8681], grad_fn=<SelectBackward0>)\n",
      "11.063015 LYS80A 102m\n",
      "\n",
      "tensor([0.8291, 0.8499, 0.8361], grad_fn=<SelectBackward0>)\n",
      "6.9735518 HIS82A 102m\n",
      "\n",
      "tensor([0.9384, 0.8597, 0.6880], grad_fn=<SelectBackward0>)\n",
      "6.884178 HIS83A 102m\n",
      "\n",
      "tensor([0.8878, 0.9625, 0.8582], grad_fn=<SelectBackward0>)\n",
      "3.616631 GLU84A 102m\n",
      "\n",
      "tensor([0.9824, 0.8542, 0.8363], grad_fn=<SelectBackward0>)\n",
      "3.8023605 GLU86A 102m\n",
      "\n",
      "tensor([0.8207, 0.7141, 1.0000], grad_fn=<SelectBackward0>)\n",
      "11.0641575 LYS88A 102m\n",
      "\n",
      "tensor([0.9058, 0.9643, 0.8223], grad_fn=<SelectBackward0>)\n",
      "6.718027 HIS94A 102m\n",
      "\n",
      "tensor([1.0000, 0.9028, 0.7400], grad_fn=<SelectBackward0>)\n",
      "11.144768 LYS97A 102m\n",
      "\n",
      "tensor([0.9771, 0.8769, 0.8618], grad_fn=<SelectBackward0>)\n",
      "7.2871904 HIS98A 102m\n",
      "\n",
      "tensor([0.8157, 0.7494, 0.8017], grad_fn=<SelectBackward0>)\n",
      "11.095961 LYS99A 102m\n",
      "\n",
      "tensor([0.7047, 0.6814, 0.7423], grad_fn=<SelectBackward0>)\n",
      "10.842415 LYS103A 102m\n",
      "\n",
      "tensor([0.7352, 0.9949, 0.9019], grad_fn=<SelectBackward0>)\n",
      "10.837112 TYR104A 102m\n",
      "\n",
      "tensor([0.7557, 0.9846, 0.6999], grad_fn=<SelectBackward0>)\n",
      "3.2070642 GLU106A 102m\n",
      "\n",
      "tensor([0.6811, 0.9149, 0.7336], grad_fn=<SelectBackward0>)\n",
      "3.7517242 GLU110A 102m\n",
      "\n",
      "tensor([0.8725, 0.7013, 0.7491], grad_fn=<SelectBackward0>)\n",
      "7.05075 HIS114A 102m\n",
      "\n",
      "tensor([0.7875, 0.7504, 0.7567], grad_fn=<SelectBackward0>)\n",
      "6.826911 HIS117A 102m\n",
      "\n",
      "tensor([1.0000, 0.7044, 0.7348], grad_fn=<SelectBackward0>)\n",
      "5.9083304 HIS120A 102m\n",
      "\n",
      "tensor([0.7795, 0.8171, 0.9222], grad_fn=<SelectBackward0>)\n",
      "3.0158834 ASP127A 102m\n",
      "\n",
      "tensor([0.9174, 0.8076, 0.7641], grad_fn=<SelectBackward0>)\n",
      "10.042362 LYS134A 102m\n",
      "\n",
      "tensor([0.7759, 0.8619, 0.8794], grad_fn=<SelectBackward0>)\n",
      "3.6510353 GLU137A 102m\n",
      "\n",
      "tensor([0.9777, 0.7299, 0.8624], grad_fn=<SelectBackward0>)\n",
      "11.070425 LYS141A 102m\n",
      "\n",
      "tensor([0.8591, 0.9329, 0.8495], grad_fn=<SelectBackward0>)\n",
      "3.1163454 ASP142A 102m\n",
      "\n",
      "tensor([0.8087, 0.9898, 0.7870], grad_fn=<SelectBackward0>)\n",
      "10.866222 LYS146A 102m\n",
      "\n",
      "tensor([0.8097, 1.0000, 0.7626], grad_fn=<SelectBackward0>)\n",
      "10.648579 TYR147A 102m\n",
      "\n",
      "tensor([0.7656, 0.7590, 0.8055], grad_fn=<SelectBackward0>)\n",
      "11.5378 LYS148A 102m\n",
      "\n",
      "tensor([0.8499, 0.8926, 0.8170], grad_fn=<SelectBackward0>)\n",
      "2.9976687 GLU149A 102m\n",
      "\n",
      "tensor([0.6850, 0.6573, 0.7916], grad_fn=<SelectBackward0>)\n",
      "10.714626 TYR152A 102m\n",
      "\n",
      "tensor([0.8637, 0.8734, 0.8340], grad_fn=<SelectBackward0>)\n",
      "3.7027183 GLU5A 102l\n",
      "\n",
      "tensor([0.7608, 0.7339, 0.8940], grad_fn=<SelectBackward0>)\n",
      "3.1731668 ASP10A 102l\n",
      "\n",
      "tensor([0.7537, 0.8884, 0.7902], grad_fn=<SelectBackward0>)\n",
      "3.7422404 GLU11A 102l\n",
      "\n",
      "tensor([0.6946, 0.8539, 0.8892], grad_fn=<SelectBackward0>)\n",
      "11.115071 LYS16A 102l\n",
      "\n",
      "tensor([0.8851, 0.8732, 0.7633], grad_fn=<SelectBackward0>)\n",
      "10.824371 TYR18A 102l\n",
      "\n",
      "tensor([0.7679, 0.7794, 0.7770], grad_fn=<SelectBackward0>)\n",
      "10.840122 LYS19A 102l\n",
      "\n",
      "tensor([0.8995, 0.9052, 0.8844], grad_fn=<SelectBackward0>)\n",
      "3.0958462 ASP20A 102l\n",
      "\n",
      "tensor([0.8506, 0.8215, 0.7977], grad_fn=<SelectBackward0>)\n",
      "3.7151747 GLU22A 102l\n",
      "\n",
      "tensor([0.6953, 0.9039, 0.8376], grad_fn=<SelectBackward0>)\n",
      "7.120758 TYR24A 102l\n",
      "\n",
      "tensor([0.7913, 0.8732, 0.7725], grad_fn=<SelectBackward0>)\n",
      "10.786644 TYR25A 102l\n",
      "\n",
      "tensor([0.9218, 0.7937, 0.6515], grad_fn=<SelectBackward0>)\n",
      "6.7736692 HIS31A 102l\n",
      "\n",
      "tensor([0.7959, 0.8958, 0.7713], grad_fn=<SelectBackward0>)\n",
      "10.61902 LYS35A 102l\n",
      "\n",
      "tensor([0.9368, 0.7514, 1.0000], grad_fn=<SelectBackward0>)\n",
      "9.42985 LYS44A 102l\n",
      "\n",
      "tensor([0.7781, 0.8462, 0.8742], grad_fn=<SelectBackward0>)\n",
      "3.566731 GLU46A 102l\n",
      "\n",
      "tensor([0.8388, 0.7313, 0.6083], grad_fn=<SelectBackward0>)\n",
      "3.1465588 ASP48A 102l\n",
      "\n",
      "tensor([0.7828, 0.8029, 0.8799], grad_fn=<SelectBackward0>)\n",
      "11.519907 LYS49A 102l\n",
      "\n",
      "tensor([0.7604, 0.7022, 0.7406], grad_fn=<SelectBackward0>)\n",
      "6.9244537 LYS61A 102l\n",
      "\n",
      "tensor([0.8332, 0.7805, 0.8254], grad_fn=<SelectBackward0>)\n",
      "2.985846 ASP62A 102l\n",
      "\n",
      "tensor([0.8149, 0.7562, 0.8447], grad_fn=<SelectBackward0>)\n",
      "3.6167564 GLU63A 102l\n",
      "\n",
      "tensor([0.7044, 0.8014, 0.7514], grad_fn=<SelectBackward0>)\n",
      "3.5683527 GLU65A 102l\n",
      "\n",
      "tensor([0.7394, 0.8210, 1.0000], grad_fn=<SelectBackward0>)\n",
      "11.098396 LYS66A 102l\n",
      "\n",
      "tensor([0.8896, 0.7400, 0.9390], grad_fn=<SelectBackward0>)\n",
      "3.1946893 ASP71A 102l\n",
      "\n",
      "tensor([0.8378, 1.0000, 0.9592], grad_fn=<SelectBackward0>)\n",
      "3.1643586 ASP73A 102l\n",
      "\n",
      "tensor([0.8094, 0.8613, 0.9010], grad_fn=<SelectBackward0>)\n",
      "10.003429 LYS84A 102l\n",
      "\n",
      "tensor([0.7480, 0.7877, 0.7834], grad_fn=<SelectBackward0>)\n",
      "11.065832 LYS86A 102l\n",
      "\n",
      "tensor([0.7810, 0.8047, 0.9045], grad_fn=<SelectBackward0>)\n",
      "10.568172 TYR89A 102l\n",
      "\n",
      "tensor([0.7408, 0.7683, 0.6954], grad_fn=<SelectBackward0>)\n",
      "3.1306877 ASP90A 102l\n",
      "\n",
      "tensor([0.8628, 0.7910, 0.9191], grad_fn=<SelectBackward0>)\n",
      "2.8910217 ASP93A 102l\n",
      "\n",
      "tensor([0.9759, 0.7457, 0.9024], grad_fn=<SelectBackward0>)\n",
      "3.790956 GLU109A 102l\n",
      "\n",
      "tensor([0.8591, 0.8148, 0.7637], grad_fn=<SelectBackward0>)\n",
      "11.142267 LYS125A 102l\n",
      "\n",
      "tensor([0.7655, 0.7585, 0.7837], grad_fn=<SelectBackward0>)\n",
      "2.7924755 ASP128A 102l\n",
      "\n",
      "tensor([0.8462, 0.8152, 0.9836], grad_fn=<SelectBackward0>)\n",
      "3.8015208 GLU129A 102l\n",
      "\n",
      "tensor([0.7692, 0.7465, 0.7665], grad_fn=<SelectBackward0>)\n",
      "10.888365 LYS136A 102l\n",
      "\n",
      "tensor([0.8426, 0.9264, 0.7301], grad_fn=<SelectBackward0>)\n",
      "10.816149 TYR140A 102l\n",
      "\n",
      "tensor([0.6551, 0.8444, 0.7121], grad_fn=<SelectBackward0>)\n",
      "10.807174 LYS148A 102l\n",
      "\n",
      "tensor([0.8334, 0.9046, 0.7167], grad_fn=<SelectBackward0>)\n",
      "3.1931005 ASP160A 102l\n",
      "\n",
      "tensor([0.6814, 0.9561, 0.9342], grad_fn=<SelectBackward0>)\n",
      "10.886627 TYR162A 102l\n",
      "\n",
      "tensor([0.6969, 0.6385, 0.6597], grad_fn=<SelectBackward0>)\n",
      "10.543062 LYS163A 102l\n",
      "\n",
      "tensor([0.8638, 0.7544, 0.7066], grad_fn=<SelectBackward0>)\n",
      "3.7815046 GLU5A 101m\n",
      "\n",
      "tensor([0.7421, 0.7952, 0.8582], grad_fn=<SelectBackward0>)\n",
      "3.6385202 GLU7A 101m\n",
      "\n",
      "tensor([0.7795, 0.9605, 0.9361], grad_fn=<SelectBackward0>)\n",
      "6.712697 HIS13A 101m\n",
      "\n",
      "tensor([0.8966, 0.7375, 0.7123], grad_fn=<SelectBackward0>)\n",
      "11.38378 LYS17A 101m\n",
      "\n",
      "tensor([0.9444, 0.7654, 0.8570], grad_fn=<SelectBackward0>)\n",
      "3.688446 GLU19A 101m\n",
      "\n",
      "tensor([0.8209, 0.7304, 0.9426], grad_fn=<SelectBackward0>)\n",
      "2.8260264 ASP21A 101m\n",
      "\n",
      "tensor([0.8733, 0.7789, 0.7596], grad_fn=<SelectBackward0>)\n",
      "7.2738295 HIS25A 101m\n",
      "\n",
      "tensor([0.8636, 0.9518, 0.9131], grad_fn=<SelectBackward0>)\n",
      "3.024137 ASP28A 101m\n",
      "\n",
      "tensor([0.8666, 0.8386, 0.8391], grad_fn=<SelectBackward0>)\n",
      "7.2049913 LYS35A 101m\n",
      "\n",
      "tensor([0.8713, 0.9494, 0.8760], grad_fn=<SelectBackward0>)\n",
      "6.92941 HIS37A 101m\n",
      "\n",
      "tensor([0.6813, 0.6442, 0.9765], grad_fn=<SelectBackward0>)\n",
      "3.1791444 GLU39A 101m\n",
      "\n",
      "tensor([0.9371, 0.8238, 0.8542], grad_fn=<SelectBackward0>)\n",
      "3.7750912 GLU42A 101m\n",
      "\n",
      "tensor([0.6238, 0.6520, 0.7727], grad_fn=<SelectBackward0>)\n",
      "11.134518 LYS43A 101m\n",
      "\n",
      "tensor([0.6070, 0.9288, 0.8579], grad_fn=<SelectBackward0>)\n",
      "2.9984207 ASP45A 101m\n",
      "\n",
      "tensor([0.8829, 0.8853, 0.8906], grad_fn=<SelectBackward0>)\n",
      "11.5142975 LYS48A 101m\n",
      "\n",
      "tensor([0.8812, 0.8278, 0.8632], grad_fn=<SelectBackward0>)\n",
      "6.73944 HIS49A 101m\n",
      "\n",
      "tensor([0.9474, 0.8409, 0.8492], grad_fn=<SelectBackward0>)\n",
      "10.944603 LYS51A 101m\n",
      "\n",
      "tensor([0.9581, 0.7005, 1.0000], grad_fn=<SelectBackward0>)\n",
      "3.829492 GLU53A 101m\n",
      "\n",
      "tensor([0.7471, 0.6811, 0.6704], grad_fn=<SelectBackward0>)\n",
      "3.6156144 GLU55A 101m\n",
      "\n",
      "tensor([0.7912, 0.8719, 0.7554], grad_fn=<SelectBackward0>)\n",
      "11.528574 LYS57A 101m\n",
      "\n",
      "tensor([0.6922, 0.9814, 0.8457], grad_fn=<SelectBackward0>)\n",
      "3.6107736 GLU60A 101m\n",
      "\n",
      "tensor([0.9686, 0.8902, 0.8211], grad_fn=<SelectBackward0>)\n",
      "3.1234384 ASP61A 101m\n",
      "\n",
      "tensor([0.8322, 0.7041, 0.7846], grad_fn=<SelectBackward0>)\n",
      "10.570353 LYS63A 101m\n",
      "\n",
      "tensor([0.9013, 0.7221, 0.8391], grad_fn=<SelectBackward0>)\n",
      "11.125967 LYS64A 101m\n",
      "\n",
      "tensor([0.9630, 0.8488, 0.7545], grad_fn=<SelectBackward0>)\n",
      "7.0332966 HIS65A 101m\n",
      "\n",
      "tensor([0.9024, 0.8255, 0.8407], grad_fn=<SelectBackward0>)\n",
      "10.529482 LYS78A 101m\n",
      "\n",
      "tensor([0.7813, 0.8855, 0.8655], grad_fn=<SelectBackward0>)\n",
      "11.060427 LYS79A 101m\n",
      "\n",
      "tensor([0.8918, 0.8137, 0.8067], grad_fn=<SelectBackward0>)\n",
      "10.834496 LYS80A 101m\n",
      "\n",
      "tensor([0.9340, 0.8689, 0.9377], grad_fn=<SelectBackward0>)\n",
      "6.881321 HIS82A 101m\n",
      "\n",
      "tensor([1.0000, 0.8817, 0.7047], grad_fn=<SelectBackward0>)\n",
      "7.2538424 HIS83A 101m\n",
      "\n",
      "tensor([0.8868, 0.8823, 0.8032], grad_fn=<SelectBackward0>)\n",
      "3.5428329 GLU84A 101m\n",
      "\n",
      "tensor([0.7299, 0.7308, 0.8134], grad_fn=<SelectBackward0>)\n",
      "3.6419635 GLU86A 101m\n",
      "\n",
      "tensor([0.8987, 0.7405, 0.8359], grad_fn=<SelectBackward0>)\n",
      "11.073074 LYS88A 101m\n",
      "\n",
      "tensor([0.9176, 0.8602, 0.8311], grad_fn=<SelectBackward0>)\n",
      "6.90728 HIS94A 101m\n",
      "\n",
      "tensor([0.7219, 0.6955, 0.8561], grad_fn=<SelectBackward0>)\n",
      "11.506866 LYS97A 101m\n",
      "\n",
      "tensor([0.8677, 0.8258, 0.7766], grad_fn=<SelectBackward0>)\n",
      "7.03967 HIS98A 101m\n",
      "\n",
      "tensor([0.7242, 0.7640, 0.7124], grad_fn=<SelectBackward0>)\n",
      "10.860205 LYS99A 101m\n",
      "\n",
      "tensor([0.8461, 0.7613, 0.8749], grad_fn=<SelectBackward0>)\n",
      "11.070332 LYS103A 101m\n",
      "\n",
      "tensor([0.8730, 0.8314, 0.9074], grad_fn=<SelectBackward0>)\n",
      "10.051129 TYR104A 101m\n",
      "\n",
      "tensor([0.7983, 0.7324, 0.8076], grad_fn=<SelectBackward0>)\n",
      "3.773014 GLU106A 101m\n",
      "\n",
      "tensor([0.7789, 0.7536, 0.7151], grad_fn=<SelectBackward0>)\n",
      "3.692245 GLU110A 101m\n",
      "\n",
      "tensor([0.7876, 0.7875, 0.7709], grad_fn=<SelectBackward0>)\n",
      "5.003651 HIS114A 101m\n",
      "\n",
      "tensor([0.9733, 0.7168, 0.8326], grad_fn=<SelectBackward0>)\n",
      "5.060767 HIS117A 101m\n",
      "\n",
      "tensor([0.8947, 0.7445, 0.7086], grad_fn=<SelectBackward0>)\n",
      "6.8532248 HIS120A 101m\n",
      "\n",
      "tensor([0.6690, 0.8537, 0.7917], grad_fn=<SelectBackward0>)\n",
      "3.0996985 ASP127A 101m\n",
      "\n",
      "tensor([0.9216, 0.8547, 0.9199], grad_fn=<SelectBackward0>)\n",
      "10.662857 LYS134A 101m\n",
      "\n",
      "tensor([0.9247, 0.7406, 0.8974], grad_fn=<SelectBackward0>)\n",
      "3.6000981 GLU137A 101m\n",
      "\n",
      "tensor([0.8007, 0.7570, 0.8750], grad_fn=<SelectBackward0>)\n",
      "10.865791 LYS141A 101m\n",
      "\n",
      "tensor([0.8236, 0.7739, 0.8250], grad_fn=<SelectBackward0>)\n",
      "3.1684637 ASP142A 101m\n",
      "\n",
      "tensor([0.7939, 0.9124, 0.8539], grad_fn=<SelectBackward0>)\n",
      "11.518013 LYS146A 101m\n",
      "\n",
      "tensor([0.7181, 0.7821, 0.9548], grad_fn=<SelectBackward0>)\n",
      "9.385159 TYR147A 101m\n",
      "\n",
      "tensor([0.7648, 0.8528, 1.0000], grad_fn=<SelectBackward0>)\n",
      "11.07547 LYS148A 101m\n",
      "\n",
      "tensor([0.8368, 0.6962, 0.7465], grad_fn=<SelectBackward0>)\n",
      "3.626616 GLU149A 101m\n",
      "\n",
      "tensor([0.9135, 0.8748, 0.9214], grad_fn=<SelectBackward0>)\n",
      "10.819073 TYR152A 101m\n",
      "\n",
      "tensor([0.8832, 0.7494, 0.8000], grad_fn=<SelectBackward0>)\n",
      "3.796177 GLU5A 104l\n",
      "\n",
      "tensor([0.8504, 0.8091, 0.7368], grad_fn=<SelectBackward0>)\n",
      "3.0374708 ASP10A 104l\n",
      "\n",
      "tensor([0.8773, 0.7009, 0.7908], grad_fn=<SelectBackward0>)\n",
      "3.7354913 GLU11A 104l\n",
      "\n",
      "tensor([1.0000, 0.7440, 0.8692], grad_fn=<SelectBackward0>)\n",
      "11.058256 LYS16A 104l\n",
      "\n",
      "tensor([0.7765, 0.9482, 0.8181], grad_fn=<SelectBackward0>)\n",
      "9.971584 TYR18A 104l\n",
      "\n",
      "tensor([0.8799, 0.9862, 0.8599], grad_fn=<SelectBackward0>)\n",
      "10.785202 LYS19A 104l\n",
      "\n",
      "tensor([0.8022, 0.8033, 0.8283], grad_fn=<SelectBackward0>)\n",
      "3.0870056 ASP20A 104l\n",
      "\n",
      "tensor([0.7677, 0.7933, 0.7484], grad_fn=<SelectBackward0>)\n",
      "3.7509766 GLU22A 104l\n",
      "\n",
      "tensor([0.8804, 0.7894, 0.8727], grad_fn=<SelectBackward0>)\n",
      "11.501759 TYR24A 104l\n",
      "\n",
      "tensor([0.7395, 0.8106, 0.7874], grad_fn=<SelectBackward0>)\n",
      "10.8952675 TYR25A 104l\n",
      "\n",
      "tensor([0.8823, 0.8089, 0.8767], grad_fn=<SelectBackward0>)\n",
      "7.0591774 HIS31A 104l\n",
      "\n",
      "tensor([0.8259, 0.8831, 0.8112], grad_fn=<SelectBackward0>)\n",
      "7.9153605 LYS35A 104l\n",
      "\n",
      "tensor([0.8455, 0.8315, 0.8227], grad_fn=<SelectBackward0>)\n",
      "10.04669 LYS43A 104l\n",
      "\n",
      "tensor([0.7299, 0.8628, 0.7583], grad_fn=<SelectBackward0>)\n",
      "3.180633 GLU47A 104l\n",
      "\n",
      "tensor([0.7664, 0.7196, 0.8375], grad_fn=<SelectBackward0>)\n",
      "3.0032482 ASP49A 104l\n",
      "\n",
      "tensor([0.6557, 0.9837, 0.7711], grad_fn=<SelectBackward0>)\n",
      "11.529646 LYS50A 104l\n",
      "\n",
      "tensor([0.7459, 0.8137, 0.8543], grad_fn=<SelectBackward0>)\n",
      "11.032026 LYS62A 104l\n",
      "\n",
      "tensor([0.7602, 0.7802, 1.0000], grad_fn=<SelectBackward0>)\n",
      "3.2223406 ASP63A 104l\n",
      "\n",
      "tensor([0.7754, 0.7534, 0.9592], grad_fn=<SelectBackward0>)\n",
      "3.6213398 GLU64A 104l\n",
      "\n",
      "tensor([0.8435, 0.8415, 0.8812], grad_fn=<SelectBackward0>)\n",
      "3.7888856 GLU66A 104l\n",
      "\n",
      "tensor([0.6975, 0.8542, 0.8863], grad_fn=<SelectBackward0>)\n",
      "10.947372 LYS67A 104l\n",
      "\n",
      "tensor([0.7901, 0.8805, 0.7724], grad_fn=<SelectBackward0>)\n",
      "3.0073657 ASP72A 104l\n",
      "\n",
      "tensor([1.0000, 0.8094, 0.9022], grad_fn=<SelectBackward0>)\n",
      "3.1027818 ASP74A 104l\n",
      "\n",
      "tensor([0.8114, 0.9883, 0.9216], grad_fn=<SelectBackward0>)\n",
      "11.062357 LYS85A 104l\n",
      "\n",
      "tensor([0.9152, 0.7493, 0.7080], grad_fn=<SelectBackward0>)\n",
      "11.098835 LYS87A 104l\n",
      "\n",
      "tensor([0.7617, 0.8452, 0.6083], grad_fn=<SelectBackward0>)\n",
      "11.106178 TYR90A 104l\n",
      "\n",
      "tensor([0.7946, 0.7466, 0.6347], grad_fn=<SelectBackward0>)\n",
      "2.9063745 ASP91A 104l\n",
      "\n",
      "tensor([0.8119, 0.7345, 0.8722], grad_fn=<SelectBackward0>)\n",
      "2.9202952 ASP94A 104l\n",
      "\n",
      "tensor([0.8963, 0.9096, 0.9090], grad_fn=<SelectBackward0>)\n",
      "3.6656814 GLU110A 104l\n",
      "\n",
      "tensor([0.9584, 0.8219, 0.8882], grad_fn=<SelectBackward0>)\n",
      "10.059591 LYS126A 104l\n",
      "\n",
      "tensor([0.7956, 0.8868, 0.7309], grad_fn=<SelectBackward0>)\n",
      "3.1636667 ASP129A 104l\n",
      "\n",
      "tensor([0.8874, 0.9459, 0.8602], grad_fn=<SelectBackward0>)\n",
      "3.7978287 GLU130A 104l\n",
      "\n",
      "tensor([0.8434, 0.8328, 0.5489], grad_fn=<SelectBackward0>)\n",
      "9.785675 LYS137A 104l\n",
      "\n",
      "tensor([0.9598, 0.7077, 0.6641], grad_fn=<SelectBackward0>)\n",
      "9.576412 TYR141A 104l\n",
      "\n",
      "tensor([0.8479, 0.9340, 0.7124], grad_fn=<SelectBackward0>)\n",
      "10.9088955 LYS149A 104l\n",
      "\n",
      "tensor([0.7128, 0.9855, 0.8720], grad_fn=<SelectBackward0>)\n",
      "3.199821 ASP161A 104l\n",
      "\n",
      "tensor([0.7915, 0.6661, 0.9271], grad_fn=<SelectBackward0>)\n",
      "10.891399 TYR163A 104l\n",
      "\n",
      "tensor([0.8693, 0.8380, 0.8459], grad_fn=<SelectBackward0>)\n",
      "9.979883 LYS164A 104l\n",
      "\n",
      "tensor([0.9788, 0.7920, 0.9948], grad_fn=<SelectBackward0>)\n",
      "3.5775719 GLU5B 104l\n",
      "\n",
      "tensor([0.8142, 0.8077, 0.7406], grad_fn=<SelectBackward0>)\n",
      "3.1565375 ASP10B 104l\n",
      "\n",
      "tensor([0.8077, 0.9627, 0.8161], grad_fn=<SelectBackward0>)\n",
      "3.0792613 GLU11B 104l\n",
      "\n",
      "tensor([0.8179, 0.8644, 0.8895], grad_fn=<SelectBackward0>)\n",
      "11.0568695 LYS16B 104l\n",
      "\n",
      "tensor([0.8703, 0.7109, 0.8167], grad_fn=<SelectBackward0>)\n",
      "10.101749 TYR18B 104l\n",
      "\n",
      "tensor([0.8621, 0.8785, 0.9101], grad_fn=<SelectBackward0>)\n",
      "10.418552 LYS19B 104l\n",
      "\n",
      "tensor([0.6509, 0.6341, 0.7658], grad_fn=<SelectBackward0>)\n",
      "3.0978599 ASP20B 104l\n",
      "\n",
      "tensor([0.8667, 0.7672, 0.7684], grad_fn=<SelectBackward0>)\n",
      "3.7889485 GLU22B 104l\n",
      "\n",
      "tensor([0.6495, 0.7551, 0.8055], grad_fn=<SelectBackward0>)\n",
      "10.939865 TYR24B 104l\n",
      "\n",
      "tensor([0.7352, 0.8514, 1.0000], grad_fn=<SelectBackward0>)\n",
      "10.5221195 TYR25B 104l\n",
      "\n",
      "tensor([0.7681, 0.9438, 0.9763], grad_fn=<SelectBackward0>)\n",
      "7.222171 HIS31B 104l\n",
      "\n",
      "tensor([0.7429, 0.6804, 0.9562], grad_fn=<SelectBackward0>)\n",
      "10.681959 LYS35B 104l\n",
      "\n",
      "tensor([0.7990, 0.7366, 0.8831], grad_fn=<SelectBackward0>)\n",
      "11.052635 LYS43B 104l\n",
      "\n",
      "tensor([0.7875, 0.8624, 0.7899], grad_fn=<SelectBackward0>)\n",
      "3.5748267 GLU47B 104l\n",
      "\n",
      "tensor([0.9063, 0.8147, 0.8775], grad_fn=<SelectBackward0>)\n",
      "3.1679816 ASP49B 104l\n",
      "\n",
      "tensor([0.7349, 0.8461, 0.8850], grad_fn=<SelectBackward0>)\n",
      "11.065203 LYS50B 104l\n",
      "\n",
      "tensor([0.7963, 0.9318, 0.8740], grad_fn=<SelectBackward0>)\n",
      "10.539764 LYS62B 104l\n",
      "\n",
      "tensor([0.7064, 0.7277, 0.7666], grad_fn=<SelectBackward0>)\n",
      "3.0148754 ASP63B 104l\n",
      "\n",
      "tensor([0.7874, 0.8990, 0.8333], grad_fn=<SelectBackward0>)\n",
      "3.5392504 GLU64B 104l\n",
      "\n",
      "tensor([0.8754, 0.7392, 0.7914], grad_fn=<SelectBackward0>)\n",
      "3.163186 GLU66B 104l\n",
      "\n",
      "tensor([0.8796, 0.8590, 0.7175], grad_fn=<SelectBackward0>)\n",
      "10.630552 LYS67B 104l\n",
      "\n",
      "tensor([0.8600, 0.8498, 0.7774], grad_fn=<SelectBackward0>)\n",
      "3.1853962 ASP72B 104l\n",
      "\n",
      "tensor([0.8880, 0.7922, 0.7820], grad_fn=<SelectBackward0>)\n",
      "3.1797214 ASP74B 104l\n",
      "\n",
      "tensor([0.9163, 0.8391, 0.7672], grad_fn=<SelectBackward0>)\n",
      "10.559164 LYS85B 104l\n",
      "\n",
      "tensor([0.7764, 0.8156, 0.7664], grad_fn=<SelectBackward0>)\n",
      "11.532242 LYS87B 104l\n",
      "\n",
      "tensor([0.8101, 0.7787, 0.7542], grad_fn=<SelectBackward0>)\n",
      "10.827587 TYR90B 104l\n",
      "\n",
      "tensor([0.9255, 0.9171, 0.8708], grad_fn=<SelectBackward0>)\n",
      "3.1088653 ASP91B 104l\n",
      "\n",
      "tensor([0.9281, 0.8567, 0.8498], grad_fn=<SelectBackward0>)\n",
      "3.13349 ASP94B 104l\n",
      "\n",
      "tensor([0.7597, 0.7271, 0.7480], grad_fn=<SelectBackward0>)\n",
      "3.7797604 GLU110B 104l\n",
      "\n",
      "tensor([0.9759, 0.7924, 0.8342], grad_fn=<SelectBackward0>)\n",
      "10.565582 LYS126B 104l\n",
      "\n",
      "tensor([0.8941, 0.8534, 0.8007], grad_fn=<SelectBackward0>)\n",
      "2.9024067 ASP129B 104l\n",
      "\n",
      "tensor([0.7210, 0.8744, 0.8267], grad_fn=<SelectBackward0>)\n",
      "3.638999 GLU130B 104l\n",
      "\n",
      "tensor([0.8568, 0.6730, 0.8648], grad_fn=<SelectBackward0>)\n",
      "10.567177 LYS137B 104l\n",
      "\n",
      "tensor([0.8477, 0.7127, 0.7892], grad_fn=<SelectBackward0>)\n",
      "10.891006 TYR141B 104l\n",
      "\n",
      "tensor([0.8577, 0.8684, 0.8458], grad_fn=<SelectBackward0>)\n",
      "10.822418 LYS149B 104l\n",
      "\n",
      "tensor([0.9310, 0.9235, 0.8974], grad_fn=<SelectBackward0>)\n",
      "3.0541983 ASP161B 104l\n",
      "\n",
      "tensor([0.7066, 0.8131, 0.7375], grad_fn=<SelectBackward0>)\n",
      "11.062122 TYR163B 104l\n",
      "\n",
      "tensor([0.8829, 0.9651, 0.8750], grad_fn=<SelectBackward0>)\n",
      "10.835701 LYS164B 104l\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outs_val={}\n",
    "\n",
    "for pdb in fails:#Ds.keys():\n",
    "    \n",
    "    outie = {}\n",
    "    #print(pdb)\n",
    "    #Ddict = Ds[pdb]\n",
    "\n",
    "\n",
    "    d = all_data[pdb]\n",
    "    pos, hs, ions, T= d[\"pos\"], d[\"Hs\"], d[\"ions\"], d[\"targets\"]\n",
    "\n",
    "    \n",
    "    for sample in pos.keys():\n",
    "        #print(pos.keys() == hs.keys() == ions.keys())# == T.keys())\n",
    "        #print()\n",
    "\n",
    "        coords = torch.tensor(tuple(pos[sample].values()))\n",
    "        species = torch.tensor(hs[sample], dtype=int) #todo\n",
    "        L = coords.shape[0]\n",
    "        net, optimizer = model(23, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "        #net, optimizer = model(23, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "        #x = loop(10, coords, species, ions[sample], net, optimizer, 1) #margin, negative slope\n",
    "        x=loop(1, coords, torch.tensor(hs[sample], dtype=int), ions[sample], net, optimizer, 1)\n",
    "        o = x[0]\n",
    "        print(x[0][0])\n",
    "        out = torch.mean(o, dim=1)\n",
    "        out=\n",
    "        #f = out.unsqueeze(0).detach().numpy()*coords.detach().numpy().T\n",
    "        combined = np.column_stack(out.unsqueeze(0).detach().numpy()*coords.detach().numpy().T).flatten()\n",
    "        \n",
    "        D=np.gradient(np.gradient(combined)).reshape(-1,3)\n",
    "        \n",
    "        #f = torch.mean(torch.tensor(x), dim=1).unsqueeze(0).detach().numpy()*coords.detach().numpy().T\n",
    "        #combined = np.column_stack(f).flatten()\n",
    "        \n",
    "        #D=np.gradient(np.gradient(np.column_stack(f).flatten())).reshape(-1,3)\n",
    "        #plot(x[0], D)\n",
    "        def loop2(D, sample):\n",
    "            #maxes=[]#,[],[]\n",
    "            #print(sample, pdb)\n",
    "            #r = sample[:3]\n",
    "            #out = torch.mean(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "            loss = torch.nn.HuberLoss()\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            ins = torch.ones(L)\n",
    "            ins[0]=0\n",
    "            ins[-1] = 2#,10,10\n",
    "\n",
    "            #label=T[sample]\n",
    "            #up = 80\n",
    "            \n",
    "            #for epoch in range(4):\n",
    "\n",
    "               # optimizer3.zero_grad()\n",
    "            #with optimizer3.no_grad()\n",
    "            y=torch.mean((pnet(torch.tensor(ins, dtype=int).unsqueeze(0), torch.tensor(D).unsqueeze(0))[0]), dim=-1)\n",
    "            pk=max(y[0].detach().numpy()[1::])\n",
    "            print(pk, sample, pdb)\n",
    "                \n",
    "            #loss2=loss(max(y[0][1::]), torch.tensor(label))\n",
    "            #print(loss2.detach().item(), sample, pdb)\n",
    "                \n",
    "            #optimizer3.step()\n",
    "            #print(loss2.detach().item(), \"finished 4 epochs\", sample, pdb)\n",
    "            print(\"\")\n",
    "            return pk\n",
    "        #print(\"finished 50 epochs\", loss2)\n",
    "                \n",
    "            \n",
    "            #a=y[0].detach().numpy()\n",
    "            #maxes.append((np.where(a == max(a[1::]))[0], max(a[1::])))\n",
    "            #maxes.append(max(a[1::]))\n",
    "            #Ns.append(a[0])\n",
    "            #scs.append(a[-1])\n",
    "            #print(loss2.item())\n",
    "\n",
    "            #print(sp, loss2)\n",
    "            #losses[sample] = loss2\n",
    "            #print(sample, loss2.detach(), pdb)\n",
    "            #print(loss2,sample)\n",
    "                    # maxes#, Ns, scs\n",
    "        \n",
    "        \n",
    "        #m = loop2(D, sample)\n",
    "        #print(sample)\n",
    "        #for i in range(59):\n",
    "        #print(\"finished\", sample, loss2)\n",
    "        outie[sample] = loop2(D, sample)\n",
    "    \n",
    "    #losses[pdb] = loss2\n",
    "\n",
    "    outs_val[pdb] = outie\n",
    "    #losses[pdb] = loss2\n",
    "        #coords = torch.tensor()\n",
    "    #x[\"ions\"], x[\"\"]\n",
    "    #n#et, optimizer = model(x[\"lengths, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'111m'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '111m'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-11fce2f93695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mddd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mddd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"111m\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '111m'"
     ]
    }
   ],
   "source": [
    "ddd = df.compute()\n",
    "ddd[\"111m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idcode</th>\n",
       "      <th>chain</th>\n",
       "      <th>residue_name</th>\n",
       "      <th>residue_number</th>\n",
       "      <th>pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2676641</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>NTR</td>\n",
       "      <td>0</td>\n",
       "      <td>6.614950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208515</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>4</td>\n",
       "      <td>2.280690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641896</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>6</td>\n",
       "      <td>2.352770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938907</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>12</td>\n",
       "      <td>6.340330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955168</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>16</td>\n",
       "      <td>10.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676396</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>18</td>\n",
       "      <td>2.537760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141068</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>ASP</td>\n",
       "      <td>20</td>\n",
       "      <td>2.570820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195178</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>24</td>\n",
       "      <td>1.739810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273385</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>ASP</td>\n",
       "      <td>27</td>\n",
       "      <td>2.773460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951285</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>34</td>\n",
       "      <td>10.829900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292603</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>36</td>\n",
       "      <td>7.492550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097992</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>38</td>\n",
       "      <td>3.355690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207362</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>41</td>\n",
       "      <td>4.343040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182799</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>42</td>\n",
       "      <td>12.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765800</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>ASP</td>\n",
       "      <td>44</td>\n",
       "      <td>2.840210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499200</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>47</td>\n",
       "      <td>11.078800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010378</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>48</td>\n",
       "      <td>5.582940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734980</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>50</td>\n",
       "      <td>10.826600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744202</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>52</td>\n",
       "      <td>2.526050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877753</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>54</td>\n",
       "      <td>3.276910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193872</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>56</td>\n",
       "      <td>11.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251201</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>59</td>\n",
       "      <td>3.107980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527256</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>ASP</td>\n",
       "      <td>60</td>\n",
       "      <td>2.173890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055033</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>62</td>\n",
       "      <td>11.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96498</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>63</td>\n",
       "      <td>10.552400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279455</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>64</td>\n",
       "      <td>5.284170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731373</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>77</td>\n",
       "      <td>11.573800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251202</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>78</td>\n",
       "      <td>11.705200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218329</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>79</td>\n",
       "      <td>11.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493170</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>81</td>\n",
       "      <td>6.522960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459968</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>82</td>\n",
       "      <td>5.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655621</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>83</td>\n",
       "      <td>2.713660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202427</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>85</td>\n",
       "      <td>2.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924849</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>87</td>\n",
       "      <td>10.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820925</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>93</td>\n",
       "      <td>5.237080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78267</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>96</td>\n",
       "      <td>10.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346449</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>97</td>\n",
       "      <td>5.604780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195179</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>98</td>\n",
       "      <td>10.888600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398573</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>102</td>\n",
       "      <td>11.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171515</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>TYR</td>\n",
       "      <td>103</td>\n",
       "      <td>10.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423082</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>105</td>\n",
       "      <td>3.658250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643394</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>109</td>\n",
       "      <td>2.594330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865531</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>113</td>\n",
       "      <td>5.973640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859367</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>116</td>\n",
       "      <td>6.789990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090024</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>HIS</td>\n",
       "      <td>119</td>\n",
       "      <td>5.084780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594445</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>ASP</td>\n",
       "      <td>126</td>\n",
       "      <td>3.622640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152200</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>133</td>\n",
       "      <td>11.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153336</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>136</td>\n",
       "      <td>3.712290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936882</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>140</td>\n",
       "      <td>9.866550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676397</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>ASP</td>\n",
       "      <td>141</td>\n",
       "      <td>0.669339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549994</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>145</td>\n",
       "      <td>11.202600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817282</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>TYR</td>\n",
       "      <td>146</td>\n",
       "      <td>13.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738103</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>LYS</td>\n",
       "      <td>147</td>\n",
       "      <td>10.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354276</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>GLU</td>\n",
       "      <td>148</td>\n",
       "      <td>3.753530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212187</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>TYR</td>\n",
       "      <td>151</td>\n",
       "      <td>11.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731374</th>\n",
       "      <td>111m</td>\n",
       "      <td>A</td>\n",
       "      <td>CTR</td>\n",
       "      <td>153</td>\n",
       "      <td>2.294000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        idcode chain residue_name  residue_number         pk\n",
       "2676641   111m     A          NTR               0   6.614950\n",
       "1208515   111m     A          GLU               4   2.280690\n",
       "641896    111m     A          GLU               6   2.352770\n",
       "938907    111m     A          HIS              12   6.340330\n",
       "1955168   111m     A          LYS              16  10.484200\n",
       "676396    111m     A          GLU              18   2.537760\n",
       "1141068   111m     A          ASP              20   2.570820\n",
       "195178    111m     A          HIS              24   1.739810\n",
       "2273385   111m     A          ASP              27   2.773460\n",
       "951285    111m     A          LYS              34  10.829900\n",
       "292603    111m     A          HIS              36   7.492550\n",
       "1097992   111m     A          GLU              38   3.355690\n",
       "207362    111m     A          GLU              41   4.343040\n",
       "182799    111m     A          LYS              42  12.051700\n",
       "1765800   111m     A          ASP              44   2.840210\n",
       "2499200   111m     A          LYS              47  11.078800\n",
       "2010378   111m     A          HIS              48   5.582940\n",
       "1734980   111m     A          LYS              50  10.826600\n",
       "2744202   111m     A          GLU              52   2.526050\n",
       "877753    111m     A          GLU              54   3.276910\n",
       "2193872   111m     A          LYS              56  11.268400\n",
       "1251201   111m     A          GLU              59   3.107980\n",
       "1527256   111m     A          ASP              60   2.173890\n",
       "1055033   111m     A          LYS              62  11.453400\n",
       "96498     111m     A          LYS              63  10.552400\n",
       "2279455   111m     A          HIS              64   5.284170\n",
       "731373    111m     A          LYS              77  11.573800\n",
       "1251202   111m     A          LYS              78  11.705200\n",
       "2218329   111m     A          LYS              79  11.499500\n",
       "2493170   111m     A          HIS              81   6.522960\n",
       "1459968   111m     A          HIS              82   5.522000\n",
       "1655621   111m     A          GLU              83   2.713660\n",
       "1202427   111m     A          GLU              85   2.306400\n",
       "1924849   111m     A          LYS              87  10.694700\n",
       "1820925   111m     A          HIS              93   5.237080\n",
       "78267     111m     A          LYS              96  10.323400\n",
       "2346449   111m     A          HIS              97   5.604780\n",
       "195179    111m     A          LYS              98  10.888600\n",
       "1398573   111m     A          LYS             102  11.138400\n",
       "1171515   111m     A          TYR             103  10.780300\n",
       "1423082   111m     A          GLU             105   3.658250\n",
       "1643394   111m     A          GLU             109   2.594330\n",
       "865531    111m     A          HIS             113   5.973640\n",
       "859367    111m     A          HIS             116   6.789990\n",
       "2090024   111m     A          HIS             119   5.084780\n",
       "1594445   111m     A          ASP             126   3.622640\n",
       "152200    111m     A          LYS             133  11.428200\n",
       "1153336   111m     A          GLU             136   3.712290\n",
       "1936882   111m     A          LYS             140   9.866550\n",
       "676397    111m     A          ASP             141   0.669339\n",
       "549994    111m     A          LYS             145  11.202600\n",
       "2817282   111m     A          TYR             146  13.115400\n",
       "2738103   111m     A          LYS             147  10.551600\n",
       "354276    111m     A          GLU             148   3.753530\n",
       "2212187   111m     A          TYR             151  11.005400\n",
       "731374    111m     A          CTR             153   2.294000"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = ddd[ddd.iloc[:, 0] == \"111m\"]#.drop(columns = [\"idcode\"])#[idcode==\"2bb7\"]\n",
    "#pdf.sort_values(by='', ascending=True)  # Sorts in ascending order\n",
    "a=pdf.sort_values(by=['chain', 'residue_number'], ascending=[True, True])\n",
    "#pyp = list(a[\"pk\"])\n",
    "a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idcode</th>\n",
       "      <th>chain</th>\n",
       "      <th>residue_name</th>\n",
       "      <th>residue_number</th>\n",
       "      <th>pk</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=5</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 5 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "               idcode   chain residue_name residue_number       pk\n",
       "npartitions=5                                                     \n",
       "               object  object       object          int64  float64\n",
       "                  ...     ...          ...            ...      ...\n",
       "...               ...     ...          ...            ...      ...\n",
       "                  ...     ...          ...            ...      ...\n",
       "                  ...     ...          ...            ...      ...\n",
       "Dask Name: read-csv, 5 tasks"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f1f229ff780>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229ff320>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229fef98>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229a4358>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229a47b8>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229a4c50>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229ae128>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229ae5c0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229aeac8>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229aef28>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229ae828>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229a4978>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229b5400>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229b5898>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229b5d30>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229bf208>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229bf6a0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229bfb38>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22949048>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229bf828>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229b5940>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22949358>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229497f0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22949c88>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22953128>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229535c0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22953a58>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22953ef0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2295b3c8>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22949eb8>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229b5518>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2295b5c0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2295ba58>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2295bef0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229653c8>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22965860>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22965cf8>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2296c1d0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22965550>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2295b6d8>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2296c208>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2296c8d0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2296cd68>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22977240>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229776d8>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22977b70>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22900080>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229777f0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2296c6a0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f229000b8>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22900828>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f22900cc0>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2290a198>,\n",
       "  <matplotlib.axis.XTick at 0x7f1f2290a630>],\n",
       " [Text(0, 0, 'G'),\n",
       "  Text(1, 0, 'G'),\n",
       "  Text(2, 0, 'H'),\n",
       "  Text(3, 0, 'L'),\n",
       "  Text(4, 0, 'G'),\n",
       "  Text(5, 0, 'A'),\n",
       "  Text(6, 0, 'H'),\n",
       "  Text(7, 0, 'A'),\n",
       "  Text(8, 0, 'L'),\n",
       "  Text(9, 0, 'H'),\n",
       "  Text(10, 0, 'G'),\n",
       "  Text(11, 0, 'G'),\n",
       "  Text(12, 0, 'L'),\n",
       "  Text(13, 0, 'A'),\n",
       "  Text(14, 0, 'L'),\n",
       "  Text(15, 0, 'H'),\n",
       "  Text(16, 0, 'L'),\n",
       "  Text(17, 0, 'G'),\n",
       "  Text(18, 0, 'G'),\n",
       "  Text(19, 0, 'L'),\n",
       "  Text(20, 0, 'G'),\n",
       "  Text(21, 0, 'A'),\n",
       "  Text(22, 0, 'L'),\n",
       "  Text(23, 0, 'L'),\n",
       "  Text(24, 0, 'H'),\n",
       "  Text(25, 0, 'L'),\n",
       "  Text(26, 0, 'L'),\n",
       "  Text(27, 0, 'L'),\n",
       "  Text(28, 0, 'H'),\n",
       "  Text(29, 0, 'H'),\n",
       "  Text(30, 0, 'G'),\n",
       "  Text(31, 0, 'G'),\n",
       "  Text(32, 0, 'L'),\n",
       "  Text(33, 0, 'H'),\n",
       "  Text(34, 0, 'L'),\n",
       "  Text(35, 0, 'H'),\n",
       "  Text(36, 0, 'L'),\n",
       "  Text(37, 0, 'L'),\n",
       "  Text(38, 0, 'T'),\n",
       "  Text(39, 0, 'G'),\n",
       "  Text(40, 0, 'G'),\n",
       "  Text(41, 0, 'H'),\n",
       "  Text(42, 0, 'H'),\n",
       "  Text(43, 0, 'H'),\n",
       "  Text(44, 0, 'A'),\n",
       "  Text(45, 0, 'L'),\n",
       "  Text(46, 0, 'G'),\n",
       "  Text(47, 0, 'L'),\n",
       "  Text(48, 0, 'A'),\n",
       "  Text(49, 0, 'L'),\n",
       "  Text(50, 0, 'T'),\n",
       "  Text(51, 0, 'L'),\n",
       "  Text(52, 0, 'G'),\n",
       "  Text(53, 0, 'T')])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(outs_val[\"111m\"].values()))\n",
    "plt.plot(pyp)\n",
    "plt.xticks(range(len(outs_val[\"111m\"])), [k[0] for k in outs_val[\"111m\"].keys()], rotation=45, fontsize=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GLU5A': 3.6614752,\n",
       " 'GLU7A': 3.7015295,\n",
       " 'HIS13A': 6.904503,\n",
       " 'LYS17A': 11.53557,\n",
       " 'GLU19A': 3.6397069,\n",
       " 'ASP21A': 2.8822436,\n",
       " 'HIS25A': 7.0057755,\n",
       " 'ASP28A': 3.2122722,\n",
       " 'LYS35A': 11.100159,\n",
       " 'HIS37A': 5.821362,\n",
       " 'GLU39A': 3.6445308,\n",
       " 'GLU42A': 3.7574067,\n",
       " 'LYS43A': 9.915466,\n",
       " 'ASP45A': 2.9819636,\n",
       " 'LYS48A': 10.527309,\n",
       " 'HIS49A': 7.0563107,\n",
       " 'LYS51A': 11.056099,\n",
       " 'GLU53A': 3.0339026,\n",
       " 'GLU55A': 3.7634654,\n",
       " 'LYS57A': 10.831911,\n",
       " 'GLU60A': 3.544126,\n",
       " 'ASP61A': 3.2108169,\n",
       " 'LYS63A': 10.398195,\n",
       " 'LYS64A': 10.975306,\n",
       " 'HIS65A': 7.0646095,\n",
       " 'LYS78A': 10.92285,\n",
       " 'LYS79A': 10.614771,\n",
       " 'LYS80A': 11.062555,\n",
       " 'HIS82A': 6.8014517,\n",
       " 'HIS83A': 7.035763,\n",
       " 'GLU84A': 3.553093,\n",
       " 'GLU86A': 3.698472,\n",
       " 'LYS88A': 11.112825,\n",
       " 'HIS94A': 7.268468,\n",
       " 'LYS97A': 10.560747,\n",
       " 'HIS98A': 7.0616617,\n",
       " 'LYS99A': 11.102276,\n",
       " 'LYS103A': 11.146502,\n",
       " 'TYR104A': 10.839798,\n",
       " 'GLU106A': 3.7775664,\n",
       " 'GLU110A': 3.568153,\n",
       " 'HIS114A': 6.85989,\n",
       " 'HIS117A': 6.8740807,\n",
       " 'HIS120A': 7.26425,\n",
       " 'ASP127A': 2.8723912,\n",
       " 'LYS134A': 10.913795,\n",
       " 'GLU137A': 3.8068194,\n",
       " 'LYS141A': 11.148605,\n",
       " 'ASP142A': 3.1685667,\n",
       " 'LYS146A': 10.76099,\n",
       " 'TYR147A': 10.444563,\n",
       " 'LYS148A': 10.596519,\n",
       " 'GLU149A': 3.825955,\n",
       " 'TYR152A': 11.529188}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs_val[\"111m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1f22550390>]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(list(outs_val[\"111m\"].values()), pyp[1:-1])\n",
    "plt.plot(range(13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['111m',\n",
       " '110m',\n",
       " '108m',\n",
       " '109m',\n",
       " '106m',\n",
       " '103m',\n",
       " '107m',\n",
       " '103l',\n",
       " '102m',\n",
       " '102l',\n",
       " '101m',\n",
       " '104l']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAAFzCAYAAADFZzQZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABEjklEQVR4nO3deVyVdd7/8dcXFElxwwU1EDVFc0sFzMYyTSvHFlssrWYmZ1rumqbBu5kanaayX5PhLKX3Pffc3U3OXd6VUOa0r1ZkmQug5i7mCioKigsqIvD9/XGBIoIHhHMuzjnv5+PB4xwOnHN96Mp6++V7rrex1iIiIiIiIjULcXsAEREREZHGTqFZRERERMQDhWYREREREQ8UmkVEREREPFBoFhERERHxQKFZRERERMSDJm4PUBvt27e33bp18/lxjx49SosWLXx+XPEtnefgoPMcHHSeA5/OcXBw8zxnZmbmW2s7VH3cL0Jzt27dyMjI8Plx09LSGDlypM+PK76l8xwcdJ6Dg85z4NM5Dg5unmdjzI7qHtf2DBERERERDxSaRUREREQ8UGgWEREREfHAL/Y0i4iIiEjDOnnyJDk5ORQVFbk9yllat27Nhg0bvHqM8PBwoqOjadq0aa2+X6FZREREJAjl5OTQsmVLunXrhjHG7XHOcOTIEVq2bOm117fWsn//fnJycujevXutnqPtGSIiIiJBqKioiHbt2jW6wOwLxhjatWtXp1V2hWYRERGRIBWMgblCXX92hWYRERGpnZMnYdYsuOEG53bixNNfmzcPkpMhK8v52tixLg0p/uQXv/gFHTt2pH///qceO3DgAOPHj6dXr15cffXVFBQUAFBQUMDNN9/MwIEDGTp0KGvXrvXprArNIiIiUjtNm8KUKTB8uHM7eLDz+Lp10Lmzcz8uDh58EC6/3K0pxY9MnjyZTz755IzHkpOTufLKK9m8eTOjR48mOTkZgBkzZjBo0CBWr17N3LlzSUpK8umsCs0iIiJSP0uWwKpVsHix8/mHH8K4ca6OJP5hxIgRREZGnvHYu+++y5133gnA3XffzTvvvAPA+vXrueqqqwDo06cP27dvZ+/evWzfvp0+ffowefJk4uLiuOuuu1i4cCHDhw+nV69eLF++vEFmVWgWERGRuvv+eyckf/wx3Hvv6RVogJUrYcgQV8cT/7V37146deoEQKdOndi7dy8Al1xyCQsWLABg+fLl7Nixg5ycHAB++OEHfvOb37Bx40Y2btzIG2+8wbfffstf/vIXZsyY0SBzKTSLiIhI3UydCpdcAu+/T2bfYfxszjIydxQ4jwM884y784nXZO4oOH2+fcAYc+oNe1OnTuXgwYMMGjSI//zP/2Tw4MGEhoYC0L17dwYMGEBISAj9+vVj9OjRGGMYMGAA27dvb5BZdJ1mEREROW+zF2axaHM+AHPvudTlacTbfHG+o6KiyM3NpWXLluzZs4eOHTsC0KpVK/73f/8XcK6z3L17d3r06MGBAwdo1qzZqeeHhISc+jwkJISSkpIGmUsrzSIiInLeksbEMaJXe5LGxLk9iviAL873jTfeyBtvvAHAq6++yvjx4wE4ePAgxcXFALz88suMGDGCVq1aeW2OqrTSLCIiIuctPratVpiDSEOf7zvuuIO0tDTy8/OJjo7m6aefZurUqdx666289tprxMbG8uabbwKwYcMG7r77bowx9OvXjzlz5jTYHLWh0CwiIiIirpg3b161j7///vtn1WhfdtllZGVlnfW93bp1O+Oaza+88kqNX6sPbc8QEREREfFAoVlERERExAOFZhERERERDxSaRURERIKUtdbtEVxT159doVlEREQkCIWHh7N///6gDM7WWvbv3094eHitn6OrZ4iIiIgEoejoaHJycsjLy3N7lLMUFRXVKdCej/DwcKKjo2v9/QrNIiIiIkGoadOmdO/e3e0xqpWWlsbgwYPdHuMM2p4hIiIiIuKBQrOIiIiIiAcKzSIiIiIiHig0i4iIiIh4oNAsIiIiIuKBQrOIiIiIiAcKzSIiIiIiHig0i4iIiIh4oNAsIiIiIuKBQrOIiIiInJac7NwWF8ONN0JuLnz8McyYAe+/7+5sLlJoFhEREZGzvf46jB3r3B82DLKzITzc3ZlcpNAsIiIiImdbswa+/RYWL4a2beFvf4OtW92eyjVN3B5ARERERBqR0lKYNQt+9CMoLIThw2HOHNiyxVlxDlJeC83GmH8C1wP7rLX9yx/7M3ADUAxsAX5urT3orRlEREREpI4ef/zsx+65x/dzNDLe3J7xCjC2ymOfA/2ttQOBLGCaF48vIiIiIvWQuaOAn81ZRuaOArdHcZ3XQrO1dhFwoMpjn1lrS8o/XQpEe+v4IiIiIlI/sxdmsWhzPrMXZvnkeNvyj5L88UbmrDnhk+PVhbHWeu/FjekGfFCxPaPK194HUq21r9Xw3PuB+wGioqLiU1JSvDZnTQoLC4mIiPD5ccW3dJ6Dg85zcNB5Dnw6x751rLiUfYeL6NgqnOZhoV45RnGpJWNvKYtyTrLxQBkhBgZEWpISWhBijFeOeS6jRo3KtNYmVH3clTcCGmMeB0qA12v6HmvtS8BLAAkJCXbkyJG+Ga6StLQ03Diu+JbOc3DQeQ4OOs+BT+c4cGzYc5iU5Tv518pdHC4qoWtkcx69NoYJ8dFsWLG00Z1nn4dmY8xknDcIjrbeXOYWERERCRTJyTB1Ksyd61wKrndvCAmBgwchNhZuvdXtCWvlSNFJ3v9+D6npO/k+5xBhoSGM7d+JSYkxDOvRjpAQZ2V5g8tzVsenodkYMxZ4DLjSWnvMl8cWERER8Xs/+xk8/zzccgv8x3/A9Olw332NOjRba1mxs4CU5dl8sHoPx0+W0juqJU/d0JebBl1I2xZhbo9YK9685Nw8YCTQ3hiTAzyFc7WMZsDnxtmjstRa+4C3ZhAREREJOAcOQGQkjB7tBOfWrd2eqFoHjhazYEUOqenZbN5XSPOwUMYP6sLExBgGxbTBuLBfuT68FpqttXdU8/Acbx1PREREJGBVFI4cOgT9y6+vUFYGRUUwfnzDH6+67SDGwJEjztenTKn2aWVllsVb8klJz+azdbmcLLUM7tqGmbcO4LqBXYho5r+9ev47uYiIiEiwqFQ4krmjgNlzlpE0ZiDxV17p3eNW3g4yfz7s3QsDBpz1bXsOHWd+Rg6pGdnkFBynTfOm/GRYLJMSu9K7U0vvzugjCs0iIiLSOFSsbj7/vLOKGhXlvMntu++gY0f4xS/cnrBRqLh2MsDcey71/gErtoOUlcFzz8EzzwBwsrSMLzfuIzU9m7RN+yizMLxnOx4b24dr+kYR3tQ7l6hzi0KziIiINC5FRZCXB0OHwpIlTpCeOdP3c4wdC598AklJzhaFF16Azz+HhQuhTx9nu8Rzz0H37jBpks/GShoTd8atV1S3HeTYMfjrX8lv0YY5n2xkfmYOeUdO0LFlMx4ceRETE7rStV1z783kMoVmERERaVwiI2HaNHj2WQhrBFdWOHYMWrSApk1h8mR47TVni8LXXzu3hYU+HSc+tq33V5irbAd54R9LiR9wPcu27Wfp1gOE5m9lVO+OTEqMYWTvDjQJ9VrJdKOh0CwiIiKNQ8Xq5ksvweHDcPHF0KGDs8rcsaN7c4WEwPDhkJoKTz8NmZnwk584twcPQkGBT1eafWnDnsM8/MYKdh8q4tst+8sLSHozIT6aqFbhbo/nUwrNIiIi0jhUrG5OmeK82W1hFkkJccSPGOHOPKWlcPPNzgrzN9/A/ffDgw86WzWGDIFHHoHt22HpUnfm85LCEyW8t2r3qQKSJiGGDhHNeGhUT352WeypApJgo9AsIiIijY7P3+xWnc8/P3X3VIifejfxsW1Pf0+3bs6Hr1S8WfKRR6BrV2eFe+lSWL8ejh8/9Sa9unIKSA6Smr6TD1bv4VixfxaQeJNCs4iIiDQ6PnmzWx1UhPg1uw7x8t2JZwZnN7Rr5+ylDgmBm26CEyegX786v0x1BSQ3XuK/BSTeFPi7tkVERMTvVLzZzfVwWi5pTBxtmzel4NhJZi/McnscZyvLww87+6zB2TJScZULD8rKLN9uzudXb6xg2Iwv+OOHG2jRrAkzbx3A8sfHkLz5IwZ3bYt59VVnVbvCAw84q9rr1sGMGfDBB174wRovrTSLiIiIeBAf25aX7050tmj4YvW7YhvGK6/Axo3O5wsWQFoaZGVB8+ZOUB461HkjYmSkx5esroDkrmFdmZgYQ59Orc5+wuTJznHBOfbIkafvt20cf5nxJYVmERERkVrwyaXeqqocXDdvdq4bPXMm/O535Ex7mu+y9nHRYYh/5JFqn36ytIyvygtIvqpPAcnq1ZCfD7t2wf798NvfOteovv76hvk5/YBCs4iIiEhjlpbmrDqXlsKNN8LAgTBnDl9vO0jbtd8ze2HWWWF+e/5RUjOyz6+ApOLSf23bwuLFzor29OnOHOHhztdfeAHatPHOz9tIKTSLiIiINDaVg+uOHU5w3b0bQkPhoovg+++5cesy1rRtdWq7SNHJUj5dl0vK8myWbN1PaIipfQHJyZPwX/8FX3wBo0c7TYypqdC6NXteepVXmnVn/Kh+9F3ylfOmw+nTffKPoTFRaBYRERFpbCo18rFnj9M8GBYGF1wArVpBnz60nDWLHz3zDBvCQpn+3jr+tXIXh46fPL8CkqZNYcoUp8K84hagRQuW7DzEprA8knMvYO60aWfOFkQUmkVEREQas9JSsp+cwTvHO3DDkIvoNnw4J+b+H+uT/sB3+4r58+xvCAsNYWz/TkxKjGFYj3YNV0By9dXExiVw/ZSpdB8TB//zP3D77Q3z2n5GoVlERESkMXv8cR6fs4xFxfm8FxLB4X+u5dCx/hRdUEbvoS15MjGGmwc3UAFJWhr8+Mfwf//nFKYMG0b84sXEcwyen+68EbBJE7jkkvofy88oNIuIiIg0NhWXnEtN5XjGSi7vNJhLl7zP/ibN+eqiBEy3Hvz3T+IbvoBk5EgnEN92Gzz2GEybBr16OXuY77sP5s9vuGP5GZWbiIiIiDQyZday+Id8nsiN4ONPM1mwLo+wjh0YGd2cwV1b8193xTsFJN5q7Nu2DV5+mcJvl/DB0i3s+s3vYft2p+AkSGmlWURERPxLTVd6yMqC1193rjgxZQqkp8NLL8E//uH2xLWWe6iI+ZnZNFm0leRDy2jTvCktH3uKOcd+4ML7X4CyMkYkJ4O3mhIrrtpx/DgcPcriNt3Yv+8Ay8Kaccuvf+20AQYphWYRERHxLzVd6eHDD+EPf4Dnn4djx2DtWufybI1cdQUkM5uF8kFpBr3DwmmamQuTJsG8eU5ovfxy7w1TcWWMoiKYOpX2dxaw4L0VPH4gA264QaFZREREJKAsXw779jnlHDk5EB3t9kRn2Z5/lDczsnmrSgHJ7QkxxLa77vQ3JifDpZc6P0ubNpCXB2+/7Vy/ubAQnnyy4Ycrv2LHtjU7eLpDUzo9eA98840zw+LFMHx4wx+zkVNoFhEREf/1/fdOiPv4Yxg3Dp591tmeMXKk82FtowrM51NAklNwnN/PWcZzuQVceLIQhg6Fgwdh717o3Nk7g1a6YsegmDa0WlFM0pj+xL//vneO5wcUmkVERMQ/TZ3q3JYHucwdBcyO+TFJY+KIr/o9LtuYe5iU5dnnVUDy3Q/5LDL5fLanmJ/Pfd75i0GHDjBzJjzzjNdmrmgaPHz8JIs25wOcVdcdTBSaRUREJCDMXphV93BXcWm3V16BjRudz8G5SsTkyc51ic9zG0ThiRLe/343KenZfJ998LwLSIb3aMuz2xdyS+ZH8Od2cPHFkJIChw7Bli3wl7/A1Vc7l4p77jno3t3ZA11P8bFtmXvPpc5fRhZmnQrRwUqhWURERAJCRag7r3A3efLpwLxggbO1A5w3HdZhG4S1lpXZB0ldns37q3dzrLiUuKgInry+73kXkFz45z9yFwAvnAqwM3r0Jfqxx5xrJzdt6nx8/bVTt11YWOdjnEtFeA52Cs0iIiISEBos3K1eDfn5zipzRESttkEUHC1mwcpdpKbvJGtvIc3DQrnxki5MTIxxCkhmzoTLq6xov/OO07p3/Hitt1lUrKZ/90M+twOEhEBSEjz1FLRu7ex1LihokJVmOZNCs4iIiASviusSt23rvKFwzRqn/S4tDcLDYdMm+NOfoFmzs55aVmZZsnU/KenZfLo2l+LSMgbFtCH5lgFcf0kXIppVE7Mqr2jfdBOcOAH9+tV63IpV9OGlbZ25e/SAv/4VBg2Cm292CkiWLq3TPwKpHYVmERERCV4V1yUGuPvu0/crtmcMG3bWUyoKSFIzssk+cJzWFzTlrmFdmZgYQ59Orep2/DVrYOLEWn/76dX0GlbUu3VzPqTBKTSLiIiIVFL5jW/x5c17JaVlfLUpj5TlO08VkPzoonb89preXNuvE+FNQ8/9otWtaEdHQ2SkV+aVhqfQLCIiIlJJ5atwPHNTf1LTs5mfmcO+swpIWtT+RSutaGeOvNEJua0g/pFHzm/Iiqt+vPAC2xZt5GiLnny2IYP46JNOa58fVYf7C4VmERERkUoeHNmTfUdOcOBoMVf+OY0QA1f16cjExK6MqqGApC7O69J4NTl0iB7PPcn9v3qE9s++CHs2Qc+e9XtNqZZCs4iIiAhnF5DERF7Ao9f25tYh0XRqfe4Ckrqo16XxqkpIYMhnb8OV/SG2Lbz6GUybVv/XlbMoNIuIiEjQKjxRwgff72ZepQKSa8sLSC6rQwFJXTTIpfEq9khHRsLRo3DLLc5jZWXONZulwSk0i4iISFDxRgGJz1XeI13xRsCIw8RPn+7eTAFOoVlERESCQsHRYv61chep6dls2nuE5mGh3DCwCxOHxjA4pg3GNPyqsi806B5pqZFCs4iIiASs8yog8TMNukdaauT//6aIiIiIVFFdAcmdlzoFJBd3rmMBSSPXYPXhck4KzSIiIhIQKgpIUtN38uXG8yggETkHr4VmY8w/geuBfdba/uWPRQKpQDdgO3C7tbbAWzOIiIhI4Nux/2jDFJCInEP9rs59bq8AY6s8NhX4wlrbC/ii/HMRERGROik6Wcq7q3Zx5z+WcuWf03jx6y0MjG7NP36WwHdTr+LRa/s0rsCcnOzcvvKK0+RX4YEHYOlSSEuDSZPcmExqyWsrzdbaRcaYblUeHg+MLL//KpAG/M5bM4iIiEhgyTlSxvT31p1RQPLba+KYEB/ToAUkXjN58ukAvWABjBzp3B850gnP0mj5ek9zlLV2T/n9XCDKx8cXERERP1NRQJKSns2q7OOEhe70egGJT6xeDfn5sGsXDBvm9jTigbHWeu/FnZXmDyrtaT5orW1T6esF1tq2NTz3fuB+gKioqPiUlBSvzVmTwsJCIiIifH5c8S2d5+Cg8xwcdJ4Dh7WWLYfKWJRTwrI9JZwohQsjDMM6lDGqewsiwvwrKHd97TXKwsM5GRFBh0WL2HbvvRzt0YM2q1ZRFhZGaVgY3f/5T3aPH8+BS3UlDDf/LI8aNSrTWptQ9XFfh+ZNwEhr7R5jTGcgzVrb29PrJCQk2IyMDK/NWZO0tDRGVvzaRAKWznNw0HkODjrP/s9TAcnXX3+tcxwE3PyzbIypNjT7envGe8DdQHL57bs+Pr6IiIg0MtUVkFwS04bnbhnADQFSQFKdU/XXY+KIj632F+/SiHjzknPzcN70194YkwM8hROW3zTG3APsAG731vFFRESkcdt7uIj5mTmkpmez88CxgC4gqY7qr/2LN6+ecUcNXxrtrWOKiIhI41ZTAclvrokLugIS1V/7l8D8fYeIiIg0Kjv2H+XNjGzeynAKSDq0bMYDVzoFJN3aN6LrKfuQ6q/9i0KziIiIeEXRyVI+XZdLano2323ZT4iBq/p0ZGJiV0b17kCTUG92rIk0LIVmERERaVAbcw+TsjzbfwtIRKqh0CwiIiL1dmYByUHCQkO4pl8Udwzt6t8FJCLlFJpFRETkvFhrWZV9kJTl2by/ejfHikvp1TGCJ67vy82DLySyRZjbI4o0GIVmERERqZOqBSQXNA3lhks6M2loVwbHtMEYrSpL4FFoFhEREY/OVUBy/cDOtAxv6vaIIl6l0CwiIiI1qlpA0iq8SVAVkIhUUGgWERGRM1RXQHJZj+AsIBGpoNAsIiIiQPUFJP925UVMDOICEpEKCs0iIiJBrLoCklG9OzIxMYZRfTrSVAUkIoBCs4iISFDamHuY1HSngOTgMRWQiHii0CwiIhIkaiogmZTYlR9dpAISkXNRaBYREQlgKiARaRgKzSIiIgGopgKSiYldGdJVBSQidaXQLCIiEiDKyixLt+5nXuUCkujWKiARaQAKzSIiIn6upgKS2xNi6NtFBSQiDUGhWURExA9VV0AyrEekCkhEvEShWURExI9ULSBpH+EUkNyeEEN3FZCIeI1Cs4iISCNXXQHJyPICkqtUQCLiEwrNIiIijdSm3COkpO88VUAS3fYCfnN1HBMSounc+gK3xxMJKgrNIiIijcjREyW8X6mApGmo4Zp+nbhDBSQirlJoFhERcVlFAUlqejbvf7+bo+UFJH+47mJuGRKtAhKRRkChWURExCUHjxWzYIUKSET8gUKziIiID1UUkKSkZ/PJulyKS5wCkhk3D+CGS1RAItJYKTSLiIj4QLUFJENVQCLiLxSaRUREvKSktIy0TXmkpO/kq015lJZZhvWI5JGr4xjbXwUkIv5EoVlERKSBVVdAcv+IHiogEfFjCs0iIiINoOhkKZ+t30tq+k4W/6ACEpFAo9AsIiJSDyogEQkOCs0iIiJ1dPRECR+sdgpIVu48XUAyKTGG4Re1VwGJSABSaBYREamF6gpIeqqARCRoKDSLiIicw8FjxfxrpVNAsjHXKSC5fmBnJg2NYUjXtiogEQkSCs0iIiJVqIBERKpSaBYRESlXXQHJHYkxTEzsqgISkSCn0CwiIkHtdAFJNl9t2qcCEhGplkKziIgEpZ37j5GasfOMApL7rujBxEQVkIjI2RSaRUQkaKiARETOl0KziIgEvE25R0hNz2bByhwOHjvJhW0u4JGr45gQH02XNiogERHPXAnNxph/B+4FLLAG+Lm1tsiNWUREJDCpgEREGpLPQ7Mx5kLg10Bfa+1xY8ybwCTgFV/PIiIigaWigCRl+c6zCkhuHnwh7SKauT2iiPgpt7ZnNAEuMMacBJoDu12aQ0REAkBFAcmcxcfJ+XSxCkhEpMEZa63vD2pMEvAscBz4zFp7VzXfcz9wP0BUVFR8SkqKb4cECgsLiYiI8Plxxbd0noODznPgKbOWjQfKWJRzkoy9pZSUQdcIy6iuzRjWpQkXNFFQDkT6sxwc3DzPo0aNyrTWJlR93Oeh2RjTFngbmAgcBN4C5ltrX6vpOQkJCTYjI8M3A1aSlpbGyJEjfX5c8S2d5+Cg8xw49h0u4q3MHN7MyGbHfqeA5ObBF3J7Ygx5WSt1ngOc/iwHBzfPszGm2tDsxvaMMcA2a20egDFmAfAjoMbQLCIiwa26ApJLu0cyZUwvfty/86kCkrQslwcVkYDlRmjeCQwzxjTH2Z4xGvD9MrKIiDR6O/cf482MbN7KzGbv4dMFJLcnRNOjg35FLyK+4/PQbK1dZoyZD6wASoCVwEu+nkNERBqnEyWlfLruzAKSK+M68PSNXRl9sQpIRMQdrlw9w1r7FPCUG8cWEZHGKWvvEVKWq4BERBonNQKKiIhrjp4o4cPVe5iXvvN0AUnfTkwaqgISEWlcFJpFxLPkZJg6FZYuhR07YPVquPlmyMmB9evh+HF45hm3pxQ/Ya3l+5xDpKbv5L1VKiAREf+g0CwinpWWwqxZTmAeNAh27YKwMLjpJjhxAvr1c3lA8QcHjxXzzspdpKRnszH3CBc0DeW6gZ25QwUkIuIHFJpFxLPQUJgyxVlpLiqCv/wFvvwSBg6ENWtg4kS3J5RGqqzMsnTbflLTs/l4bS7FJWUMjG7Nszf354ZLutAqvKnbI4qI1IpCs4jUzcqV8O67MGkSFBRAZKTbE0kjVLWApGV4EyYlxjAxMYZ+XVq7PZ6ISJ0pNIuIZ1OnOrfDhsGwYWTuKGD2wiySOkH8I4+4O5s0GiWlZXyd5RSQfLmx5gISERF/pNAsInU2e2EWizbnAzD3nktdnkbcdnYBSRj3XtGdiQkxKiARkYDhMTQbY7pba7d5ekxEgkfSmLgzbiX4nCgp5bN1e0lNz+bbH/JVQCIiAa82K81vA0OqPDYfiG/4cUTEH8THttUKc5CqKCD518ocCsoLSP59TBy3JaiAREQCW42h2RjTB+gHtDbG3FLpS62AcG8PJiIijUNFAUlK+k5WVCogmZgYw/Ce7QlVAYmIBIFzrTT3Bq4H2gA3VHr8CHCfF2cSERGXWWtZnXOIlEoFJBd1aMHj4y7mliEqIBGR4FNjaLbWvgu8a4y5zFq7xIcziYiIS6oWkIQ3DeH6gV2YlBhDfKwKSEQkeNVmT/N+Y8wXQJS1tr8xZiBwo7X2j16eTUREfMBay9KtB0hJ33mqgGTAhSogERGprDah+R/Ao8D/AFhrVxtj3gAUmkVE/Ni+w0XMX5HDm+nZbK9UQHJ7Qgz9L1QBiYhIZbUJzc2ttcur/EquxEvziIiIF5WUlrFocx7zlp8uIBnaPZIkFZCIiJxTbUJzvjHmIsACGGMmAHu8OpWIiDSo7APlBSQZOeQeLlIBiYhIHdUmND8EvAT0McbsArYBP/HqVCIiUm9VC0hMeQHJ9Bv7qYBERKSOPIZma+1WYIwxpgUQYq094v2xRETkfGXtPUJqejYLVqiARESkodSmRvuRKp8DHAIyrbWrvDOWiEiQS06GqVNh6VLYuNH5SE6Gd96BTz6BF1+EL7+E1ath3TqO/u2/zyogGXNxFJOGduVyFZCIiNRbbbZnJJR/vF/++fXAauABY8xb1to/eWs4EZGgVVoKs2bBjh0wfjzk5jqP33STE6ABO2oUPxwp5bt9TfjzjC8oPFFyqoDk5iEX0l4FJCIiDaY2oTkaGGKtLQQwxjwFfAiMADIBhWYRkYYWGgpTpjgrzUVFZ3ypqLiUlMXbSEnP5pq3X+WfV0zk2n6duGOoCkhERLylNqG5I3Ci0ucncYpOjhtjTtTwHBERaShHj2IXL2b1R9/w9fIs+r/+Hu9nhdEscShjerfn3ifHqoBERMTLahOaXweWGWPeLf/8BuCN8jcGrvfaZCIiwWzqVAD29RvE/Mwc3hzxCNsXHSY0pDNXP/l3nr6qZ3kByZXuzikiEiTOGZqN8zu+V4CPgeHlDz9grc0ov3+X90YTEQlOFQUkKcuz+aJSAUmzJiFs2lvIseISNfaJiPjYOUOztdYaYz6y1g4AMs71vSIiUj81FZDcnhDDRR0iyNxRwOyFWczI+gi41Nnv/N13UFYGUVHQsiWsWgVxcXDnnW7/OCIiAaU22zNWGGMSrbXpXp9GRCTInCgp5fP1pwtIoKKApC9X9YkirMnpApL42LbMvedSeHbh6StrdOgAeXkwdCiMGAGjR8Pf/+7STyMiErhqE5ovBe4yxuwAjgIGZxF6oFcnExEJYJv3HiGlSgHJlNFxTEiI5kJPBSSVr6yxahVMmwbPPgvDh8Of/gSPPuqLH0FEJKjUJjRf6/UpRESCwLHiEj5YvYfU9GwydxTQNNRwdd8oJibWo4DkjTfg8GG4+GJ44gkoKYElS+Ba/adbRKQh1aZGeweAMaYjEO71iUREAoi1ltU5h0hJz+b973dTeKKEHvUtICm/sgbDhsG4cfDYY86q85EjTvHJtdfCxx/DypUwYADccEPD/lAiIkGoNjXaNwJ/BboA+4BYYAPQz7ujiYj4r0PHTvLOql2kpGezYc9hwpuGcN2ALkwaGkNCQxaQlJaS/eQM1i1ZQ/d77qR3mzbO48OGwXvvQWJiwxxHRCTI1WZ7xjPAMGChtXawMWYU8BPvjiUi4n+stSzdeoDU9J18tDaX4pIy+l/Yimdu6s/4QV28U0ASGsrjsaM4sq0ZiZnZ/L5d+eNt28Lf/gYvv9zwxxQRCUK1Cc0nrbX7jTEhxpgQa+1XxphZ3h5MRMRf7DtSxNuZu0hN38n2/cdoGd6EiQkxTEyM8cn1lJPGxPHBlrXc1isCFnwMa9bA8uWwZYuz4iwiIvVWm9B80BgTASwCXjfG7AMKvTuWiEjjVlpm+Tpr35kFJN0iefiqXowb0JkLwkJ9M8jUqcQD8TPucT7/+UTndsAA3xxfRCRI1CY0fw8cA/4dpwGwNRDhzaFERBqr7APHeCsjmzfLC0jatQjj3su7c3uiU0DitooClKQxccTHtnV7HBGRgFGb0DzKWlsGlAGvAhhjVnt1KhGRRqS6ApIRvTrw1A19GX3xmQUkbpu9MItFm50Z595zqcvTiIgEjhpDszHmQeCXwEVVQnJLYLG3BxMRcdvmvUdITc9mwcpdHDhaTJfW4SSN7sVtCTGeC0hckjQm7oxbERFpGOdaaX4D+Bh4Dpha6fEj1toDXp1KRMQlVQtImoQYrulXzwISHzpVtS0iIg2qxtBsrT0EHALu8N04IiK+Z61lzS6ngOS9VacLSH4/rg+3DIk+vwKShpKc7JSZLF3qFJds3Og89s478Mkn8OKLzvc99xx07w6TJrk3q4hIAKvNnmYRkYB06NhJ3v1+F/OWe7mApD5KS2HWLNixA8aPh9xc5/GbbnICNMDXXztXyyjUhY1ERLzFldBsjGkDvAz0ByzwC2vtEjdmEZHgYq1l2bYDpKZn89GaPZyoVEBy4yVdaH2BFwpI6iM0FKZMcVaai4qq/57MTDh4EAoKtNIsIuIlbq00zwY+sdZOMMaEAc1dmkNEgsTBE2X8d9oW3szIZlv+UVo2a8JtCdFMSuzqkwKSBnH0KCxe7JSXHDzo3F+8GB55BLZvd4K1iIh4hc9DszGmNTACmAxgrS0Gin09h4gEvtIyy6KsPFLSd7Jw/XFK7UaGdovkV6N6+raApD6mlr8Pu6LZ77rrTn/tiitO3+/WzfkQERGvMNZa3x7QmEHAS8B64BIgE0iy1h6t8n33A/cDREVFxaekpPh0ToDCwkIiItwvKxDv0nkOPHnHyvhmVwnf7irhQJGlZRgM7WAZ3b05XSIazzWVz9ex4lL2HS6iY6twmvtD8Pch/XkOfDrHwcHN8zxq1KhMa21C1cfdCM0JwFJguLV2mTFmNnDYWvtETc9JSEiwGRkZPpuxQlpaGiNHjvT5ccW3dJ4Dw4mSUhau30dK+s4zCkgmJcYw+uIovvt2UcCc55/NWcaizfmM6NVel5erQn+eA5/OcXBw8zwbY6oNzW7sac4Bcqy1y8o/n8+Z14EWEam1H/YdIWW5fxWQ1JcKTEREfM/nodlam2uMyTbG9LbWbgJG42zVEBGplWPFJXxYXkCSUV5AcnXfKCYmxnBFrw6NvoCkvlRgIiLie25dPeNh4PXyK2dsBX7u0hwi4iestazddZh56TsbXwFJY1K5DOXwYfjb3+Cll+Cjj5wrbsTGwq231u75S5Y4V+wYNcq5PvSOHc61oJ980mc/johIY+FKaLbWrgLO2isiIlJVRQFJyvJs1pcXkIwb0JlJiV1J7NZICkgak8plKAMHwtixzuM7d8L06XDffecOzZWfv3KlE7anTYPRo2HvXujc2Qc/hIhI46NGQBFpdKorIOnXpREXkDSUyqu8330HZWUQFQU9ejjXY46KgrvvPvdrVC5D+eUvoU8f53mjR8N//Ae09nBN6srPHz0aXn4Z2reHPXtg5kx45pmG+mlFRPyKQrOINBp5R07w9oocUtP9uICkPiqv8nboAHl5MHSoc43md9+Fjh3r9nrPP++UngwfDps2OY2C48fX/vllZc72jFtugYwM+NOfoJm2wYhIcFJoFhFXVS4g+WLDPkrKrP8VkDSUyqu8q1Y52yKefRZGjHAC69//7vk1qpahVOjUCa68sk7Pz9xRwOy8KJIiuhDvaYVbRCTAKTSLiCuyDxzjrcwc3srIZs+hItq1COMXl3fn9oQYenZUcQFvvOG8ke/ii2H+fKc6Oza2zi+TuaOA2QuzSBoTR3xs2zo9d/bCLBZtdq55rat1iEiwU2gWEZ8pLinj8/V7zyogefL6voy+OIqwJv7f1lcvlVeJFy06M/BOmHBeL1mf4KvrQYuInKbQLCJe98O+I6SmZ/P2iuApIGkIDbHSW5/gq+tBi4icptAsIl4R7AUkDcFj4K18tY19+yArC66+Gr76ynkT34gRxCckKPiKiDQAhWYRaTAVBSQp5QUkR1RAUi8eV3orX23jgw/gV7+Cpk0hMhJ274aSkroftKZyk+HD4bnnoHt3mDTpvH8mERF/pdAsIvWmAhKXVL7axkcfQVISPPUUPP208/U//vHsq2h4UlO5SUkJDBjgNAKKiAQhhWYROS/WWpZvO0BKpQKS/hcGQQFJY/XYY/DXv8KgQfDhh851lXv2rPvr1FRukpnp1HAXFGilWUSCkkKziNRJ0BeQ+FLlrRIbNzofycnw6quwbh38+MewaBF89x3bQ1vwZOTlJA0pv7TcddfV//iVy0369XOKUpYurf/rioj4IYVmEfGotMyyaHMeKctVQOJTlbdKjB8PubnO43ffDVu3Om/4y8+HqVNZNuEhFvXsAdTzmso1lZsAdOvmfIiIBCGFZhGpUU7BMd7MUAGJaypvlSgqOv340aMwZw5Mnw4vvADA8J4dGNGrfYNeU1nlJiIipyk0i8gZikvKWLhhL/OWq4CkUTl6FBYvdpoB//IXZ79yRobzRr+ZM4nuHcvcnzdssFW5iYjIaQrNIgKogKRRqtwQCKf3Kb/66pnfN2KEVw6vchMRkdMUmkWC2LHiEj5ak0vK8p0qIPETZ1Rrx7Z1exwRkaCh0CwSZFRA4t+0z1hExB0KzSJB4tDxk7y3ahfzVEDi17TPWETEHQrNIgGsooAkNT2bDysXkIzvx42DLlQBiR/SPmMREXcoNIsEoIoCkjfTs9mqAhIREZF6U2gWCRAVBSSpy7NZuGHvqQKSh1RAIiIiUm8KzSJ+TgUkIiIi3qfQLOKHKgpIUtKz+WZzHqACEhEREW9SaBbxIz/sKyQ1fScLVuxif3kBya+v6sXtiSogERER8SaFZpFG7nhxKR+u2UNq+k7St6uARERExA0KzSKN1Npdh5i3vFIBSfsWTPuxU0DSoaUKSERERHxJoVmkEakoIElJz2bd7sM0axLCdQNVQCIiIuI2hWYRl1lrSd9eQEr6Tj5as4eik2X07awCEhERkcZEoVkCR3o6fPEFlJXBFVfA66/D5MmQmwurVkFcHNx5p9tTnpJ35AQLVuSQWqmAZEK8CkhEREQaI4VmCRwLF8K0ac79BQtg5Ejn/k03wejR8Pe/uzXZKdUVkCR2a8svR/Vk3IBONA/TH0kREZHGSP+HlsAzdy4kJ8NVV8GuXZCYCH/6Ezz6qGsj5RQc463yApLdKiARERHxOwrNEjjGjIHnnoM2beCPf4TISAgPhyeegJISWLIErr3WZ+NUV0ByRa8OPKECEhEREb+j0CyBIzHR+QAydxQwe2EWSWPiiJ8xw6dj/LCvkDczsnk7M+eMApLbEqKJbtvcp7OIiIhIw1BoloA0e2EWizbnAzD3nku9frzqCkjGXBzFpKEqIBEREQkECs0SkJLGxJ1x6y1rdx0iJX0n765UAYmIiEggU2iWgBQf29ZrK8zVFpAM6MykoSogERERCVQKzSK1oAISERGR4KbQLHIO+YVOAUlKejZb85wCkluHRHPHUBWQiIiIBBPXQrMxJhTIAHZZa693aw6RqkrLLN9sziM1PZvP11cqIBmpAhIREZFg5eb//ZOADUArF2cQOWXXweO8lZHNWxk57Dp4nMgWYfx8eDcmJsbQs2NLt8cTERERF7kSmo0x0cB1wLPAI27MIAJOAUl6bgn/+8/lLKpUQPL4dRczRgUkIiIiUs6tleZZwGOAlu/EFVULSDq3PsLDV/XitvhoYiJVQCIiIiJnMtZa3x7QmOuBcdbaXxpjRgK/rW5PszHmfuB+gKioqPiUlBSfzglQWFhIRESEz48b7Fpu3EjbFSugrAxTVkaz/HyyHnF+IdH19dcp6tyZok6daLNiBWVhYeTcfnutXvdEqSUjt4Svc0rIKigj1MCgjqEMbVdCYkwLQnSpuICmP8/BQec58OkcBwc3z/OoUaMyrbUJVR93IzQ/B/wUKAHCcfY0L7DW/qSm5yQkJNiMjAwfTXhaWloaI0eO9Plxg1J6OnzxBZSVwVdfwUUXwYsvwuLF8Mc/wqRJUFQEGRmwZg1MmAAPPghTpsA//nHOl66ugGRiYsypAhKd5+Cg8xwcdJ4Dn85xcHDzPBtjqg3NPt+eYa2dBkwrH2okzkpzjYFZgsTChTBtmnPfGOdj7lwIC4Njx8BaOHoUIiLg5EnYvx9uvx127IAHHnAC9qJF8N130LEjh2+ewLbHnuLbQyH8ucdVpwpIJibGMLR7pApIREREpE70Lqdg99BDsHQpTJ7s3B44AMnJcP/9UFwMf/sb3HOPE2p9Ye5cJyCnpTkhedMmaNXKWV1+5BFo3RpiYpz5rroKfvc76NYNAPvddyyfeD8ffPE9v3vgr6St30upCeGZ8f1Y/vgYnp84iEt7tFNgFhERkTpzNTRba9N0jWYX5ebC2LGwdq0TmgEiI2HqVOjRA06cgF/9ygmpo0Z5d5YxY+C555ygfPIkNG0KAwdCv34wZAjbO3XjZ3OWsWd/IVx6KXTvDh06QLNmHD1RwkuLtvDSt9u4/X+WkLW3kMu7teHGX97Gw8Nj+GnPFmrsExERkXpRS0Mw+/BDZ5vDli3Qs+fpx7/5BmJjoWVLJ8CGhEBoqHdnSUx0PqrI3FHA7EMxHC4qYdXmfH436DbmDgyhbMYMfihuQtbqLTRfs4pPNzejb9wlvHf0W3pPvIxmE26FmTOdPdKRkd6dXURERAKeQnMwy8tzVpU//dQJmH36OCu4f/gD3HwzHDoE334L11zj2oizF2axaHM+g6JbM6JXe+68NJZZuYd5K2QYu4qOEzl4MLfek8TM6gpIkpPdGVpEREQCjkJzMJs61bm99lrno8LXX5++f911vp2piqQxcZRZy7Ae7UnffoAHX88E4PKe7fn9uIu5uq9TQJK5o4CfzVlG0pg44mPbujqziIiIBB6FZjlL5o4CZi/Mcj2A/rCvkE/X5bJhzxG+/WE/nVuH11hAUrEiDTD3nkvdGFdEREQCmEKznMXNAHq8uJSP1uwhNT2b5dsP0CTEMObiKCYOjWFErw6EhlR/5YukMXFn3IqIiIg0JIVmOUu9A2jlopLSUudNhAMGOPe3bnXeYHjffWc8pWoBSff2LZj64z7cMuRCOrYM93jI+Ni2WmEWERERr1FolrPUO4BWLiopKIDf/965MsbixTB9uhOY77uPQ8dP8t73u0lN38naXYfPLCDJ34L58i1YXOYE8BtugHHj4KOPnDA+YgQknFXWIyIiIuIVCs3iPXPnQni4U5Dy8sswYQJ21izyDheR/OYqPlqzh6KTZVzcuRX/b3w/xl9yIa2bl19POfWl08G7ZUs4fNi5HxkJu3dDSYk7P5OIiIgEJYVmaXgVRSVt2jiXtXviCQ5fEs/ny7aR98Um0pt1Z9m6vdw6JJpJiV3pf2Grmlv65s6FqCinuTA52Vm1BvjjH2HYMJ/9SCIiIhLcFJql4ZUXlZSWWb7ZnMeLX29h2coDWCBh3E+ZmBjDfw7sTPOwc/zrVzl4h4bCk0/C5Zc7hSwZGWeWsYiIiIh4mUKzNLhdB4/zVkY2b2XksOvgcZqGGgbsyeL2Q5v4SauucPwKmDXPWSnu1Alefx3atoUpU06/SA0NgYDr144WERGR4KPQLA2iuKSMLzbsJSU9m0Wb84DTBSTtIsLY+egHXDT7OYhtC48+Cl27OvXcH37oNBA+//w5X7+xXDtaREREgpNCs9TLD/sKeTMjm7czc9h/tLjGApJhCTFOYJ47F+bPh23bnC0XbWsXgFVeIiIiIm5SaJY6q66AZPTFHZmU2JURcTUUkFTeo/zXvzory7Gxzj7lZ5/1GJ5VXiIiIiJuUmiWWqtXAcm59ihPn+7x2CovERERETcpNMs5VVdAMq68gOTS7pE1XyrOA+1RFhEREX+i0CxnsdaSvr2AlPSd5y4gqQftURYRERF/otAspxy//wHeu+IW0jK2MiLtX3x20yO89eUsIn88mi6/fvS8V5Wroz3KIiIi4k8UmoNcaZkl96f3MK/7ZfRYspnYT35D/ytvYIwpYMLhT2ka0xaaNVxYrqA9yiIiIuJPQtweQNyx6+BxZi3M4qYn3ubJE9HEffAmrRMGkXhkFw/1b02Hgr003ZsLf/87xMXB2rVujywiIiLiGq00B5HqCkge37OS4TGG3jsPEXLj5dC3E1x2GWRmwq9/DUlJ0Lo1zJx57hdPT4cvvoCyMjh50nkOOC1/8+bBjh0wdap3f0ARERERL1FoDgJb8gp5Mz2bt1fkkF9YTKdW4Tw8qie3JcQQ89IaJ8zO7uCsKl93HSs69uTgvhL6vvIGnaZPh4su8nyQhQth2jTn/ksvOeUlAwbAunXQubMTmkVERET8lEJzgKp1AUnF6m9SkvMBzJqzjEXD7mFEr/bMrU1grmzuXMjNdYpMnnkGjh2DwkJYvLgBfzoRERER31JoDjBrdx0iNT2bd1bt4khRCd3aNed3Y/twa3wNBSSVt1W8+y4HOnRhTEgUD+zcSJ91kbCmq9Pcd/Cg09o3ZUr1B67c+BcR4bT+RUXBvfc6Xy8q8tJPLCIiIuJ9Cs0B4HDRSd5dVccCkoqw/NVXUFICV14JP/85e194iV1d2jDAlhA5fChMmgRPPglz5jjV1zWp0vh3qrxkR4FTXuKL/cyV/wJQWgq7dsGLL8I778DWrdCyJdx3n/fnEBERkYCj0OynrLVk7Chg3vLTBSR9OrXk6Rv7cdOgWhSQVOxBttZZGX7lFejShZhwuPrgVjoPG1yv+VwpL6m8rxogOdm5XbXKqeq+7z6FZhERETkvCs1+Jr/wBAtW5JCSns3WvKNENGvCLUOimZQYw4ALW9e9gOTECVi9GqKjobCQiNJiEsKOwZYNsHs7bNwIhw/DH/4AXbrU+mVdLS+ZOxfCK21FmTABZs92/oIgIiIich4Umv1AaZnl2x/ySVm+k8/X76WkzBIf25Y/TbiI6wd2pnnYeZzGij3IUVHQqxds2ACXX+68aW/NGrZHtOfJyKEkjYlztlfUkSvlJZX3VW/bBhkZzhsQW7RwLoM3frxv5xEREZGAodDciO06eJy3MrJ5KyOHXQeP07Z5Uyb/qBsTE2PoFdWyfi9eZQ/yGSZMYMp/LWbV5nwOF5XwzkPD63csX6jYz2wt5OdD06bs6d6b321sQtKYWOJ/+1u3JxQRERE/ptDcyBSXlPHlxr3MW+4UkFgLV/Rqz7Rxfbi6bxTNmoR65bin3rhXsbJcsZXBX7Y0VN3PnJrKf5R08/2+ahEREQlICs2NxDkLSCKbe/34Z7xxb2AIf8v+nCWb9zH8RGuYleF806RJTnHJRRfBXXd5b5jKV8H44gu44QYYN86p8/akYj/zmjVMuO837Cr/i4CIiIhIfSg0u+h4cSkfr91DynKngCQ0xDC6T0cmDY1hRK8ONAkN8dksZ7xx740XiZ75NLfBme1+b73l1GOHeHmuyqvGLVs6b0T0pPJ+5hYtIDLSnX3VIiIiEpAUml1Q5wISH6g2YFZt92vRAq6/Hl57zTdDzZ3rvFHxoYecy8f9/vc1f++59miLiIiI1JNCs48cLjrJe6t2k1JeQBLWJIRx/TsxaWjXmgtI3JKZCf/+704hSFkZ/PnPsGYNdO0K//d/EOqdfdWnVF41Dg11ylUuv7x2o1fdmy0iIiLSABSavaiigCRleTYfrtld9wISN+Tmwt13O1syTp6Enj3JvPEnzI4cwYysj4h+5mnvz1CPVWNXSlVEREQk4Ck0e0HVApIWYaHcPDiaO4aeZwGJL334IezfD59/DrfdBp07894/32dRSUe++yGf2308Tl1Xjl0tVREREZGApdDcQCoKSFLTnQKSk6WnC0iuG9CZFs385B91Xh5MnQqDBjnbMgYN4qaf/Yqo/5zLNYe2wI4dEBvrs3HqunKsN/+JiIiIN/hJkmu8dh88zptVCkh+dlk3JjVEAYkbpk51bq+5Bq655vRK7+O/ok3sEz4fRyvHIiIi0hgoNJ+Hk6VlfLFhLynp2Xyd5bsCEje4vUdYK8ciIiLSGPg8NBtjYoC5QBRggZestbN9Pcf52JpXSGqlApKoVs341aie3O6jAhI3aKVXRERExJ2V5hLgN9baFcaYlkCmMeZza+16F2bx6FQBSXo2y7e5W0DiBq30ioiIiLgQmq21e4A95fePGGM2ABcCjSo0b8krZO76EzyctpAjRSXEtmvOY2N7M2FINB1buVNAIiIiIiLuMNZa9w5uTDdgEdDfWnu4ytfuB+4HiIqKik9JSfHpbMtzS3jp+yISOzVhRHRTekeGENKYLxUn562wsJCIiAi3xxAv03kODjrPgU/nODi4eZ5HjRqVaa1NqPq4a6HZGBMBfA08a61dcK7vTUhIsBkZGb4ZrFxxSRmfffk1118zyqfHFd9LS0tj5MiRbo8hXub35zk52bm6zdKlsHw5bN4MDz4Ib77ptGeOGwdxeu+B359n8UjnODi4eZ6NMdWGZleunmGMaQq8DbzuKTC7JaxJCBFhWlkWkUaitBRmzXKulT5+PPTvD3v2QLt2cPiwx6eLiEj9+PxdbMapw5sDbLDWPu/r44uI+KXQUJgyBSZOhL17YfFiGD0aHn4Ypk2D+fPdnlBEJKC5cemH4cBPgauMMavKP8a5MIeIiH+6+2644AJYtw7mzYMnn4QhQ9yeSkQkoLlx9YxvAe17EBGpi4q2zmHDyNy0x2nqjOhC/B393J1LRCRIBPZFhkVEAlBFU+fshVlujyIiEjRUoy0i4mfU1Cki4nsKzSIifkZNnSIivqftGSIiIiIiHig0i4iIiIh4oNAsIiIiIuKBQrOIiIiIiAcKzSIiIiIiHig0i4iIiIh4oNAsIiIiIuKBQrOIiIiIiAcKzSIiIiIiHig0i4iIiIh4oNAsIiIiIuKBQrOIiIiIiAcKzSIiIiIiHig0i4iIiIh4oNAsIiIiIuKBQrOIiIiIiAcKzSIiIiIiHig0i4iIiIh4oNAs8NBDsHQpTJ7s3AJs3Qq33AJlZTBjBvzbv0FenqtjioiIiLhFoTnY5ebC2LGwdq0TmsEJyp9+CkOHQkgI/P73cNllcPCgm5OKiIiIuKaJ2wOIyz78EPbvhy1boGdP57FNm2DvXsjMhPXrneB85Aj06uXurCIiIiIu0UpzsMvLg8cec7ZizJwJqakQFwfTp8Pw4U5QvvdeKC2F7Gy3pxURERFxhVaag93Uqc7ttdc6H0DmjgJmL8wi6Y5/I75pU/j2WxcHFBEREXGfVprlLLMXZrFocz6zF2a5PYqIiIhIo6CVZjlL0pi4M25FREREgp1Cs5wlPrYtc++51O0xRERERBoNbc8QEREREfFAoVlERERExAOFZhERERERDxSaRUREREQ8UGgWEREREfFAoVlERERExAOFZhERERERDxSaRUREREQ8UGgWEREREfFAoVlERERExAOFZhERERERDxSaRUREREQ8MNZat2fwyBiTB+xw4dDtgXwXjiu+pfMcHHSeg4POc+DTOQ4Obp7nWGtth6oP+kVodosxJsNam+D2HOJdOs/BQec5OOg8Bz6d4+DQGM+ztmeIiIiIiHig0CwiIiIi4oFC87m95PYA4hM6z8FB5zk46DwHPp3j4NDozrP2NIuIiIiIeKCVZhERERERDxSaq2GMiTHGfGWMWW+MWWeMSXJ7JvEOY0yoMWalMeYDt2cR7zDGtDHGzDfGbDTGbDDGXOb2TNLwjDH/Xv7f67XGmHnGmHC3Z5L6M8b80xizzxizttJjkcaYz40xm8tv27o5o9RfDef5z+X/3V5tjPmXMaaNiyMCCs01KQF+Y63tCwwDHjLG9HV5JvGOJGCD20OIV80GPrHW9gEuQec74BhjLgR+DSRYa/sDocAkd6eSBvIKMLbKY1OBL6y1vYAvyj8X//YKZ5/nz4H+1tqBQBYwzddDVaXQXA1r7R5r7Yry+0dw/id7obtTSUMzxkQD1wEvuz2LeIcxpjUwApgDYK0tttYedHUo8ZYmwAXGmCZAc2C3y/NIA7DWLgIOVHl4PPBq+f1XgZt8OZM0vOrOs7X2M2ttSfmnS4Fonw9WhUKzB8aYbsBgYJnLo0jDmwU8BpS5PId4T3cgD/jf8m04LxtjWrg9lDQsa+0u4C/ATmAPcMha+5m7U4kXRVlr95TfzwWi3BxGfOIXwMduD6HQfA7GmAjgbWCKtfaw2/NIwzHGXA/ss9Zmuj2LeFUTYAjw39bawcBR9KvcgFO+p3U8zl+SugAtjDE/cXcq8QXrXAJMlwELYMaYx3G2zb7u9iwKzTUwxjTFCcyvW2sXuD2PNLjhwI3GmO1ACnCVMeY1d0cSL8gBcqy1Fb8pmo8ToiWwjAG2WWvzrLUngQXAj1yeSbxnrzGmM0D57T6X5xEvMcZMBq4H7rKN4BrJCs3VMMYYnD2QG6y1z7s9jzQ8a+00a220tbYbzhuGvrTWamUqwFhrc4FsY0zv8odGA+tdHEm8YycwzBjTvPy/36PRGz4D2XvA3eX37wbedXEW8RJjzFicLZQ3WmuPuT0PKDTXZDjwU5zVx1XlH+PcHkpEzsvDwOvGmNXAIGCGu+NIQyv/TcJ8YAWwBuf/bY2uTUzqzhgzD1gC9DbG5Bhj7gGSgauNMZtxfsuQ7OaMUn81nOe/AS2Bz8tz2IuuDokaAUVEREREPNJKs4iIiIiIBwrNIiIiIiIeKDSLiIiIiHig0CwiIiIi4oFCs4iIiIiIBwrNIiJByhgz0hjzgdtziIj4A4VmEZEAY4wJdXsGEZFAo9AsIuJHjDHdjDEbjTGvG2M2GGPmlzfhbTfGzDTGrABuM8ZcY4xZYoxZYYx5yxgTUf78seXPXwHc4u5PIyLiPxSaRUT8T2/g79bai4HDwC/LH99vrR0CLAT+AIwp/zwDeMQYEw78A7gBiAc6+XxyERE/pdAsIuJ/sq21i8vvvwZcXn4/tfx2GNAXWGyMWQXcDcQCfYBt1trN1qmDfc13I4uI+Lcmbg8gIiJ1Zmv4/Gj5rQE+t9beUfmbjDGDvDyXiEjA0kqziIj/6WqMuaz8/p3At1W+vhQYbozpCWCMaWGMiQM2At2MMReVf98diIhIrSg0i4j4n03AQ8aYDUBb4L8rf9FamwdMBuYZY1YDS4A+1toi4H7gw/I3Au7z6dQiIn7MONvaRETEHxhjugEfWGv7uz2LiEgw0UqziIiIiIgHWmkWEREREfFAK80iIiIiIh4oNIuIiIiIeKDQLCIiIiLigUKziIiIiIgHCs0iIiIiIh4oNIuIiIiIePD/AUqITr52YzN/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103l\n",
      "102m\n",
      "102l\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104l\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "#x = list(range(len(outs_val[\"111m\"])))  # X values (index)\n",
    "#y = list(outs_val[\"111m\"].values())  # Y values (numeric data)\n",
    "for pd in fails:\n",
    "    x,y=list(outs_val[pd].values()), pyp[1:-1]\n",
    "    if len(x)==len(y):\n",
    "    \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        x,y=list(outs_val[pd].values()), pyp[1:-1]\n",
    "        # Scatter plot with small points\n",
    "        plt.scatter(x,y, s=2, label=pd)  # `s=5` makes points smaller\n",
    "        labels=[k[0]+k[3:-1] for k in outs_val[pd].keys()]\n",
    "        # Annotate each point with only the first letter of its value\n",
    "        for i in range(len(x)):\n",
    "            label = labels[i] # First letter of the value\n",
    "            plt.annotate(label, (x[i], y[i]), textcoords=\"offset points\", xytext=(0, 2),\n",
    "                        ha=\"center\", fontsize=5, color=\"red\")\n",
    "\n",
    "        plt.plot(range(2,13),range(2,13))\n",
    "\n",
    "        plt.xlabel(\"pred\")\n",
    "        plt.ylabel(\"target\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10gs': {'TYR3A': 12.241243,\n",
       "  'TYR7A': 9.992758,\n",
       "  'CYS14A': 1.8577654,\n",
       "  'ASP23A': 2.6118472,\n",
       "  'LYS29A': 8.288166,\n",
       "  'GLU30A': 2.0764036,\n",
       "  'GLU31A': 2.6264453,\n",
       "  'GLU36A': 3.082324,\n",
       "  'GLU40A': 4.1402225,\n",
       "  'LYS44A': 10.451363,\n",
       "  'CYS47A': 5.720317,\n",
       "  'TYR49A': 13.57588,\n",
       "  'LYS54A': 14.046604,\n",
       "  'ASP57A': 7.3318458,\n",
       "  'ASP59A': 5.673253,\n",
       "  'TYR63A': 9.055871,\n",
       "  'HIS71A': 4.877515,\n",
       "  'TYR79A': 8.898472,\n",
       "  'LYS81A': 10.590946,\n",
       "  'ASP82A': 4.49314,\n",
       "  'GLU85A': 4.016248,\n",
       "  'ASP90A': 2.5784245,\n",
       "  'ASP94A': 1.9138052,\n",
       "  'GLU97A': 1.5825927,\n",
       "  'ASP98A': 0.81982064,\n",
       "  'CYS101A': 1.0380249,\n",
       "  'LYS102A': 8.420016,\n",
       "  'TYR103A': 10.698648,\n",
       "  'TYR108A': 12.345486,\n",
       "  'TYR111A': 8.74176,\n",
       "  'GLU112A': 2.2069817,\n",
       "  'LYS115A': 10.014381,\n",
       "  'ASP116A': 2.64243,\n",
       "  'ASP117A': 3.3464024,\n",
       "  'TYR118A': 12.173979,\n",
       "  'LYS120A': 11.096049,\n",
       "  'LYS127A': 9.999535,\n",
       "  'GLU130A': 3.4181526,\n",
       "  'LYS140A': 9.406173,\n",
       "  'ASP146A': 3.1580005,\n",
       "  'ASP152A': 2.9361567,\n",
       "  'TYR153A': 10.984007,\n",
       "  'ASP157A': 3.4372072,\n",
       "  'HIS162A': 6.224125,\n",
       "  'GLU163A': 2.9615812,\n",
       "  'CYS169A': 2.94318,\n",
       "  'ASP171A': 4.437922,\n",
       "  'TYR179A': 11.214672,\n",
       "  'LYS188A': 13.531277,\n",
       "  'LYS190A': 13.97793,\n",
       "  'GLU197A': 4.3260627,\n",
       "  'TYR198A': 10.398311,\n",
       "  'LYS208A': 10.516354,\n",
       "  'TYR3B': 11.543524,\n",
       "  'TYR7B': 11.359813,\n",
       "  'CYS14B': 3.614863,\n",
       "  'ASP23B': 3.5242832,\n",
       "  'LYS29B': 9.579666,\n",
       "  'GLU30B': 3.5062814,\n",
       "  'GLU31B': 3.0188975,\n",
       "  'GLU36B': 3.2397783,\n",
       "  'GLU40B': 3.7352777,\n",
       "  'LYS44B': 10.911482,\n",
       "  'CYS47B': 4.657178,\n",
       "  'TYR49B': 11.155127,\n",
       "  'LYS54B': 12.41659,\n",
       "  'ASP57B': 6.120713,\n",
       "  'ASP59B': 5.2386823,\n",
       "  'TYR63B': 9.748354,\n",
       "  'HIS71B': 4.84117,\n",
       "  'TYR79B': 9.140552,\n",
       "  'LYS81B': 10.813648,\n",
       "  'ASP82B': 4.7819633,\n",
       "  'GLU85B': 4.3009963,\n",
       "  'ASP90B': 3.100587,\n",
       "  'ASP94B': 2.2034872,\n",
       "  'GLU97B': 1.917891,\n",
       "  'ASP98B': 1.340998,\n",
       "  'CYS101B': 1.5236659,\n",
       "  'LYS102B': 9.33023,\n",
       "  'TYR103B': 11.893078,\n",
       "  'TYR108B': 11.112162,\n",
       "  'TYR111B': 13.163498,\n",
       "  'GLU112B': 3.479641,\n",
       "  'LYS115B': 11.273527,\n",
       "  'ASP116B': 2.7938914,\n",
       "  'ASP117B': 3.6390955,\n",
       "  'TYR118B': 10.190943,\n",
       "  'LYS120B': 11.029022,\n",
       "  'LYS127B': 10.60573,\n",
       "  'GLU130B': 2.713842,\n",
       "  'LYS140B': 10.199488,\n",
       "  'ASP146B': 2.7602303,\n",
       "  'ASP152B': 3.1561737,\n",
       "  'TYR153B': 8.1972065,\n",
       "  'ASP157B': 3.2371383,\n",
       "  'HIS162B': 5.6298866,\n",
       "  'GLU163B': 3.6714625,\n",
       "  'CYS169B': 4.3575306,\n",
       "  'ASP171B': 5.1051645,\n",
       "  'TYR179B': 13.330185,\n",
       "  'LYS188B': 14.011957,\n",
       "  'LYS190B': 13.961109,\n",
       "  'GLU197B': 4.2489424,\n",
       "  'TYR198B': 11.749058,\n",
       "  'LYS208B': 10.748291},\n",
       " '109l': {'GLU5A': 3.5531683,\n",
       "  'ASP10A': 3.262599,\n",
       "  'GLU11A': 2.9230833,\n",
       "  'LYS16A': 9.880344,\n",
       "  'TYR18A': 10.990821,\n",
       "  'LYS19A': 12.051921,\n",
       "  'ASP20A': 3.8796124,\n",
       "  'GLU22A': 2.9889717,\n",
       "  'TYR24A': 10.000607,\n",
       "  'TYR25A': 9.591951,\n",
       "  'HIS31A': 4.895872,\n",
       "  'LYS35A': 9.897608,\n",
       "  'LYS43A': 11.21789,\n",
       "  'LYS44A': 11.99139,\n",
       "  'GLU45A': 2.954733,\n",
       "  'ASP47A': 2.6619234,\n",
       "  'LYS48A': 10.220914,\n",
       "  'LYS60A': 11.077299,\n",
       "  'ASP61A': 3.265019,\n",
       "  'GLU62A': 3.2216935,\n",
       "  'GLU64A': 3.4355793,\n",
       "  'LYS65A': 10.982117,\n",
       "  'ASP70A': 3.5564616,\n",
       "  'ASP72A': 2.5715084,\n",
       "  'LYS83A': 9.628987,\n",
       "  'LYS85A': 11.185417,\n",
       "  'TYR88A': 11.458182,\n",
       "  'ASP89A': 2.9450088,\n",
       "  'ASP92A': 2.3887947,\n",
       "  'GLU108A': 2.1787522,\n",
       "  'LYS124A': 9.039495,\n",
       "  'ASP127A': 2.4609728,\n",
       "  'GLU128A': 3.7928677,\n",
       "  'LYS135A': 11.228176,\n",
       "  'TYR139A': 11.147208,\n",
       "  'LYS147A': 11.205528,\n",
       "  'ASP159A': 3.9999747,\n",
       "  'TYR161A': 9.627211,\n",
       "  'LYS162A': 10.750711},\n",
       " '110l': {'GLU5A': 4.1787043,\n",
       "  'ASP10A': 3.260923,\n",
       "  'GLU11A': 3.1486971,\n",
       "  'LYS16A': 10.620965,\n",
       "  'TYR18A': 10.125333,\n",
       "  'LYS19A': 11.067547,\n",
       "  'ASP20A': 3.0075178,\n",
       "  'GLU22A': 4.2856684,\n",
       "  'TYR24A': 8.421765,\n",
       "  'TYR25A': 9.517889,\n",
       "  'HIS31A': 5.2999763,\n",
       "  'LYS35A': 7.687213,\n",
       "  'LYS43A': 9.996749,\n",
       "  'GLU45A': 3.6452625,\n",
       "  'ASP47A': 2.9282267,\n",
       "  'LYS48A': 12.343403,\n",
       "  'LYS60A': 11.992918,\n",
       "  'ASP61A': 2.9873624,\n",
       "  'GLU62A': 3.2552385,\n",
       "  'GLU64A': 2.9354389,\n",
       "  'LYS65A': 10.442112,\n",
       "  'ASP70A': 2.9406161,\n",
       "  'ASP72A': 2.887074,\n",
       "  'LYS83A': 10.947594,\n",
       "  'LYS85A': 7.756858,\n",
       "  'TYR88A': 11.423654,\n",
       "  'ASP89A': 2.8750281,\n",
       "  'ASP92A': 2.1562028,\n",
       "  'GLU108A': 2.1802855,\n",
       "  'LYS124A': 9.015644,\n",
       "  'ASP127A': 2.165702,\n",
       "  'GLU128A': 4.155978,\n",
       "  'LYS135A': 10.598229,\n",
       "  'TYR139A': 10.32727,\n",
       "  'LYS147A': 10.04442,\n",
       "  'ASP159A': 3.7956824,\n",
       "  'TYR161A': 11.9128685,\n",
       "  'LYS162A': 11.830853},\n",
       " '111l': {'GLU5A': 4.3437996,\n",
       "  'ASP10A': 2.8665628,\n",
       "  'GLU11A': 2.858057,\n",
       "  'LYS16A': 10.454982,\n",
       "  'TYR18A': 10.17758,\n",
       "  'LYS19A': 11.190891,\n",
       "  'ASP20A': 2.756143,\n",
       "  'GLU22A': 4.385288,\n",
       "  'TYR24A': 11.405214,\n",
       "  'TYR25A': 9.873009,\n",
       "  'HIS31A': 6.3233843,\n",
       "  'LYS35A': 8.038581,\n",
       "  'LYS43A': 9.641951,\n",
       "  'GLU45A': 3.253737,\n",
       "  'ASP47A': 3.251038,\n",
       "  'LYS48A': 10.856615,\n",
       "  'LYS60A': 11.061756,\n",
       "  'ASP61A': 2.2660656,\n",
       "  'GLU62A': 2.587081,\n",
       "  'GLU64A': 2.6487503,\n",
       "  'LYS65A': 10.579777,\n",
       "  'ASP70A': 3.398879,\n",
       "  'ASP72A': 2.856484,\n",
       "  'LYS83A': 10.894046,\n",
       "  'LYS85A': 10.253717,\n",
       "  'TYR88A': 10.701141,\n",
       "  'ASP89A': 2.1939187,\n",
       "  'ASP92A': 2.0846102,\n",
       "  'GLU108A': 2.5178292,\n",
       "  'LYS124A': 9.127045,\n",
       "  'ASP127A': 2.1996322,\n",
       "  'GLU128A': 4.0307236,\n",
       "  'LYS135A': 11.168895,\n",
       "  'TYR139A': 9.411969,\n",
       "  'LYS147A': 9.704611,\n",
       "  'ASP159A': 3.3321242,\n",
       "  'TYR161A': 10.3494625,\n",
       "  'LYS162A': 11.301232},\n",
       " '108l': {'GLU5A': 4.3688455,\n",
       "  'ASP10A': 3.1767316,\n",
       "  'GLU11A': 2.7703214,\n",
       "  'LYS16A': 11.003742,\n",
       "  'TYR18A': 10.137115,\n",
       "  'LYS19A': 10.934496,\n",
       "  'ASP20A': 2.582111,\n",
       "  'GLU22A': 3.9662404,\n",
       "  'TYR24A': 10.870109,\n",
       "  'TYR25A': 9.213373,\n",
       "  'HIS31A': 7.132431,\n",
       "  'LYS35A': 9.456718,\n",
       "  'LYS43A': 9.856498,\n",
       "  'GLU45A': 3.0737686,\n",
       "  'ASP47A': 2.7810273,\n",
       "  'LYS48A': 12.095861,\n",
       "  'LYS60A': 11.152367,\n",
       "  'ASP61A': 2.6475575,\n",
       "  'GLU62A': 3.0544152,\n",
       "  'GLU64A': 2.8747406,\n",
       "  'LYS65A': 9.82811,\n",
       "  'ASP70A': 3.0459113,\n",
       "  'ASP72A': 2.947054,\n",
       "  'LYS83A': 10.073019,\n",
       "  'LYS85A': 10.593998,\n",
       "  'TYR88A': 11.830038,\n",
       "  'ASP89A': 2.7409716,\n",
       "  'ASP92A': 2.3259258,\n",
       "  'GLU108A': 2.351044,\n",
       "  'LYS124A': 9.79747,\n",
       "  'ASP127A': 2.265715,\n",
       "  'GLU128A': 4.022548,\n",
       "  'LYS135A': 11.201261,\n",
       "  'TYR139A': 9.34231,\n",
       "  'LYS147A': 10.1143465,\n",
       "  'ASP159A': 3.4076312,\n",
       "  'TYR161A': 9.953235,\n",
       "  'LYS162A': 12.560711},\n",
       " '104m': {'GLU4A': 4.482956,\n",
       "  'GLU6A': 3.6031404,\n",
       "  'HIS12A': 4.76799,\n",
       "  'LYS16A': 9.203559,\n",
       "  'GLU18A': 1.8857386,\n",
       "  'ASP20A': 1.8742177,\n",
       "  'HIS24A': 7.4907513,\n",
       "  'ASP27A': 1.7427742,\n",
       "  'LYS34A': 8.382457,\n",
       "  'HIS36A': 7.0900693,\n",
       "  'GLU38A': 3.079205,\n",
       "  'GLU41A': 4.649328,\n",
       "  'LYS42A': 11.9244585,\n",
       "  'ASP44A': 2.9853806,\n",
       "  'LYS47A': 10.771619,\n",
       "  'HIS48A': 6.870656,\n",
       "  'LYS50A': 9.392281,\n",
       "  'GLU52A': 2.3559356,\n",
       "  'GLU54A': 3.0251637,\n",
       "  'LYS56A': 10.369518,\n",
       "  'GLU59A': 3.9920201,\n",
       "  'ASP60A': 3.1151893,\n",
       "  'LYS62A': 11.985996,\n",
       "  'LYS63A': 10.135529,\n",
       "  'HIS64A': 6.019649,\n",
       "  'LYS77A': 10.8104,\n",
       "  'LYS78A': 11.308865,\n",
       "  'LYS79A': 13.33535,\n",
       "  'HIS81A': 7.6008787,\n",
       "  'HIS82A': 5.860424,\n",
       "  'GLU83A': 3.0878592,\n",
       "  'GLU85A': 2.893744,\n",
       "  'LYS87A': 10.334574,\n",
       "  'HIS93A': 5.1846056,\n",
       "  'LYS96A': 10.804533,\n",
       "  'HIS97A': 4.8989706,\n",
       "  'LYS98A': 10.931896,\n",
       "  'LYS102A': 11.6073475,\n",
       "  'TYR103A': 10.909475,\n",
       "  'GLU105A': 3.8273535,\n",
       "  'GLU109A': 3.2115612,\n",
       "  'HIS113A': 5.393795,\n",
       "  'HIS116A': 5.7932324,\n",
       "  'HIS119A': 6.888796,\n",
       "  'ASP122A': 3.0551052,\n",
       "  'ASP126A': 2.9155319,\n",
       "  'LYS133A': 10.162575,\n",
       "  'GLU136A': 4.702079,\n",
       "  'LYS140A': 11.402781,\n",
       "  'ASP141A': 2.2656097,\n",
       "  'LYS145A': 8.718934,\n",
       "  'TYR146A': 9.988386,\n",
       "  'LYS147A': 10.684291,\n",
       "  'GLU148A': 3.610683,\n",
       "  'TYR151A': 11.871313},\n",
       " '105m': {'GLU4A': 3.1304922,\n",
       "  'GLU6A': 2.1030998,\n",
       "  'HIS12A': 5.361251,\n",
       "  'LYS16A': 9.912737,\n",
       "  'GLU18A': 2.9909472,\n",
       "  'ASP20A': 2.4146657,\n",
       "  'HIS24A': 7.4204984,\n",
       "  'ASP27A': 2.0946589,\n",
       "  'LYS34A': 10.330282,\n",
       "  'HIS36A': 4.163971,\n",
       "  'GLU38A': 2.623155,\n",
       "  'GLU41A': 3.6904798,\n",
       "  'LYS42A': 11.891628,\n",
       "  'ASP44A': 3.5777893,\n",
       "  'LYS47A': 11.391394,\n",
       "  'HIS48A': 6.8593855,\n",
       "  'LYS50A': 10.97971,\n",
       "  'GLU52A': 4.5464864,\n",
       "  'GLU54A': 3.336511,\n",
       "  'LYS56A': 9.709288,\n",
       "  'GLU59A': 2.2768545,\n",
       "  'ASP60A': 2.154678,\n",
       "  'LYS62A': 11.214675,\n",
       "  'LYS63A': 11.223039,\n",
       "  'HIS64A': 5.4940863,\n",
       "  'LYS77A': 10.891301,\n",
       "  'LYS78A': 10.728727,\n",
       "  'LYS79A': 11.721642,\n",
       "  'HIS81A': 6.392565,\n",
       "  'HIS82A': 6.8526344,\n",
       "  'GLU83A': 2.6606793,\n",
       "  'GLU85A': 3.2135005,\n",
       "  'LYS87A': 11.722763,\n",
       "  'HIS93A': 4.749173,\n",
       "  'LYS96A': 9.181177,\n",
       "  'HIS97A': 4.7380505,\n",
       "  'LYS98A': 10.5619755,\n",
       "  'LYS102A': 11.581802,\n",
       "  'TYR103A': 10.340225,\n",
       "  'GLU105A': 3.1893792,\n",
       "  'GLU109A': 3.4096427,\n",
       "  'HIS113A': 6.2532005,\n",
       "  'HIS116A': 5.8867464,\n",
       "  'HIS119A': 6.0079966,\n",
       "  'ASP122A': 3.1944947,\n",
       "  'ASP126A': 3.3493283,\n",
       "  'LYS133A': 11.623075,\n",
       "  'GLU136A': 4.1574173,\n",
       "  'LYS140A': 10.649149,\n",
       "  'ASP141A': 1.8385668,\n",
       "  'LYS145A': 8.477755,\n",
       "  'TYR146A': 8.858945,\n",
       "  'LYS147A': 10.453159,\n",
       "  'GLU148A': 3.9552639,\n",
       "  'TYR151A': 13.19767},\n",
       " '107l': {'GLU5A': 3.646733,\n",
       "  'ASP10A': 2.2969365,\n",
       "  'GLU11A': 2.621711,\n",
       "  'LYS16A': 8.764441,\n",
       "  'TYR18A': 11.032301,\n",
       "  'LYS19A': 12.77204,\n",
       "  'ASP20A': 2.9310715,\n",
       "  'GLU22A': 2.6109514,\n",
       "  'TYR24A': 9.924685,\n",
       "  'TYR25A': 10.174028,\n",
       "  'HIS31A': 5.12231,\n",
       "  'LYS35A': 10.130886,\n",
       "  'LYS43A': 11.156216,\n",
       "  'GLU45A': 4.103751,\n",
       "  'ASP47A': 3.0272717,\n",
       "  'LYS48A': 12.257852,\n",
       "  'LYS60A': 11.085534,\n",
       "  'ASP61A': 1.8704406,\n",
       "  'GLU62A': 2.1813374,\n",
       "  'GLU64A': 2.761286,\n",
       "  'LYS65A': 9.833454,\n",
       "  'ASP70A': 4.453958,\n",
       "  'ASP72A': 4.0660024,\n",
       "  'LYS83A': 10.015431,\n",
       "  'LYS85A': 10.663908,\n",
       "  'TYR88A': 12.04438,\n",
       "  'ASP89A': 2.4274778,\n",
       "  'ASP92A': 2.0504284,\n",
       "  'GLU108A': 2.300839,\n",
       "  'LYS124A': 9.26157,\n",
       "  'ASP127A': 2.635361,\n",
       "  'GLU128A': 4.1312876,\n",
       "  'LYS135A': 8.080824,\n",
       "  'TYR139A': 9.855326,\n",
       "  'LYS147A': 9.629097,\n",
       "  'ASP159A': 3.6017947,\n",
       "  'TYR161A': 10.102335,\n",
       "  'LYS162A': 11.468702}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GLU5A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'GLU7A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'HIS13A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.]),\n",
       " 'LYS17A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'GLU19A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'ASP21A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.]),\n",
       " 'HIS25A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.]),\n",
       " 'ASP28A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.]),\n",
       " 'LYS35A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'HIS37A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.]),\n",
       " 'GLU39A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'GLU42A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'LYS43A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'ASP45A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.]),\n",
       " 'LYS48A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'HIS49A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.]),\n",
       " 'LYS51A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'GLU53A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'GLU55A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'LYS57A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'GLU60A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'ASP61A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.]),\n",
       " 'LYS63A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'LYS64A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'HIS65A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  1.,\n",
       "          0.,  1.,  0.]),\n",
       " 'LYS78A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'LYS79A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'LYS80A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'HIS82A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.]),\n",
       " 'HIS83A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.]),\n",
       " 'GLU84A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'GLU86A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'LYS88A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'HIS94A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  1.,\n",
       "          0.,  1.,  0.]),\n",
       " 'LYS97A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'HIS98A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.]),\n",
       " 'LYS99A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'LYS103A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'TYR104A': tensor([ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'GLU106A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'GLU110A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'HIS114A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.]),\n",
       " 'HIS117A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.]),\n",
       " 'HIS120A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  1.,\n",
       "          0.,  1.,  0.]),\n",
       " 'ASP127A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.]),\n",
       " 'LYS134A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'GLU137A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'LYS141A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'ASP142A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.]),\n",
       " 'LYS146A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'TYR147A': tensor([ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " 'LYS148A': tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.]),\n",
       " 'GLU149A': tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
       "         -1.]),\n",
       " 'TYR152A': tensor([ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails\n",
    "len(all_targets.keys())\n",
    "fails\n",
    "all_data[\"111m\"][\"ions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = EGNN_Network(\n",
    "        num_tokens = 6, #vocabulary siye, number of unique species\n",
    "        num_positions = 23,  #number of nodes         # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "        dim = 2,# #internal rep size. c has square dependence. richer rep but overfitting for small d.s.\n",
    "        depth = 2, #number of layers #deeper need more memort to store intermediate reps\n",
    "        num_nearest_neighbors = 2, #number of nearest neighbors to consider #make this the max hood size\n",
    "        dropout=.05,\n",
    "        m_pool_method='mean')\n",
    "    \n",
    "#net, optimizer = model(23, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "\n",
    "optimizer3 = torch.optim.Adam(pnet.parameters(), lr=.01, weight_decay=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10gs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0008, grad_fn=<DivBackward0>)\n",
      "fake TYR3A 0 10gs tensor(7.7298, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 1 10gs tensor(7.3296, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 2 10gs tensor(6.9160, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 3 10gs tensor(6.4950, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 4 10gs tensor(6.0611, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 5 10gs tensor(5.6093, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 6 10gs tensor(5.1363, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 7 10gs tensor(4.6462, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 8 10gs tensor(4.1394, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 9 10gs tensor(3.6149, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 10 10gs tensor(3.0685, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 11 10gs tensor(2.6878, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:972: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake TYR3A 12 10gs tensor(1.7306, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 13 10gs tensor(5.8473, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 14 10gs tensor(0.4865, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 15 10gs tensor(0.2884, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 16 10gs tensor(0.1077, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 17 10gs tensor(0.3363, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 18 10gs tensor(0.0102, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 19 10gs tensor(0.0640, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 20 10gs tensor(0.0743, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 21 10gs tensor(0.0157, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 22 10gs tensor(0.0071, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 23 10gs tensor(6.1350e-06, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 24 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 25 10gs tensor(0.0282, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 26 10gs tensor(0.0011, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 27 10gs tensor(0.0365, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 28 10gs tensor(0.0035, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 29 10gs tensor(0.0032, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 30 10gs tensor(0.0014, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 31 10gs tensor(0.0222, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 32 10gs tensor(0.0297, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 33 10gs tensor(0.0083, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 34 10gs tensor(0.0034, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 35 10gs tensor(0.0035, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 36 10gs tensor(0.0023, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 37 10gs tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 38 10gs tensor(0.0017, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 39 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 40 10gs tensor(0.0152, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 41 10gs tensor(0.0070, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 42 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 43 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 44 10gs tensor(0.0022, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 45 10gs tensor(0.0073, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 46 10gs tensor(0.0081, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 47 10gs tensor(0.0022, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 48 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 49 10gs tensor(0.0009, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 50 10gs tensor(0.3228, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 51 10gs tensor(0.1603, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 52 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 53 10gs tensor(0.1112, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 54 10gs tensor(0.1337, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 55 10gs tensor(0.6479, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 56 10gs tensor(0.4296, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 57 10gs tensor(0.0485, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 58 10gs tensor(0.1020, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 59 10gs tensor(0.2905, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 60 10gs tensor(0.2102, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 61 10gs tensor(0.0174, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 62 10gs tensor(0.0826, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 63 10gs tensor(0.1259, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 64 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 65 10gs tensor(0.0919, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 66 10gs tensor(0.1565, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 67 10gs tensor(0.0545, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 68 10gs tensor(0.0095, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 69 10gs tensor(0.0842, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 70 10gs tensor(0.0080, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 71 10gs tensor(0.0523, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 72 10gs tensor(0.0832, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 73 10gs tensor(0.0101, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 74 10gs tensor(0.8715, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 75 10gs tensor(0.0953, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 76 10gs tensor(2.5862, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 77 10gs tensor(1.1849, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 78 10gs tensor(0.0167, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 79 10gs tensor(0.7974, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 80 10gs tensor(1.1990, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 81 10gs tensor(1.2324, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 82 10gs tensor(0.9805, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 83 10gs tensor(0.4082, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 84 10gs tensor(0.0076, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 85 10gs tensor(0.5905, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 86 10gs tensor(0.3817, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 87 10gs tensor(0.6978, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 88 10gs tensor(0.0680, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 89 10gs tensor(0.2728, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 90 10gs tensor(0.3317, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 91 10gs tensor(0.1099, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 92 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 93 10gs tensor(0.0101, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 94 10gs tensor(0.0297, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 95 10gs tensor(0.6910, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 96 10gs tensor(0.0832, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 97 10gs tensor(0.3591, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 98 10gs tensor(0.1981, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 99 10gs tensor(0.0830, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 100 10gs tensor(0.0016, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 101 10gs tensor(0.0947, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 102 10gs tensor(0.1760, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 103 10gs tensor(0.1111, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 104 10gs tensor(0.0071, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 105 10gs tensor(0.0535, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 106 10gs tensor(0.0390, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 107 10gs tensor(0.0112, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 108 10gs tensor(0.0175, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 109 10gs tensor(1.1304, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 110 10gs tensor(0.0177, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 111 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 112 10gs tensor(0.0229, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 113 10gs tensor(0.0135, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 114 10gs tensor(0.0013, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 115 10gs tensor(0.0267, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 116 10gs tensor(0.0359, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 117 10gs tensor(0.0117, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 118 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 119 10gs tensor(0.0117, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 120 10gs tensor(0.0063, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 121 10gs tensor(0.0020, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 122 10gs tensor(2.2705, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 123 10gs tensor(0.0377, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 124 10gs tensor(0.0977, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 125 10gs tensor(0.0094, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 126 10gs tensor(0.0397, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 127 10gs tensor(0.1059, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 128 10gs tensor(0.0422, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 129 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 130 10gs tensor(0.0538, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 131 10gs tensor(0.0123, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 132 10gs tensor(0.0182, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 133 10gs tensor(0.0625, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 134 10gs tensor(0.0197, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 135 10gs tensor(0.0068, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 136 10gs tensor(0.0286, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 137 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 138 10gs tensor(0.0270, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 139 10gs tensor(0.0310, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 140 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 141 10gs tensor(0.0166, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 142 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 143 10gs tensor(2.1005, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 144 10gs tensor(0.4421, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 145 10gs tensor(2.0941, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 146 10gs tensor(1.8797, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 147 10gs tensor(0.1268, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 148 10gs tensor(0.9261, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 149 10gs tensor(1.6482, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 150 10gs tensor(1.6598, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 151 10gs tensor(0.9826, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 152 10gs tensor(0.1118, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 153 10gs tensor(0.7456, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 154 10gs tensor(0.0001, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 155 10gs tensor(0.2495, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 156 10gs tensor(0.1612, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 157 10gs tensor(0.0120, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 158 10gs tensor(0.3162, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 159 10gs tensor(0.1411, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 160 10gs tensor(0.1233, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 161 10gs tensor(0.1886, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 162 10gs tensor(0.0958, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 163 10gs tensor(0.0029, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 164 10gs tensor(0.1012, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 165 10gs tensor(0.0532, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 166 10gs tensor(0.0109, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 167 10gs tensor(0.1679, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 168 10gs tensor(0.0394, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 169 10gs tensor(0.0063, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 170 10gs tensor(0.0556, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 171 10gs tensor(0.0102, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 172 10gs tensor(0.0227, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 173 10gs tensor(0.0594, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 174 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 175 10gs tensor(0.0161, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 176 10gs tensor(0.0159, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 177 10gs tensor(0.0033, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 178 10gs tensor(0.0339, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 179 10gs tensor(0.0136, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 180 10gs tensor(0.0400, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 181 10gs tensor(0.1704, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 182 10gs tensor(0.0221, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 183 10gs tensor(0.0859, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 184 10gs tensor(0.1364, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 185 10gs tensor(0.0591, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 186 10gs tensor(0.2852, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 187 10gs tensor(0.0739, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 188 10gs tensor(0.1099, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 189 10gs tensor(0.2721, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 190 10gs tensor(0.0089, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 191 10gs tensor(0.1676, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 192 10gs tensor(0.0885, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 193 10gs tensor(0.0460, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 194 10gs tensor(0.1845, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 195 10gs tensor(0.0282, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 196 10gs tensor(0.0674, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 197 10gs tensor(0.0803, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 198 10gs tensor(0.0088, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR3A 199 10gs tensor(0.1101, grad_fn=<HuberLossBackward0>)\n",
      "tensor(1.2542e-05, grad_fn=<DivBackward0>)\n",
      "fake TYR7A 0 10gs tensor(0.0774, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 1 10gs tensor(4.8271, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 2 10gs tensor(0.9552, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 3 10gs tensor(0.8460, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 4 10gs tensor(2014.1580, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 5 10gs tensor(1.9302, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 6 10gs tensor(10.4607, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 7 10gs tensor(5.5029, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 8 10gs tensor(2485.4136, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 9 10gs tensor(6.1426, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 10 10gs tensor(6.2818, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 11 10gs tensor(6.2910, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 12 10gs tensor(6.1733, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 13 10gs tensor(5.9297, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 14 10gs tensor(5.9453, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 15 10gs tensor(5.2674, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 16 10gs tensor(4.9200, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 17 10gs tensor(5.1629, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 18 10gs tensor(2.7274, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 19 10gs tensor(0.1588, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 20 10gs tensor(10.7058, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 21 10gs tensor(27.6579, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 22 10gs tensor(1.6761, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 23 10gs tensor(2.7396, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 24 10gs tensor(4.5546, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 25 10gs tensor(4.7003, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 26 10gs tensor(4.7209, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 27 10gs tensor(4.6503, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 28 10gs tensor(4.5158, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 29 10gs tensor(4.1756, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 30 10gs tensor(4.0145, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 31 10gs tensor(2.3230, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 32 10gs tensor(1.5513, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 33 10gs tensor(0.0262, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 34 10gs tensor(0.8802, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 35 10gs tensor(0.5784, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 36 10gs tensor(0.4624, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 37 10gs tensor(2.8987, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 38 10gs tensor(1.7779, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 39 10gs tensor(0.0169, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 40 10gs tensor(1.1089, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 41 10gs tensor(0.1973, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 42 10gs tensor(1.6234, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 43 10gs tensor(2.5265, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 44 10gs tensor(2.4609, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 45 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 46 10gs tensor(0.0823, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 47 10gs tensor(2.4150, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 48 10gs tensor(0.2423, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 49 10gs tensor(0.0965, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 50 10gs tensor(0.3834, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 51 10gs tensor(0.6657, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 52 10gs tensor(0.0023, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 53 10gs tensor(0.5118, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 54 10gs tensor(0.0234, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 55 10gs tensor(0.5586, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 56 10gs tensor(0.5343, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 57 10gs tensor(2.0708, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 58 10gs tensor(0.1353, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 59 10gs tensor(0.4268, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 60 10gs tensor(0.0084, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 61 10gs tensor(0.2784, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 62 10gs tensor(0.0220, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 63 10gs tensor(2.0857, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 64 10gs tensor(2.0405, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 65 10gs tensor(1.9342, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 66 10gs tensor(1.7878, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 67 10gs tensor(1.8684, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 68 10gs tensor(0.0034, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 69 10gs tensor(1.2686, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 70 10gs tensor(1.1325, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 71 10gs tensor(0.0099, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 72 10gs tensor(1.1155, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 73 10gs tensor(0.0140, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 74 10gs tensor(1.3253, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 75 10gs tensor(1.3069, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 76 10gs tensor(1.2513, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 77 10gs tensor(1.1607, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 78 10gs tensor(1.0536, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 79 10gs tensor(0.9162, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 80 10gs tensor(0.7339, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 81 10gs tensor(0.5194, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 82 10gs tensor(0.3108, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 83 10gs tensor(0.1550, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 84 10gs tensor(0.0565, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 85 10gs tensor(0.0102, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 86 10gs tensor(0.0005, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 87 10gs tensor(0.0147, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 88 10gs tensor(0.0361, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 89 10gs tensor(0.0540, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 90 10gs tensor(0.0568, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 91 10gs tensor(0.0408, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 92 10gs tensor(0.0161, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 93 10gs tensor(0.0013, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 94 10gs tensor(0.0021, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 95 10gs tensor(0.0110, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 96 10gs tensor(0.0221, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 97 10gs tensor(0.0248, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 98 10gs tensor(0.0226, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 99 10gs tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 100 10gs tensor(0.0611, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 101 10gs tensor(0.0661, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 102 10gs tensor(0.0758, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 103 10gs tensor(0.0781, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 104 10gs tensor(0.0771, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 105 10gs tensor(0.0695, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 106 10gs tensor(0.0636, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 107 10gs tensor(0.0538, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 108 10gs tensor(0.0476, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 109 10gs tensor(0.0386, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 110 10gs tensor(0.0322, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 111 10gs tensor(0.0270, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 112 10gs tensor(0.0237, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 113 10gs tensor(0.0202, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 114 10gs tensor(0.0185, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 115 10gs tensor(0.0226, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 116 10gs tensor(0.0165, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 117 10gs tensor(0.0174, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 118 10gs tensor(0.0187, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 119 10gs tensor(0.0211, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 120 10gs tensor(0.0231, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 121 10gs tensor(0.0265, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 122 10gs tensor(0.0303, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 123 10gs tensor(0.0327, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 124 10gs tensor(0.0503, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 125 10gs tensor(0.0370, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 126 10gs tensor(0.0376, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 127 10gs tensor(0.0373, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 128 10gs tensor(0.0362, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 129 10gs tensor(0.0352, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 130 10gs tensor(0.0335, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 131 10gs tensor(0.0316, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 132 10gs tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 133 10gs tensor(0.0256, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 134 10gs tensor(0.0492, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 135 10gs tensor(0.0443, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 136 10gs tensor(0.0213, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 137 10gs tensor(0.0183, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 138 10gs tensor(0.0175, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 139 10gs tensor(0.0171, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 140 10gs tensor(0.0178, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 141 10gs tensor(0.0189, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 142 10gs tensor(0.0501, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 143 10gs tensor(0.0275, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 144 10gs tensor(0.0238, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 145 10gs tensor(0.0250, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 146 10gs tensor(0.0258, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 147 10gs tensor(0.0262, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 148 10gs tensor(0.0263, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 149 10gs tensor(0.0264, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 150 10gs tensor(0.0266, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 151 10gs tensor(0.0271, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 152 10gs tensor(0.0276, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 153 10gs tensor(0.0291, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 154 10gs tensor(0.0524, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 155 10gs tensor(0.0289, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 156 10gs tensor(0.0286, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 157 10gs tensor(0.0287, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 158 10gs tensor(0.0289, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 159 10gs tensor(0.0291, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 160 10gs tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 161 10gs tensor(0.0289, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 162 10gs tensor(0.0284, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 163 10gs tensor(0.0279, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 164 10gs tensor(0.0275, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 165 10gs tensor(0.0272, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 166 10gs tensor(0.0272, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 167 10gs tensor(0.0273, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 168 10gs tensor(0.0278, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 169 10gs tensor(0.0279, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 170 10gs tensor(0.0284, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 171 10gs tensor(0.0291, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 172 10gs tensor(0.0291, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 173 10gs tensor(0.0293, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 174 10gs tensor(0.0294, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 175 10gs tensor(0.0295, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 176 10gs tensor(0.0297, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 177 10gs tensor(0.0375, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 178 10gs tensor(0.0283, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 179 10gs tensor(0.0273, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 180 10gs tensor(0.0358, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 181 10gs tensor(0.0265, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 182 10gs tensor(0.0263, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 183 10gs tensor(0.0267, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 184 10gs tensor(0.0279, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 185 10gs tensor(0.0285, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 186 10gs tensor(0.0285, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 187 10gs tensor(0.0282, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 188 10gs tensor(0.0279, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 189 10gs tensor(0.0273, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 190 10gs tensor(0.0273, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 191 10gs tensor(0.0280, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 192 10gs tensor(0.0284, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 193 10gs tensor(0.0287, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 194 10gs tensor(0.0293, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 195 10gs tensor(0.0305, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 196 10gs tensor(0.0311, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 197 10gs tensor(0.0309, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 198 10gs tensor(0.0842, grad_fn=<HuberLossBackward0>)\n",
      "fake TYR7A 199 10gs tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "tensor(9.2764e-11, grad_fn=<DivBackward0>)\n",
      "fake CYS14A 0 10gs tensor(1.0738, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 1 10gs tensor(0.4990, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 2 10gs tensor(0.1448, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 3 10gs tensor(0.0067, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 4 10gs tensor(0.0291, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 5 10gs tensor(6.5952, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 6 10gs tensor(0.3568, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 7 10gs tensor(0.5483, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 8 10gs tensor(0.6292, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 9 10gs tensor(0.6165, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 10 10gs tensor(0.5285, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 11 10gs tensor(0.3809, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 12 10gs tensor(0.2258, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 13 10gs tensor(0.1013, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 14 10gs tensor(0.0282, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 15 10gs tensor(0.0019, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 16 10gs tensor(0.0033, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 17 10gs tensor(0.0156, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 18 10gs tensor(0.0247, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 19 10gs tensor(0.0205, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 20 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 21 10gs tensor(0.1429, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 22 10gs tensor(6.7685e-06, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 23 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 24 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 25 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 26 10gs tensor(0.0013, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 27 10gs tensor(0.0104, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 28 10gs tensor(0.0368, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 29 10gs tensor(0.0438, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 30 10gs tensor(0.0577, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 31 10gs tensor(0.0656, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 32 10gs tensor(0.0666, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 33 10gs tensor(0.0603, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 34 10gs tensor(0.0481, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 35 10gs tensor(0.0339, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 36 10gs tensor(0.0221, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 37 10gs tensor(0.0126, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 38 10gs tensor(0.0076, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 39 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 40 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 41 10gs tensor(0.0071, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 42 10gs tensor(0.0090, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 43 10gs tensor(0.0136, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 44 10gs tensor(0.0192, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 45 10gs tensor(0.0248, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 46 10gs tensor(0.0298, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 47 10gs tensor(0.0464, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 48 10gs tensor(0.0332, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 49 10gs tensor(0.0457, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 50 10gs tensor(0.0717, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 51 10gs tensor(0.0209, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 52 10gs tensor(0.0155, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 53 10gs tensor(0.0115, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 54 10gs tensor(0.0092, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 55 10gs tensor(0.0083, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 56 10gs tensor(0.0084, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 57 10gs tensor(0.0096, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 58 10gs tensor(0.0118, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 59 10gs tensor(0.0149, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 60 10gs tensor(0.0188, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 61 10gs tensor(0.0218, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 62 10gs tensor(0.0246, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 63 10gs tensor(0.0264, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 64 10gs tensor(0.0271, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 65 10gs tensor(0.0262, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 66 10gs tensor(0.0242, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 67 10gs tensor(0.0218, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 68 10gs tensor(0.0192, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 69 10gs tensor(0.0172, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 70 10gs tensor(0.0246, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 71 10gs tensor(0.0430, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 72 10gs tensor(0.0184, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 73 10gs tensor(0.0087, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 74 10gs tensor(0.0084, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 75 10gs tensor(0.0125, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 76 10gs tensor(0.0102, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 77 10gs tensor(0.0128, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 78 10gs tensor(0.0162, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 79 10gs tensor(0.0199, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 80 10gs tensor(0.0234, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 81 10gs tensor(0.0266, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 82 10gs tensor(0.0276, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 83 10gs tensor(0.0275, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 84 10gs tensor(0.0261, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 85 10gs tensor(0.0239, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 86 10gs tensor(0.0213, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 87 10gs tensor(0.0188, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 88 10gs tensor(0.0168, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 89 10gs tensor(0.0160, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 90 10gs tensor(0.0211, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 91 10gs tensor(0.0141, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 92 10gs tensor(0.0143, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 93 10gs tensor(0.0159, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 94 10gs tensor(0.0166, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 95 10gs tensor(0.0183, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 96 10gs tensor(0.0200, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 97 10gs tensor(0.0216, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 98 10gs tensor(0.0230, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 99 10gs tensor(0.0229, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 100 10gs tensor(0.0227, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 101 10gs tensor(0.0219, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 102 10gs tensor(0.0208, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 103 10gs tensor(0.0198, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 104 10gs tensor(0.0189, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 105 10gs tensor(0.0177, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 106 10gs tensor(0.0172, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 107 10gs tensor(0.0171, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 108 10gs tensor(0.0175, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 109 10gs tensor(0.0181, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 110 10gs tensor(0.0324, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 111 10gs tensor(0.0185, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 112 10gs tensor(0.0182, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 113 10gs tensor(0.0181, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 114 10gs tensor(0.0199, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 115 10gs tensor(0.0185, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 116 10gs tensor(0.0188, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 117 10gs tensor(0.0188, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 118 10gs tensor(0.0190, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 119 10gs tensor(0.0210, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 120 10gs tensor(0.0192, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 121 10gs tensor(0.0191, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 122 10gs tensor(0.0190, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 123 10gs tensor(0.0189, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 124 10gs tensor(0.0189, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 125 10gs tensor(0.0190, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 126 10gs tensor(0.0191, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 127 10gs tensor(0.0192, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 128 10gs tensor(0.0194, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 129 10gs tensor(0.0196, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 130 10gs tensor(0.0201, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 131 10gs tensor(0.0199, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 132 10gs tensor(0.0199, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 133 10gs tensor(0.0199, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 134 10gs tensor(0.0199, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 135 10gs tensor(0.0610, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 136 10gs tensor(0.0156, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 137 10gs tensor(0.0128, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 138 10gs tensor(0.0116, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 139 10gs tensor(0.0116, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 140 10gs tensor(0.0128, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 141 10gs tensor(0.0151, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 142 10gs tensor(0.0180, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 143 10gs tensor(0.0210, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 144 10gs tensor(0.0236, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 145 10gs tensor(0.0253, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 146 10gs tensor(0.0252, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 147 10gs tensor(0.0244, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 148 10gs tensor(0.0222, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 149 10gs tensor(0.0200, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 150 10gs tensor(0.0180, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 151 10gs tensor(0.0166, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 152 10gs tensor(0.0164, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 153 10gs tensor(0.0227, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 154 10gs tensor(0.0155, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 155 10gs tensor(0.0162, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 156 10gs tensor(0.0171, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 157 10gs tensor(0.0185, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 158 10gs tensor(0.0200, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 159 10gs tensor(0.0216, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 160 10gs tensor(0.0221, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 161 10gs tensor(0.0230, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 162 10gs tensor(0.0217, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 163 10gs tensor(0.0207, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 164 10gs tensor(0.0196, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 165 10gs tensor(0.0186, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 166 10gs tensor(0.0179, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 167 10gs tensor(0.0176, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 168 10gs tensor(0.0246, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 169 10gs tensor(0.0172, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 170 10gs tensor(0.0173, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 171 10gs tensor(0.0178, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 172 10gs tensor(0.0186, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 173 10gs tensor(0.0200, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 174 10gs tensor(0.0202, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 175 10gs tensor(0.0207, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 176 10gs tensor(0.0208, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 177 10gs tensor(0.0211, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 178 10gs tensor(0.0202, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 179 10gs tensor(0.0195, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 180 10gs tensor(0.0189, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 181 10gs tensor(0.0185, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 182 10gs tensor(0.0183, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 183 10gs tensor(0.0187, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 184 10gs tensor(0.0187, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 185 10gs tensor(0.0191, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 186 10gs tensor(0.0265, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 187 10gs tensor(0.0192, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 188 10gs tensor(0.0188, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 189 10gs tensor(0.0187, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 190 10gs tensor(0.0190, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 191 10gs tensor(0.0188, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 192 10gs tensor(0.0190, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 193 10gs tensor(0.0193, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 194 10gs tensor(0.0195, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 195 10gs tensor(0.0523, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 196 10gs tensor(0.0155, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 197 10gs tensor(0.0129, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 198 10gs tensor(0.0118, grad_fn=<HuberLossBackward0>)\n",
      "fake CYS14A 199 10gs tensor(0.0123, grad_fn=<HuberLossBackward0>)\n",
      "tensor(3.0893e-15, grad_fn=<DivBackward0>)\n",
      "fake ASP23A 0 10gs tensor(3.7090, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 1 10gs tensor(3.1347, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 2 10gs tensor(1.7911, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 3 10gs tensor(0.2281, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 4 10gs tensor(0.2137, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 5 10gs tensor(0.4646, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 6 10gs tensor(0.1048, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 7 10gs tensor(0.0784, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 8 10gs tensor(1.4260, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 9 10gs tensor(0.1613, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 10 10gs tensor(0.0230, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 11 10gs tensor(0.0037, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 12 10gs tensor(0.0192, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 13 10gs tensor(0.0262, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 14 10gs tensor(0.0105, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 15 10gs tensor(0.0002, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 16 10gs tensor(0.0070, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 17 10gs tensor(0.0279, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 18 10gs tensor(0.0597, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 19 10gs tensor(0.0881, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 20 10gs tensor(1.4741, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 21 10gs tensor(0.1761, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 22 10gs tensor(0.1529, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 23 10gs tensor(0.1352, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 24 10gs tensor(0.0972, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 25 10gs tensor(0.0891, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 26 10gs tensor(0.0207, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 27 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 28 10gs tensor(4.6203e-06, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 29 10gs tensor(0.0009, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 30 10gs tensor(0.0011, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 31 10gs tensor(0.0022, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 32 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 33 10gs tensor(0.0001, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 34 10gs tensor(5.2968e-07, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 35 10gs tensor(0.0017, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 36 10gs tensor(0.0018, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 37 10gs tensor(0.0024, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 38 10gs tensor(0.0034, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 39 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 40 10gs tensor(0.0005, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 41 10gs tensor(0.0008, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 42 10gs tensor(0.0019, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 43 10gs tensor(0.0207, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 44 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 45 10gs tensor(0.0081, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 46 10gs tensor(0.0087, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 47 10gs tensor(0.0080, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 48 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 49 10gs tensor(0.0034, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 50 10gs tensor(0.0024, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 51 10gs tensor(0.0034, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 52 10gs tensor(0.0023, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 53 10gs tensor(0.0031, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 54 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 55 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 56 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 57 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 58 10gs tensor(0.0039, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 59 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 60 10gs tensor(0.0026, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 61 10gs tensor(0.0028, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 62 10gs tensor(0.0164, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 63 10gs tensor(0.0048, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 64 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 65 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 66 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 67 10gs tensor(0.0030, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 68 10gs tensor(0.0025, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 69 10gs tensor(0.0021, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 70 10gs tensor(0.0024, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 71 10gs tensor(0.0030, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 72 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 73 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 74 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 75 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 76 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 77 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 78 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 79 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 80 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 81 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 82 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 83 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 84 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 85 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 86 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 87 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 88 10gs tensor(0.0039, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 89 10gs tensor(0.0039, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 90 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 91 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 92 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 93 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 94 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 95 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 96 10gs tensor(0.0092, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 97 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 98 10gs tensor(0.0034, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 99 10gs tensor(0.0031, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 100 10gs tensor(0.0103, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 101 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 102 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 103 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 104 10gs tensor(0.0035, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 105 10gs tensor(0.0031, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 106 10gs tensor(0.0028, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 107 10gs tensor(0.0028, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 108 10gs tensor(0.0030, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 109 10gs tensor(0.0035, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 110 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 111 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 112 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 113 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 114 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 115 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 116 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 117 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 118 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 119 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 120 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 121 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 122 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 123 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 124 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 125 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 126 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 127 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 128 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 129 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 130 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 131 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 132 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 133 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 134 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 135 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 136 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 137 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 138 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 139 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 140 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 141 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 142 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 143 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 144 10gs tensor(0.0094, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 145 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 146 10gs tensor(0.0072, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 147 10gs tensor(0.0034, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 148 10gs tensor(0.0031, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 149 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 150 10gs tensor(0.0028, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 151 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 152 10gs tensor(0.0031, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 153 10gs tensor(0.0033, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 154 10gs tensor(0.0035, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 155 10gs tensor(0.0037, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 156 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 157 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 158 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 159 10gs tensor(0.0048, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 160 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 161 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 162 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 163 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 164 10gs tensor(0.0048, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 165 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 166 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 167 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 168 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 169 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 170 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 171 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 172 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 173 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 174 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 175 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 176 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 177 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 178 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 179 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 180 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 181 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 182 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 183 10gs tensor(0.0076, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 184 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 185 10gs tensor(0.0037, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 186 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 187 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 188 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 189 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 190 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 191 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 192 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 193 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 194 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 195 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 196 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 197 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 198 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake ASP23A 199 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "tensor(2.5256e-13, grad_fn=<DivBackward0>)\n",
      "fake LYS29A 0 10gs tensor(6.2273, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 1 10gs tensor(5.7873, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 2 10gs tensor(5.3230, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 3 10gs tensor(4.8660, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 4 10gs tensor(4.3852, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 5 10gs tensor(3.8797, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 6 10gs tensor(3.3654, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 7 10gs tensor(2.8579, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 8 10gs tensor(2.5672, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 9 10gs tensor(1.8842, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 10 10gs tensor(1.4032, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 11 10gs tensor(0.9133, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 12 10gs tensor(0.4639, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 13 10gs tensor(0.0888, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 14 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 15 10gs tensor(0.0636, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 16 10gs tensor(0.1409, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 17 10gs tensor(0.0638, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 18 10gs tensor(0.2036, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 19 10gs tensor(0.1578, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 20 10gs tensor(0.0269, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 21 10gs tensor(0.0138, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 22 10gs tensor(0.0064, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 23 10gs tensor(0.0269, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 24 10gs tensor(0.0484, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 25 10gs tensor(0.0797, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 26 10gs tensor(0.0802, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 27 10gs tensor(0.0114, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 28 10gs tensor(3.8657e-07, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 29 10gs tensor(0.0215, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 30 10gs tensor(0.0122, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 31 10gs tensor(0.0604, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 32 10gs tensor(0.0864, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 33 10gs tensor(0.0691, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 34 10gs tensor(0.0565, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 35 10gs tensor(0.0579, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 36 10gs tensor(0.0518, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 37 10gs tensor(0.0425, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 38 10gs tensor(0.0535, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 39 10gs tensor(0.0228, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 40 10gs tensor(0.0099, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 41 10gs tensor(0.0034, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 42 10gs tensor(0.0025, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 43 10gs tensor(0.0020, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 44 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 45 10gs tensor(0.2193, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 46 10gs tensor(0.0115, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 47 10gs tensor(0.0100, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 48 10gs tensor(0.0108, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 49 10gs tensor(0.0122, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 50 10gs tensor(0.0136, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 51 10gs tensor(0.0142, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 52 10gs tensor(0.0132, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 53 10gs tensor(0.0105, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 54 10gs tensor(0.0071, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 55 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 56 10gs tensor(0.0071, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 57 10gs tensor(0.0023, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 58 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 59 10gs tensor(0.0083, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 60 10gs tensor(0.0084, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 61 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 62 10gs tensor(0.0012, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 63 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 64 10gs tensor(0.0074, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 65 10gs tensor(0.0014, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 66 10gs tensor(1.4077, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 67 10gs tensor(0.0216, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 68 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 69 10gs tensor(0.0005, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 70 10gs tensor(0.0029, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 71 10gs tensor(0.0177, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 72 10gs tensor(0.0072, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 73 10gs tensor(0.0001, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 74 10gs tensor(0.0012, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 75 10gs tensor(0.0136, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 76 10gs tensor(0.0065, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 77 10gs tensor(5.1889e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 78 10gs tensor(0.0019, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 79 10gs tensor(0.0120, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 80 10gs tensor(0.0025, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 81 10gs tensor(2.1940e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 82 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 83 10gs tensor(0.0072, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 84 10gs tensor(6.0219e-06, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 85 10gs tensor(0.0025, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 86 10gs tensor(0.0077, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 87 10gs tensor(0.0001, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 88 10gs tensor(0.0020, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 89 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 90 10gs tensor(4.8497e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 91 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 92 10gs tensor(0.0025, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 93 10gs tensor(0.0002, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 94 10gs tensor(0.0078, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 95 10gs tensor(7.2360e-06, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 96 10gs tensor(0.0068, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 97 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 98 10gs tensor(0.0027, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 99 10gs tensor(0.0020, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 100 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 101 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 102 10gs tensor(0.0005, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 103 10gs tensor(0.0226, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 104 10gs tensor(0.0075, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 105 10gs tensor(0.0190, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 106 10gs tensor(0.0025, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 107 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 108 10gs tensor(0.0216, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 109 10gs tensor(0.7542, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 110 10gs tensor(0.6025, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 111 10gs tensor(0.3113, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 112 10gs tensor(0.0136, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 113 10gs tensor(0.3884, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 114 10gs tensor(0.3791, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 115 10gs tensor(0.0095, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 116 10gs tensor(0.3163, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 117 10gs tensor(0.0393, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 118 10gs tensor(0.0773, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 119 10gs tensor(0.1427, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 120 10gs tensor(0.0048, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 121 10gs tensor(0.7621, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 122 10gs tensor(0.1144, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 123 10gs tensor(0.3386, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 124 10gs tensor(20.9808, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 125 10gs tensor(0.4704, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 126 10gs tensor(0.1664, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 127 10gs tensor(0.0516, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 128 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 129 10gs tensor(0.0651, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 130 10gs tensor(0.0402, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 131 10gs tensor(0.0110, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 132 10gs tensor(0.0226, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 133 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 134 10gs tensor(0.0794, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 135 10gs tensor(0.0261, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 136 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 137 10gs tensor(0.0655, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 138 10gs tensor(0.0899, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 139 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 140 10gs tensor(0.1165, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 141 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 142 10gs tensor(0.2604, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 143 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 144 10gs tensor(0.2298, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 145 10gs tensor(0.4117, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 146 10gs tensor(0.0165, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 147 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 148 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 149 10gs tensor(0.1740, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 150 10gs tensor(0.9631, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 151 10gs tensor(0.6552, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 152 10gs tensor(7.4785e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 153 10gs tensor(0.4102, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 154 10gs tensor(0.2706, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 155 10gs tensor(0.0515, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 156 10gs tensor(0.0012, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 157 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 158 10gs tensor(0.0082, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 159 10gs tensor(0.0148, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 160 10gs tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 161 10gs tensor(0.0002, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 162 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 163 10gs tensor(0.0020, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 164 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 165 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 166 10gs tensor(0.0001, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 167 10gs tensor(2.5689e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 168 10gs tensor(0.0011, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 169 10gs tensor(0.0014, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 170 10gs tensor(1.1817e-06, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 171 10gs tensor(5.9099e-07, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 172 10gs tensor(0.0022, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 173 10gs tensor(0.0109, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 174 10gs tensor(0.0005, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 175 10gs tensor(8.1825e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 176 10gs tensor(0.0075, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 177 10gs tensor(0.0384, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 178 10gs tensor(0.0072, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 179 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 180 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 181 10gs tensor(3.2801e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 182 10gs tensor(0.0092, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 183 10gs tensor(0.0067, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 184 10gs tensor(9.1678e-06, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 185 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 186 10gs tensor(0.0002, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 187 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 188 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 189 10gs tensor(0.0005, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 190 10gs tensor(5.3396e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 191 10gs tensor(0.0002, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 192 10gs tensor(0.0030, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 193 10gs tensor(0.0074, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 194 10gs tensor(0.0026, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 195 10gs tensor(0.4604, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 196 10gs tensor(0.0076, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 197 10gs tensor(0.7319, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 198 10gs tensor(0.3011, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS29A 199 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "fake GLU30A 0 10gs tensor(4.5151, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 1 10gs tensor(50.3310, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 2 10gs tensor(0.5083, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 3 10gs tensor(0.5893, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 4 10gs tensor(0.9796, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 5 10gs tensor(0.9114, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 6 10gs tensor(0.4675, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 7 10gs tensor(0.0720, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 8 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 9 10gs tensor(0.0497, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 10 10gs tensor(0.0464, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 11 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 12 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 13 10gs tensor(0.0386, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 14 10gs tensor(0.0809, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 15 10gs tensor(0.0340, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 16 10gs tensor(0.0009, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 17 10gs tensor(0.0070, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 18 10gs tensor(8.9010, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 19 10gs tensor(0.0024, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 20 10gs tensor(0.0327, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 21 10gs tensor(0.0853, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 22 10gs tensor(0.0760, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 23 10gs tensor(0.0400, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 24 10gs tensor(0.0291, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 25 10gs tensor(0.0009, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 26 10gs tensor(0.0110, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 27 10gs tensor(0.0077, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 28 10gs tensor(0.0016, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 29 10gs tensor(0.0093, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 30 10gs tensor(0.0287, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 31 10gs tensor(0.0243, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 32 10gs tensor(0.0077, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 33 10gs tensor(0.0007, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 34 10gs tensor(2.6237e-06, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 35 10gs tensor(1.4098e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 36 10gs tensor(0.0007, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 37 10gs tensor(0.0011, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 38 10gs tensor(0.0026, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 39 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 40 10gs tensor(0.0115, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 41 10gs tensor(0.0168, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 42 10gs tensor(0.0171, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 43 10gs tensor(0.0120, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 44 10gs tensor(0.0061, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 45 10gs tensor(0.0032, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 46 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 47 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 48 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 49 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 50 10gs tensor(0.0030, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 51 10gs tensor(0.2232, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 52 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 53 10gs tensor(0.0008, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 54 10gs tensor(0.0028, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 55 10gs tensor(0.0013, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 56 10gs tensor(0.0011, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 57 10gs tensor(0.0002, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 58 10gs tensor(9.8400e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 59 10gs tensor(0.0019, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 60 10gs tensor(0.0109, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 61 10gs tensor(0.0186, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 62 10gs tensor(0.0150, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 63 10gs tensor(0.0104, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 64 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 65 10gs tensor(0.0019, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 66 10gs tensor(0.0009, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 67 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 68 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 69 10gs tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 70 10gs tensor(0.0035, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 71 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 72 10gs tensor(0.0084, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 73 10gs tensor(0.0081, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 74 10gs tensor(0.0063, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 75 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 76 10gs tensor(0.0032, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 77 10gs tensor(0.0026, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 78 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 79 10gs tensor(0.0069, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 80 10gs tensor(0.0026, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 81 10gs tensor(0.0445, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 82 10gs tensor(0.0017, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 83 10gs tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 84 10gs tensor(0.0066, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 85 10gs tensor(0.0018, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 86 10gs tensor(0.0027, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 87 10gs tensor(0.0023, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 88 10gs tensor(0.0765, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 89 10gs tensor(1.1396e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 90 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 91 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 92 10gs tensor(2.9865e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 93 10gs tensor(0.0013, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 94 10gs tensor(0.0065, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 95 10gs tensor(0.0133, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 96 10gs tensor(0.0156, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 97 10gs tensor(0.0132, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 98 10gs tensor(0.0081, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 99 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 100 10gs tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 101 10gs tensor(0.0007, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 102 10gs tensor(0.0008, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 103 10gs tensor(0.0039, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 104 10gs tensor(0.0031, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 105 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 106 10gs tensor(0.0075, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 107 10gs tensor(0.0085, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 108 10gs tensor(0.0077, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 109 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 110 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 111 10gs tensor(0.0025, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 112 10gs tensor(0.0029, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 113 10gs tensor(0.0019, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 114 10gs tensor(0.0025, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 115 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 116 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 117 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 118 10gs tensor(0.0063, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 119 10gs tensor(0.0183, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 120 10gs tensor(0.0033, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 121 10gs tensor(0.0019, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 122 10gs tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 123 10gs tensor(0.0016, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 124 10gs tensor(0.0029, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 125 10gs tensor(0.0069, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 126 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 127 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 128 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 129 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 130 10gs tensor(0.0136, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 131 10gs tensor(0.0022, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 132 10gs tensor(0.0019, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 133 10gs tensor(0.0022, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 134 10gs tensor(0.0132, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 135 10gs tensor(0.0032, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 136 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 137 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 138 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 139 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 140 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 141 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 142 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 143 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 144 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 145 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 146 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 147 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 148 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 149 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 150 10gs tensor(0.0064, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 151 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 152 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 153 10gs tensor(0.0118, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 154 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 155 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 156 10gs tensor(0.0035, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 157 10gs tensor(0.0291, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 158 10gs tensor(0.0018, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 159 10gs tensor(0.0009, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 160 10gs tensor(0.0171, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 161 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 162 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 163 10gs tensor(0.0013, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 164 10gs tensor(0.0034, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 165 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 166 10gs tensor(0.0078, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 167 10gs tensor(0.0086, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 168 10gs tensor(0.0072, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 169 10gs tensor(0.0119, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 170 10gs tensor(0.0031, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 171 10gs tensor(0.0028, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 172 10gs tensor(0.0021, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 173 10gs tensor(0.0028, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 174 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 175 10gs tensor(0.0060, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 176 10gs tensor(0.0071, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 177 10gs tensor(0.0071, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 178 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 179 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 180 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 181 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 182 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 183 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 184 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 185 10gs tensor(0.0063, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 186 10gs tensor(0.0067, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 187 10gs tensor(0.0065, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 188 10gs tensor(0.0218, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 189 10gs tensor(0.0027, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 190 10gs tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 191 10gs tensor(0.0011, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 192 10gs tensor(0.0111, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 193 10gs tensor(0.0016, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 194 10gs tensor(0.0023, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 195 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 196 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 197 10gs tensor(0.0070, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 198 10gs tensor(0.0072, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU30A 199 10gs tensor(0.0064, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "fake GLU31A 0 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 1 10gs tensor(7.3032, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 2 10gs tensor(0.4247, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 3 10gs tensor(0.3405, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 4 10gs tensor(0.1663, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 5 10gs tensor(0.0452, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 6 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 7 10gs tensor(0.0189, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 8 10gs tensor(0.0474, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 9 10gs tensor(0.0506, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 10 10gs tensor(0.0303, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 11 10gs tensor(0.0072, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 12 10gs tensor(0.0012, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 13 10gs tensor(0.0093, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 14 10gs tensor(0.0217, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 15 10gs tensor(0.0255, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 16 10gs tensor(0.0294, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 17 10gs tensor(0.0216, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 18 10gs tensor(0.0161, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 19 10gs tensor(0.0100, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 20 10gs tensor(0.0064, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 21 10gs tensor(0.0021, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 22 10gs tensor(0.0009, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 23 10gs tensor(0.0007, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 24 10gs tensor(0.0014, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 25 10gs tensor(0.0032, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 26 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 27 10gs tensor(0.0091, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 28 10gs tensor(0.0107, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 29 10gs tensor(0.0101, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 30 10gs tensor(0.0081, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 31 10gs tensor(0.0067, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 32 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 33 10gs tensor(0.0039, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 34 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 35 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 36 10gs tensor(0.0168, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 37 10gs tensor(0.0048, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 38 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 39 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 40 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 41 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 42 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 43 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 44 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 45 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 46 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 47 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 48 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 49 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 50 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 51 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 52 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 53 10gs tensor(0.0063, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 54 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 55 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 56 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 57 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 58 10gs tensor(0.0048, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 59 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 60 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 61 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 62 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 63 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 64 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 65 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 66 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 67 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 68 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 69 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 70 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 71 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 72 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 73 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 74 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 75 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 76 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 77 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 78 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 79 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 80 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 81 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 82 10gs tensor(0.0106, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 83 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 84 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 85 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 86 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 87 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 88 10gs tensor(0.0095, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 89 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 90 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 91 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 92 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 93 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 94 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 95 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 96 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 97 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 98 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 99 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 100 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 101 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 102 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 103 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 104 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 105 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 106 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 107 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 108 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 109 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 110 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 111 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 112 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 113 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 114 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 115 10gs tensor(0.0099, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 116 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 117 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 118 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 119 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 120 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 121 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 122 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 123 10gs tensor(0.0061, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 124 10gs tensor(0.0061, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 125 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 126 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 127 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 128 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 129 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 130 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 131 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 132 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 133 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 134 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 135 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 136 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 137 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 138 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 139 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 140 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 141 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 142 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 143 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 144 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 145 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 146 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 147 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 148 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 149 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 150 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 151 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 152 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 153 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 154 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 155 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 156 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 157 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 158 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 159 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 160 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 161 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 162 10gs tensor(0.0098, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 163 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 164 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 165 10gs tensor(0.0070, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 166 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 167 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 168 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 169 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 170 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 171 10gs tensor(0.0060, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 172 10gs tensor(0.0096, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 173 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 174 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 175 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 176 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 177 10gs tensor(0.0048, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 178 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 179 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 180 10gs tensor(0.0065, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 181 10gs tensor(0.0064, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 182 10gs tensor(0.0060, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 183 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 184 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 185 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 186 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 187 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 188 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 189 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 190 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 191 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 192 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 193 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 194 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 195 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 196 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 197 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 198 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU31A 199 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "fake GLU36A 0 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 1 10gs tensor(0.0431, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 2 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 3 10gs tensor(0.0469, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 4 10gs tensor(0.0747, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 5 10gs tensor(0.0474, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 6 10gs tensor(0.0096, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 7 10gs tensor(0.0006, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 8 10gs tensor(0.0030, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 9 10gs tensor(0.0002, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 10 10gs tensor(0.0027, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 11 10gs tensor(0.0129, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 12 10gs tensor(0.0223, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 13 10gs tensor(0.0234, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 14 10gs tensor(0.0168, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 15 10gs tensor(0.0080, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 16 10gs tensor(0.0022, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 17 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 18 10gs tensor(3.1816e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 19 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 20 10gs tensor(0.0021, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 21 10gs tensor(0.0066, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 22 10gs tensor(0.0122, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 23 10gs tensor(0.0166, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 24 10gs tensor(0.0153, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 25 10gs tensor(0.0115, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 26 10gs tensor(0.0063, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 27 10gs tensor(0.0029, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 28 10gs tensor(0.0014, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 29 10gs tensor(0.0012, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 30 10gs tensor(0.0020, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 31 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 32 10gs tensor(0.0070, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 33 10gs tensor(0.0098, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 34 10gs tensor(0.0107, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 35 10gs tensor(0.0097, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 36 10gs tensor(0.0070, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 37 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 38 10gs tensor(0.0032, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 39 10gs tensor(0.0026, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 40 10gs tensor(0.0032, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 41 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 42 10gs tensor(0.0064, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 43 10gs tensor(0.0080, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 44 10gs tensor(0.0085, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 45 10gs tensor(0.0076, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 46 10gs tensor(0.0060, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 47 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 48 10gs tensor(0.0037, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 49 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 50 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 51 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 52 10gs tensor(0.0067, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 53 10gs tensor(0.0073, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 54 10gs tensor(0.0071, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 55 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 56 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 57 10gs tensor(0.0044, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 58 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 59 10gs tensor(0.0046, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 60 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 61 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 62 10gs tensor(0.0066, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 63 10gs tensor(0.0065, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 64 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 65 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 66 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 67 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 68 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 69 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 70 10gs tensor(0.0060, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 71 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 72 10gs tensor(0.0060, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 73 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 74 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 75 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 76 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 77 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 78 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 79 10gs tensor(0.0060, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 80 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 81 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 82 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 83 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 84 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 85 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 86 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 87 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 88 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 89 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 90 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 91 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 92 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 93 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 94 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 95 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 96 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 97 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 98 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 99 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 100 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 101 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 102 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 103 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 104 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 105 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 106 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 107 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 108 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 109 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 110 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 111 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 112 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 113 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 114 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 115 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 116 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 117 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 118 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 119 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 120 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 121 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 122 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 123 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 124 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 125 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 126 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 127 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 128 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 129 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 130 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 131 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 132 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 133 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 134 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 135 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 136 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 137 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 138 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 139 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 140 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 141 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 142 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 143 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 144 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 145 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 146 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 147 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 148 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 149 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 150 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 151 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 152 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 153 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 154 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 155 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 156 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 157 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 158 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 159 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 160 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 161 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 162 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 163 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 164 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 165 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 166 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 167 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 168 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 169 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 170 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 171 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 172 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 173 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 174 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 175 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 176 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 177 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 178 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 179 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 180 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 181 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 182 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 183 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 184 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 185 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 186 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 187 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 188 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 189 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 190 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 191 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 192 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 193 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 194 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 195 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 196 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 197 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 198 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU36A 199 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "fake GLU40A 0 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 1 10gs tensor(0.0466, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 2 10gs tensor(0.0001, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 3 10gs tensor(0.0412, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 4 10gs tensor(0.0754, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 5 10gs tensor(0.0546, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 6 10gs tensor(0.0156, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 7 10gs tensor(1.8980e-06, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 8 10gs tensor(0.0026, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 9 10gs tensor(0.0007, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 10 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 11 10gs tensor(0.0083, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 12 10gs tensor(0.0181, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 13 10gs tensor(0.0228, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 14 10gs tensor(0.0195, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 15 10gs tensor(0.0115, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 16 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 17 10gs tensor(0.0008, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 18 10gs tensor(6.2149e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 19 10gs tensor(0.0003, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 20 10gs tensor(0.0008, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 21 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 22 10gs tensor(0.0093, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 23 10gs tensor(0.0149, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 24 10gs tensor(0.0170, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 25 10gs tensor(0.0144, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 26 10gs tensor(0.0092, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 27 10gs tensor(0.0045, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 28 10gs tensor(0.0019, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 29 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 30 10gs tensor(0.0012, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 31 10gs tensor(0.0026, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 32 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 33 10gs tensor(0.0086, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 34 10gs tensor(0.0108, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 35 10gs tensor(0.0108, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 36 10gs tensor(0.0087, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 37 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 38 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 39 10gs tensor(0.0027, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 40 10gs tensor(0.0027, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 41 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 42 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 43 10gs tensor(0.0073, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 44 10gs tensor(0.0084, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 45 10gs tensor(0.0083, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 46 10gs tensor(0.0070, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 47 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 48 10gs tensor(0.0040, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 49 10gs tensor(0.0035, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 50 10gs tensor(0.0038, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 51 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 52 10gs tensor(0.0060, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 53 10gs tensor(0.0071, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 54 10gs tensor(0.0074, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 55 10gs tensor(0.0069, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 56 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 57 10gs tensor(0.0048, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 58 10gs tensor(0.0042, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 59 10gs tensor(0.0043, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 60 10gs tensor(0.0048, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 61 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 62 10gs tensor(0.0064, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 63 10gs tensor(0.0067, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 64 10gs tensor(0.0063, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 65 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 66 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 67 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 68 10gs tensor(0.0047, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 69 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 70 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 71 10gs tensor(0.0061, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 72 10gs tensor(0.0062, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 73 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 74 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 75 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 76 10gs tensor(0.0049, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 77 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 78 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 79 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 80 10gs tensor(0.0060, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 81 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 82 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 83 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 84 10gs tensor(0.0051, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 85 10gs tensor(0.0052, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 86 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 87 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 88 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 89 10gs tensor(0.0058, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 90 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 91 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 92 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 93 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 94 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 95 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 96 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 97 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 98 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 99 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 100 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 101 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 102 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 103 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 104 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 105 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 106 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 107 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 108 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 109 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 110 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 111 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 112 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 113 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 114 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 115 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 116 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 117 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 118 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 119 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 120 10gs tensor(0.0056, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 121 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 122 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 123 10gs tensor(0.0054, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 124 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 125 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 126 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 127 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 128 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 129 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 130 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 131 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 132 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 133 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 134 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 135 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 136 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 137 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 138 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 139 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 140 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 141 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 142 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 143 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 144 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 145 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 146 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 147 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 148 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 149 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 150 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 151 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 152 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 153 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 154 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 155 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 156 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 157 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 158 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 159 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 160 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 161 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 162 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 163 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 164 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 165 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 166 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 167 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 168 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 169 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 170 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 171 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 172 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 173 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 174 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 175 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 176 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 177 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 178 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 179 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 180 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 181 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 182 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 183 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 184 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 185 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 186 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 187 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 188 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 189 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 190 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 191 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 192 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 193 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 194 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 195 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 196 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 197 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 198 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "fake GLU40A 199 10gs tensor(0.0055, grad_fn=<HuberLossBackward0>)\n",
      "tensor(0., grad_fn=<DivBackward0>)\n",
      "fake LYS44A 0 10gs tensor(5.8049, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 1 10gs tensor(5.4031, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 2 10gs tensor(4.9714, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 3 10gs tensor(4.5299, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 4 10gs tensor(4.0493, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 5 10gs tensor(3.4892, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 6 10gs tensor(2.8034, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 7 10gs tensor(1.7844, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 8 10gs tensor(7.5721, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 9 10gs tensor(0.2525, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 10 10gs tensor(0.1439, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 11 10gs tensor(0.0155, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 12 10gs tensor(0.0068, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 13 10gs tensor(0.0172, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 14 10gs tensor(0.0001, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 15 10gs tensor(0.0213, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 16 10gs tensor(0.0010, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 17 10gs tensor(0.0016, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 18 10gs tensor(572.3492, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 19 10gs tensor(0.0414, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 20 10gs tensor(0.0253, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 21 10gs tensor(0.0167, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 22 10gs tensor(0.4085, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 23 10gs tensor(0.0576, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 24 10gs tensor(0.2523, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 25 10gs tensor(0.2069, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 26 10gs tensor(0.2142, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 27 10gs tensor(0.1830, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 28 10gs tensor(0.1327, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 29 10gs tensor(0.0785, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 30 10gs tensor(0.0346, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 31 10gs tensor(0.0125, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 32 10gs tensor(0.0033, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 33 10gs tensor(0.0007, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 34 10gs tensor(0.0036, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 35 10gs tensor(7.4961e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 36 10gs tensor(0.0005, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 37 10gs tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 38 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 39 10gs tensor(0.0074, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 40 10gs tensor(0.0127, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 41 10gs tensor(0.0203, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 42 10gs tensor(0.0344, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 43 10gs tensor(0.0447, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 44 10gs tensor(0.0546, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 45 10gs tensor(0.0568, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 46 10gs tensor(0.0576, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 47 10gs tensor(0.0552, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 48 10gs tensor(0.0592, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 49 10gs tensor(0.1515, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 50 10gs tensor(0.0536, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 51 10gs tensor(0.0482, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 52 10gs tensor(0.0284, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 53 10gs tensor(0.0329, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 54 10gs tensor(0.0212, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 55 10gs tensor(0.0140, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 56 10gs tensor(0.0088, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 57 10gs tensor(0.0236, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 58 10gs tensor(0.0070, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 59 10gs tensor(0.0101, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 60 10gs tensor(0.0133, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 61 10gs tensor(0.0178, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 62 10gs tensor(0.0275, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 63 10gs tensor(0.0234, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 64 10gs tensor(0.0281, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 65 10gs tensor(0.0396, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 66 10gs tensor(0.0428, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 67 10gs tensor(0.0535, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 68 10gs tensor(0.0902, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 69 10gs tensor(0.0324, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 70 10gs tensor(0.0230, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 71 10gs tensor(0.0252, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 72 10gs tensor(0.0161, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 73 10gs tensor(0.0159, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 74 10gs tensor(0.0127, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 75 10gs tensor(0.0148, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 76 10gs tensor(0.0186, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 77 10gs tensor(0.0240, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 78 10gs tensor(0.0253, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 79 10gs tensor(0.0318, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 80 10gs tensor(0.0229, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 81 10gs tensor(0.0259, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 82 10gs tensor(0.0223, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 83 10gs tensor(0.0187, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 84 10gs tensor(0.0197, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 85 10gs tensor(0.0371, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 86 10gs tensor(0.0156, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 87 10gs tensor(0.0561, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 88 10gs tensor(0.0497, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 89 10gs tensor(0.0025, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 90 10gs tensor(0.0007, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 91 10gs tensor(0.0107, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 92 10gs tensor(0.0015, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 93 10gs tensor(0.0368, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 94 10gs tensor(0.0643, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 95 10gs tensor(0.0032, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 96 10gs tensor(0.0108, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 97 10gs tensor(0.0017, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 98 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 99 10gs tensor(0.0098, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 100 10gs tensor(0.0185, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 101 10gs tensor(0.0224, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 102 10gs tensor(0.0188, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 103 10gs tensor(0.0400, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 104 10gs tensor(4.7964e-07, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 105 10gs tensor(0.0016, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 106 10gs tensor(1.3115, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 107 10gs tensor(0.0072, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 108 10gs tensor(0.0004, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 109 10gs tensor(0.0123, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 110 10gs tensor(0.0984, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 111 10gs tensor(0.0713, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 112 10gs tensor(0.0002, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 113 10gs tensor(0.0017, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 114 10gs tensor(0.0011, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 115 10gs tensor(1.9142e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 116 10gs tensor(0.0486, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 117 10gs tensor(0.0123, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 118 10gs tensor(0.0126, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 119 10gs tensor(0.0035, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 120 10gs tensor(0.0005, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 121 10gs tensor(1.5574, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 122 10gs tensor(0.0312, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 123 10gs tensor(0.2093, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 124 10gs tensor(0.0288, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 125 10gs tensor(0.0016, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 126 10gs tensor(0.1409, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 127 10gs tensor(0.3139, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 128 10gs tensor(0.0766, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 129 10gs tensor(0.0009, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 130 10gs tensor(0.0562, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 131 10gs tensor(0.0094, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 132 10gs tensor(0.0098, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 133 10gs tensor(0.0111, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 134 10gs tensor(0.0933, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 135 10gs tensor(0.0744, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 136 10gs tensor(0.0166, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 137 10gs tensor(0.0026, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 138 10gs tensor(0.0241, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 139 10gs tensor(0.0031, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 140 10gs tensor(0.0112, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 141 10gs tensor(0.1450, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 142 10gs tensor(0.0377, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 143 10gs tensor(2.5805e-05, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 144 10gs tensor(0.0695, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 145 10gs tensor(0.0984, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 146 10gs tensor(0.0147, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 147 10gs tensor(0.3013, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 148 10gs tensor(0.1087, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 149 10gs tensor(0.0103, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 150 10gs tensor(0.1125, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 151 10gs tensor(0.0973, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 152 10gs tensor(0.0016, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 153 10gs tensor(0.1296, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 154 10gs tensor(0.1536, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 155 10gs tensor(0.0144, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 156 10gs tensor(0.0341, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 157 10gs tensor(0.0113, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 158 10gs tensor(0.0089, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 159 10gs tensor(0.0041, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 160 10gs tensor(0.0068, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 161 10gs tensor(0.0053, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 162 10gs tensor(0.1052, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 163 10gs tensor(0.0107, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 164 10gs tensor(0.0567, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 165 10gs tensor(0.0225, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 166 10gs tensor(0.0174, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 167 10gs tensor(0.0868, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 168 10gs tensor(0.0316, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 169 10gs tensor(0.0059, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 170 10gs tensor(0.0990, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 171 10gs tensor(0.0343, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 172 10gs tensor(0.0050, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 173 10gs tensor(0.0507, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 174 10gs tensor(0.0186, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 175 10gs tensor(0.6516, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 176 10gs tensor(0.3114, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 177 10gs tensor(0.1406, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 178 10gs tensor(0.0057, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 179 10gs tensor(0.2684, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 180 10gs tensor(0.2523, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 181 10gs tensor(0.0021, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 182 10gs tensor(0.0798, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 183 10gs tensor(0.2475, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 184 10gs tensor(0.0009, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 185 10gs tensor(0.1834, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 186 10gs tensor(0.2577, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 187 10gs tensor(0.0447, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 188 10gs tensor(0.0528, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 189 10gs tensor(0.1930, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 190 10gs tensor(0.0021, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 191 10gs tensor(0.1167, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 192 10gs tensor(0.1568, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 193 10gs tensor(0.0918, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 194 10gs tensor(0.1081, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 195 10gs tensor(0.1897, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 196 10gs tensor(0.0165, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 197 10gs tensor(0.1584, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 198 10gs tensor(0.0266, grad_fn=<HuberLossBackward0>)\n",
      "fake LYS44A 199 10gs tensor(0.0028, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fb1f066b82f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#margin, negative slope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#out = torch.mean(x, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-dfad182ad467>\u001b[0m in \u001b[0;36mloop\u001b[0;34m(nepochs, coors, Hs, ion_labels, model, optimizer, negative_slope)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboltzmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#apply activation (output raw numbers), and softmax (output nonnormalized probabilities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrastive_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mion_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "outs={}\n",
    "\n",
    "for pdb in pdbs:\n",
    "    #print(pdb)\n",
    "    maxes, Ns, scs=[],[],[]\n",
    "\n",
    "    d = all_data[pdb]\n",
    "    pos, hs, ions = d[\"pos\"], d[\"Hs\"], d[\"ions\"]\n",
    "    outie = {}\n",
    "\n",
    "    \n",
    "    for sample in pos.keys():\n",
    "\n",
    "        coords = torch.tensor(tuple(pos[sample].values()))\n",
    "        #species = torch.tensor(hs[sample], dtype=int) #todo\n",
    "        L = coords.shape[0]\n",
    "        \n",
    "        x = loop(10, coords, torch.tensor(hs[sample], dtype=int), ions[sample], net, optimizer, 1) #margin, negative slope\n",
    "        #out = torch.mean(x, dim=1)\n",
    "        f = torch.mean(x, dim=1).unsqueeze(0).detach().numpy()*coords.detach().numpy().T\n",
    "        #combined = np.column_stack(f).flatten()\n",
    "        \n",
    "        D=np.gradient(np.gradient(np.column_stack(f).flatten())).reshape(-1,3)\n",
    "        #plot(x[0], D)\n",
    "\n",
    "        def loop2(D, sample):\n",
    "            maxes=[]#,[],[]\n",
    "\n",
    "            r = sample[:3]\n",
    "            #out = torch.mean(x, dim=1)\n",
    "            \n",
    "\n",
    "            loss = torch.nn.HuberLoss()\n",
    "            \n",
    "            optimizer3 = torch.optim.Adam(pnet.parameters(), lr=.1, weight_decay=.1)\n",
    "            \n",
    "\n",
    "            ins = torch.ones(L)\n",
    "            ins[0]=0\n",
    "            ins[-1] = 2#,10,10\n",
    "\n",
    "            \n",
    "\n",
    "            for epoch in range(200):\n",
    "                optimizer3.zero_grad()\n",
    "                y=pnet(torch.tensor(ins, dtype=int).unsqueeze(0), torch.tensor(D).unsqueeze(0))[0]\n",
    "                label=labels[r]\n",
    "                loss2=loss(max(y[0][1::]), torch.tensor(label))\n",
    "                loss2.backward()\n",
    "                print(\"fake\", sample, epoch, pdb, loss2)\n",
    "                optimizer3.step()\n",
    "\n",
    "            #a=y[0].detach().numpy()\n",
    "            #maxes.append((np.where(a == max(a[1::]))[0], max(a[1::])))\n",
    "            #maxes.append(max(a[1::]))\n",
    "            #Ns.append(a[0])\n",
    "            #scs.append(a[-1])\n",
    "            #print(loss2.item())\n",
    "\n",
    "\n",
    "            return max(y[0].detach().numpy()[1::]) # maxes#, Ns, scs\n",
    "        \n",
    "        \n",
    "        #m = loop2(D, sample)\n",
    "        #print(sample)\n",
    "\n",
    "        outie[sample] = loop2(D, sample)\n",
    "\n",
    "    outs[pdb] = outie\n",
    "        #coords = torch.tensor()\n",
    "    #x[\"ions\"], x[\"\"]\n",
    "    #n#et, optimizer = model(x[\"lengths, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot call get_autograd_meta() on undefined tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-7f98ae58018c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m#net, optimizer = model(L, 3, 2, .01, .01) # dim, depth, lr, weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#margin, negative slope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;31m#out = torch.mean(x, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-856ba090949b>\u001b[0m in \u001b[0;36mloop\u001b[0;34m(nepochs, coors, Hs, ion_labels, model, optimizer, negative_slope)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#can turn off return po\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboltzmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#apply activation (output raw numbers), and softmax (output nonnormalized probabilities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    211\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot call get_autograd_meta() on undefined tensor"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ds.keys())\n",
    "len(fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"108l\"][\"targets\"].keys() == all_data[\"111l\"][\"targets\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet = EGNN_Network(\n",
    "        num_tokens = 6, #vocabulary siye, number of unique species\n",
    "        num_positions = 23,  #number of nodes         # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "        dim = 2,# #internal rep size. c has square dependence. richer rep but overfitting for small d.s.\n",
    "        depth = 3, #number of layers #deeper need more memort to store intermediate reps\n",
    "        num_nearest_neighbors = 2, #number of nearest neighbors to consider #make this the max hood size\n",
    "        dropout=.1,\n",
    "        m_pool_method='mean')\n",
    "\n",
    "optimizer3 = torch.optim.Adam(pnet.parameters(), lr=.001, weight_decay=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrhoernschemeyer/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot call get_autograd_meta() on undefined tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-a23d88b9989d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#net, optimizer = model(L, 3, 2, .01, .01) # dim, depth, lr, weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#margin, negative slope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#out = torch.mean(x, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-856ba090949b>\u001b[0m in \u001b[0;36mloop\u001b[0;34m(nepochs, coors, Hs, ion_labels, model, optimizer, negative_slope)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#can turn off return po\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboltzmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#apply activation (output raw numbers), and softmax (output nonnormalized probabilities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    211\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot call get_autograd_meta() on undefined tensor"
     ]
    }
   ],
   "source": [
    "for pdb in [pdbs[5]]:\n",
    "    \n",
    "    maxes, Ns, scs, outie =[],[],[],{}\n",
    "\n",
    "    d = all_data[pdb]\n",
    "    pos, hs, ions, T = d[\"pos\"], d[\"Hs\"], d[\"ions\"], d[\"targets\"]\n",
    "    \n",
    "    for sample in pos.keys():\n",
    "\n",
    "        coords = torch.tensor(tuple(pos[sample].values()))\n",
    "        #species = torch.tensor(hs[sample], dtype=int) #todo\n",
    "        L = coords.shape[0]\n",
    "        #net, optimizer = model(L, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "        x = loop(10, coords, torch.tensor(hs[sample], dtype=int), ions[sample], net, optimizer, 1) #margin, negative slope\n",
    "        #out = torch.mean(x, dim=1)\n",
    "        f = torch.mean(x, dim=1).unsqueeze(0).detach().numpy()*coords.detach().numpy().T\n",
    "        #combined = np.column_stack(f).flatten()\n",
    "        \n",
    "        D=np.gradient(np.gradient(np.column_stack(f).flatten())).reshape(-1,3)\n",
    "\n",
    "        print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, optimizer = model(23, 3, 2, .01, .01) # dim, depth, lr, weight decay#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(all_data[\"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdbs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-97d491b9954d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpdb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#pdb=\"104l\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdbs' is not defined"
     ]
    }
   ],
   "source": [
    "#r = sample[:3]\n",
    "            #out = torch.mean(x, dim=1)\n",
    "\n",
    "Ds = {}\n",
    "for pdb in pdbs[:15]:\n",
    "    #pdb=\"104l\"\n",
    "    \n",
    "    maxes, Ns, scs =[],[],[]\n",
    "\n",
    "    d = all_data[pdb]\n",
    "    \n",
    "    pos, hs, ions = d[\"pos\"], d[\"Hs\"], d[\"ions\"]#, d[\"targets\"]\n",
    "    \n",
    "    \n",
    "    for sample in pos.keys():\n",
    "        label=all_targets[pdb][sample]\n",
    "\n",
    "        coords = torch.tensor(tuple(pos[sample].values()))\n",
    "        #species = torch.tensor(hs[sample], dtype=int) #todo\n",
    "        L = coords.shape[0]\n",
    "        print(sample)\n",
    "        #net, optimizer = model(L, 3, 2, .01, .01) # dim, depth, lr, weight decay\n",
    "        x = loop(10, coords, torch.tensor(hs[sample], dtype=int), ions[sample], net, optimizer, 1) #margin, negative slope\n",
    "        print(\"\")\n",
    "        #out = torch.mean(x, dim=1)\n",
    "        #f = torch.mean(x, dim=1).unsqueeze(0).detach().numpy()*coords.detach().numpy().T\n",
    "        #combined = np.column_stack(f).flatten()\n",
    "        \n",
    "        #D=np.gradient(np.gradient(np.column_stack(f).flatten())).reshape(-1,3)\n",
    "        Ds[pdb] = np.gradient(np.gradient(np.column_stack(torch.mean(x, dim=1).unsqueeze(0).detach().numpy()*coords.detach().numpy().T).flatten())).reshape(-1,3)\n",
    "        \n",
    "        #loss = torch.nn.HuberLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    #losses[pdb] = loss2\n",
    "\n",
    "    #outs2[pdb] = outie\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'104l'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-8ae1d8981ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"104l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '104l'"
     ]
    }
   ],
   "source": [
    "all_targets[\"104l\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GLU5A': {0: (32.009, 18.561, -13.362),\n",
       "  1: (31.918, 18.195, -14.31),\n",
       "  2: (33.354, 19.012, -12.992),\n",
       "  3: (33.895, 18.202, -12.503),\n",
       "  4: (33.234, 20.252, -12.077),\n",
       "  5: (33.977, 20.384, -11.109),\n",
       "  6: (34.173, 19.347, -14.254),\n",
       "  7: (33.75, 20.031, -14.99),\n",
       "  8: (34.326, 18.509, -14.934),\n",
       "  9: (35.521, 20.04, -14.013),\n",
       "  10: (35.462, 21.106, -13.792),\n",
       "  11: (36.077, 20.073, -14.95),\n",
       "  12: (36.543, 19.16, -13.319),\n",
       "  13: (36.366, 17.925, -13.308),\n",
       "  14: (37.539, 19.697, -12.792)},\n",
       " 'GLU7A': {0: (30.947, 20.938, -9.879),\n",
       "  1: (30.523, 20.545, -10.719),\n",
       "  2: (30.587, 20.503, -8.528),\n",
       "  3: (30.246, 21.427, -8.061),\n",
       "  4: (31.836, 19.916, -7.825),\n",
       "  5: (32.106, 20.238, -6.665),\n",
       "  6: (29.422, 19.502, -8.573),\n",
       "  7: (29.664, 18.668, -9.232),\n",
       "  8: (29.241, 19.097, -7.577),\n",
       "  9: (28.083, 20.17, -8.944),\n",
       "  10: (28.015, 20.802, -9.83),\n",
       "  11: (27.639, 20.75, -8.135),\n",
       "  12: (26.945, 19.185, -9.247),\n",
       "  13: (27.206, 18.058, -9.707),\n",
       "  14: (25.779, 19.559, -9.039)},\n",
       " 'HIS13A': {0: (37.845, 22.512, -3.534),\n",
       "  1: (36.936, 22.631, -3.981),\n",
       "  2: (38.335, 23.835, -3.106),\n",
       "  3: (39.359, 23.919, -3.47),\n",
       "  4: (38.185, 23.933, -1.593),\n",
       "  5: (39.118, 24.339, -0.906),\n",
       "  6: (37.574, 24.969, -3.795),\n",
       "  7: (37.822, 25.093, -4.849),\n",
       "  8: (36.488, 25.065, -3.789),\n",
       "  9: (38.011, 26.343, -3.37),\n",
       "  10: (39.271, 26.864, -3.591),\n",
       "  11: (39.758, 26.825, -4.486),\n",
       "  12: (37.317, 27.324, -2.741),\n",
       "  13: (36.682, 27.268, -1.869),\n",
       "  14: (39.298, 28.113, -3.105),\n",
       "  15: (39.778, 28.991, -3.512),\n",
       "  16: (38.134, 28.435, -2.581)},\n",
       " 'LYS17A': {0: (41.151, 23.533, 2.004),\n",
       "  1: (40.572, 23.896, 1.247),\n",
       "  2: (41.196, 24.328, 3.241),\n",
       "  3: (42.134, 24.865, 3.384),\n",
       "  4: (41.22, 23.373, 4.426),\n",
       "  5: (41.983, 23.558, 5.357),\n",
       "  6: (39.984, 25.276, 3.376),\n",
       "  7: (39.026, 24.827, 3.638),\n",
       "  8: (40.295, 25.922, 4.197),\n",
       "  9: (39.761, 26.213, 2.191),\n",
       "  10: (39.046, 26.938, 2.58),\n",
       "  11: (39.29, 25.909, 1.256),\n",
       "  12: (41.008, 26.993, 1.843),\n",
       "  13: (41.338, 27.663, 2.637),\n",
       "  14: (41.911, 26.46, 1.544),\n",
       "  15: (40.758, 27.838, 0.606),\n",
       "  16: (40.106, 27.496, -0.198),\n",
       "  17: (40.224, 28.733, 0.924),\n",
       "  18: (41.985, 28.498, 0.08),\n",
       "  19: (42.223, 28.411, -0.929),\n",
       "  20: (42.862, 28.076, 0.447),\n",
       "  21: (42.236, 29.503, 0.169)},\n",
       " 'GLU19A': {0: (42.413, 20.457, 4.548),\n",
       "  1: (42.355, 20.958, 3.661),\n",
       "  2: (43.748, 19.84, 4.621),\n",
       "  3: (43.477, 18.987, 5.243),\n",
       "  4: (44.832, 20.701, 5.271),\n",
       "  5: (45.913, 20.203, 5.547),\n",
       "  6: (44.207, 19.366, 3.247),\n",
       "  7: (44.52, 20.123, 2.528),\n",
       "  8: (45.211, 18.964, 3.38),\n",
       "  9: (43.375, 18.22, 2.731),\n",
       "  10: (43.651, 17.372, 3.358),\n",
       "  11: (42.304, 18.357, 2.58),\n",
       "  12: (43.897, 17.652, 1.428),\n",
       "  13: (44.486, 18.411, 0.628),\n",
       "  14: (43.678, 16.445, 1.214)},\n",
       " 'ASP21A': {0: (44.606, 21.764, 8.1),\n",
       "  1: (43.711, 22.102, 7.746),\n",
       "  2: (44.593, 21.319, 9.492),\n",
       "  3: (45.627, 21.122, 9.774),\n",
       "  4: (43.765, 20.017, 9.604),\n",
       "  5: (42.65, 20.028, 10.143),\n",
       "  6: (43.985, 22.422, 10.377),\n",
       "  7: (44.885, 23.017, 10.532),\n",
       "  8: (43.174, 23.033, 9.981),\n",
       "  9: (43.932, 22.026, 11.851),\n",
       "  10: (44.679, 21.107, 12.253),\n",
       "  11: (43.102, 22.596, 12.596)},\n",
       " 'HIS25A': {0: (40.305, 19.444, 12.078),\n",
       "  1: (41.027, 20.026, 11.653),\n",
       "  2: (39.017, 19.161, 11.464),\n",
       "  3: (38.242, 19.889, 11.702),\n",
       "  4: (38.531, 17.787, 11.911),\n",
       "  5: (37.354, 17.63, 12.198),\n",
       "  6: (39.092, 19.224, 9.938),\n",
       "  7: (40.034, 18.812, 9.578),\n",
       "  8: (38.195, 18.83, 9.461),\n",
       "  9: (39.164, 20.62, 9.404),\n",
       "  10: (40.323, 21.358, 9.335),\n",
       "  11: (38.178, 21.437, 8.941),\n",
       "  12: (37.103, 21.335, 8.921),\n",
       "  13: (40.016, 22.573, 8.844),\n",
       "  14: (40.599, 23.404, 8.476),\n",
       "  15: (38.727, 22.669, 8.588),\n",
       "  16: (38.233, 23.477, 8.21)},\n",
       " 'ASP28A': {0: (37.035, 18.024, 15.439),\n",
       "  1: (37.771, 18.725, 15.521),\n",
       "  2: (35.738, 18.696, 15.375),\n",
       "  3: (35.57, 18.949, 16.422),\n",
       "  4: (34.614, 17.819, 14.82),\n",
       "  5: (33.496, 17.821, 15.337),\n",
       "  6: (35.834, 19.959, 14.501),\n",
       "  7: (36.297, 19.885, 13.517),\n",
       "  8: (34.843, 20.406, 14.578),\n",
       "  9: (36.526, 21.101, 15.187),\n",
       "  10: (36.916, 20.959, 16.365),\n",
       "  11: (36.674, 22.157, 14.539)},\n",
       " 'LYS35A': {0: (28.605, 13.768, 19.956),\n",
       "  1: (29.517, 13.989, 20.356),\n",
       "  2: (27.709, 14.581, 20.754),\n",
       "  3: (27.507, 14.211, 21.759),\n",
       "  4: (26.285, 14.582, 20.183),\n",
       "  5: (25.316, 14.49, 20.933),\n",
       "  6: (28.226, 16.009, 20.85),\n",
       "  7: (27.433, 16.52, 20.304),\n",
       "  8: (28.996, 16.205, 20.104),\n",
       "  9: (29.352, 16.21, 21.843),\n",
       "  10: (30.384, 15.902, 21.673),\n",
       "  11: (29.175, 15.563, 22.702),\n",
       "  12: (29.296, 17.59, 22.508),\n",
       "  13: (28.795, 17.356, 23.447),\n",
       "  14: (30.32, 17.72, 22.857),\n",
       "  15: (28.946, 18.706, 21.515),\n",
       "  16: (28.235, 19.044, 20.761),\n",
       "  17: (29.7, 18.485, 20.759),\n",
       "  18: (29.343, 20.066, 21.991),\n",
       "  19: (28.562, 20.377, 22.603),\n",
       "  20: (29.613, 21.06, 21.847),\n",
       "  21: (29.998, 19.932, 22.788)},\n",
       " 'HIS37A': {0: (24.91, 12.331, 18.143),\n",
       "  1: (25.765, 12.234, 18.691),\n",
       "  2: (24.374, 10.984, 18.095),\n",
       "  3: (23.35, 10.988, 18.468),\n",
       "  4: (25.244, 10.059, 18.933),\n",
       "  5: (26.044, 9.274, 18.396),\n",
       "  6: (24.336, 10.503, 16.646),\n",
       "  7: (23.775, 9.571, 16.58),\n",
       "  8: (25.356, 10.524, 16.262),\n",
       "  9: (23.663, 11.456, 15.704),\n",
       "  10: (22.304, 11.525, 15.512),\n",
       "  11: (24.194, 12.376, 14.86),\n",
       "  12: (25.235, 12.626, 14.718),\n",
       "  13: (22.059, 12.45, 14.583),\n",
       "  14: (21.13, 12.95, 14.352),\n",
       "  15: (23.175, 12.998, 14.155),\n",
       "  16: (23.271, 13.729, 13.45)},\n",
       " 'GLU39A': {0: (24.868, 7.328, 20.284),\n",
       "  1: (24.022, 7.498, 20.827),\n",
       "  2: (24.811, 5.894, 20.006),\n",
       "  3: (24.794, 5.451, 21.002),\n",
       "  4: (25.995, 5.436, 19.153),\n",
       "  5: (26.485, 4.316, 19.302),\n",
       "  6: (23.465, 5.506, 19.36),\n",
       "  7: (22.625, 5.675, 20.034),\n",
       "  8: (23.336, 4.427, 19.451),\n",
       "  9: (23.176, 6.136, 18.016),\n",
       "  10: (23.699, 6.319, 17.077),\n",
       "  11: (22.299, 5.506, 17.863),\n",
       "  12: (22.352, 7.414, 18.099),\n",
       "  13: (22.519, 8.2, 19.054),\n",
       "  14: (21.522, 7.631, 17.192)},\n",
       " 'GLU42A': {0: (29.14, 4.151, 20.866),\n",
       "  1: (28.469, 4.717, 21.385),\n",
       "  2: (29.048, 2.767, 21.345),\n",
       "  3: (29.353, 2.768, 22.391),\n",
       "  4: (29.796, 1.78, 20.45),\n",
       "  5: (30.175, 0.694, 20.893),\n",
       "  6: (27.574, 2.321, 21.424),\n",
       "  7: (26.987, 2.116, 20.529),\n",
       "  8: (27.622, 1.293, 21.782),\n",
       "  9: (26.698, 3.079, 22.426),\n",
       "  10: (26.614, 4.102, 22.792),\n",
       "  11: (26.877, 2.632, 23.404),\n",
       "  12: (25.216, 2.687, 22.343),\n",
       "  13: (24.874, 1.502, 22.552),\n",
       "  14: (24.379, 3.573, 22.079)},\n",
       " 'LYS43A': {0: (29.97, 2.137, 19.181),\n",
       "  1: (29.246, 2.789, 18.879),\n",
       "  2: (30.65, 1.269, 18.224),\n",
       "  3: (30.449, 0.249, 18.551),\n",
       "  4: (32.159, 1.247, 18.384),\n",
       "  5: (32.843, 0.428, 17.781),\n",
       "  6: (30.302, 1.703, 16.798),\n",
       "  7: (30.98, 1.36, 16.016),\n",
       "  8: (30.486, 2.776, 16.746),\n",
       "  9: (28.866, 1.414, 16.389),\n",
       "  10: (28.628, 1.995, 15.498),\n",
       "  11: (28.115, 1.637, 17.147),\n",
       "  12: (28.66, -0.083, 16.148),\n",
       "  13: (28.701, -0.657, 17.074),\n",
       "  14: (29.356, -0.566, 15.462),\n",
       "  15: (27.247, -0.368, 15.712),\n",
       "  16: (26.896, 0.214, 14.86),\n",
       "  17: (26.455, -0.226, 16.447),\n",
       "  18: (27.088, -1.793, 15.377),\n",
       "  19: (27.437, -2.604, 15.927),\n",
       "  20: (27.071, -1.97, 14.352),\n",
       "  21: (26.094, -2.044, 15.552)},\n",
       " 'ASP45A': {0: (35.043, 0.495, 20.801),\n",
       "  1: (34.811, -0.27, 20.168),\n",
       "  2: (35.522, -0.081, 22.055),\n",
       "  3: (35.048, -0.214, 23.027),\n",
       "  4: (36.655, 0.756, 22.64),\n",
       "  5: (36.811, 0.823, 23.853),\n",
       "  6: (35.966, -1.527, 21.844),\n",
       "  7: (36.837, -1.779, 21.239),\n",
       "  8: (36.298, -2.059, 22.735),\n",
       "  9: (34.82, -2.429, 21.416),\n",
       "  10: (33.647, -2.108, 21.732),\n",
       "  11: (35.093, -3.45, 20.745)},\n",
       " 'LYS48A': {0: (34.475, 4.377, 23.77),\n",
       "  1: (35.038, 3.527, 23.742),\n",
       "  2: (33.214, 4.231, 24.504),\n",
       "  3: (32.294, 4.672, 24.119),\n",
       "  4: (33.18, 4.856, 25.89),\n",
       "  5: (32.123, 5.272, 26.358),\n",
       "  6: (32.896, 2.738, 24.681),\n",
       "  7: (33.732, 2.154, 25.065),\n",
       "  8: (32.257, 2.73, 25.564),\n",
       "  9: (32.1, 2.105, 23.569),\n",
       "  10: (32.223, 2.6, 22.606),\n",
       "  11: (31.082, 2.243, 23.932),\n",
       "  12: (32.315, 0.594, 23.538),\n",
       "  13: (31.398, 0.162, 23.138),\n",
       "  14: (32.937, 0.283, 22.698),\n",
       "  15: (32.229, -0.032, 24.922),\n",
       "  16: (32.917, 0.178, 25.741),\n",
       "  17: (31.276, 0.121, 25.429),\n",
       "  18: (32.399, -1.511, 24.847),\n",
       "  19: (32.825, -1.826, 23.952),\n",
       "  20: (31.498, -2.023, 24.756),\n",
       "  21: (32.807, -2.186, 25.525)},\n",
       " 'HIS49A': {0: (34.32, 4.834, 26.572),\n",
       "  1: (34.966, 4.094, 26.297),\n",
       "  2: (34.423, 5.345, 27.933),\n",
       "  3: (33.629, 4.82, 28.464),\n",
       "  4: (34.416, 6.868, 28.075),\n",
       "  5: (34.089, 7.378, 29.141),\n",
       "  6: (35.679, 4.785, 28.584),\n",
       "  7: (35.803, 5.044, 29.636),\n",
       "  8: (35.752, 3.707, 28.725),\n",
       "  9: (36.926, 5.222, 27.898),\n",
       "  10: (37.262, 4.86, 26.613),\n",
       "  11: (37.411, 4.157, 25.889),\n",
       "  12: (37.888, 6.092, 28.302),\n",
       "  13: (38.271, 6.205, 29.305),\n",
       "  14: (38.385, 5.508, 26.283),\n",
       "  15: (39.355, 5.133, 25.991),\n",
       "  16: (38.803, 6.268, 27.277)},\n",
       " 'LYS51A': {0: (33.414, 10.349, 28.575),\n",
       "  1: (34.117, 10.05, 29.251),\n",
       "  2: (32.177, 10.906, 29.133),\n",
       "  3: (31.282, 10.394, 28.78),\n",
       "  4: (31.809, 12.325, 28.718),\n",
       "  5: (30.629, 12.655, 28.621),\n",
       "  6: (32.242, 10.867, 30.66),\n",
       "  7: (32.837, 11.612, 31.187),\n",
       "  8: (31.214, 11.107, 30.931),\n",
       "  9: (32.5, 9.488, 31.254),\n",
       "  10: (31.649, 8.859, 30.991),\n",
       "  11: (33.501, 9.086, 31.1),\n",
       "  12: (32.688, 9.572, 32.762),\n",
       "  13: (33.515, 10.19, 33.111),\n",
       "  14: (31.857, 10.021, 33.306),\n",
       "  15: (32.974, 8.217, 33.376),\n",
       "  16: (33.136, 8.46, 34.426),\n",
       "  17: (33.918, 7.686, 33.253),\n",
       "  18: (31.79, 7.328, 33.27),\n",
       "  19: (31.317, 7.156, 32.36),\n",
       "  20: (31.495, 6.483, 33.8),\n",
       "  21: (31.014, 7.872, 33.697)},\n",
       " 'GLU53A': {0: (32.994, 16.179, 26.4),\n",
       "  1: (32.412, 16.753, 27.011),\n",
       "  2: (33.693, 16.754, 25.261),\n",
       "  3: (33.495, 15.958, 24.543),\n",
       "  4: (35.117, 17.101, 25.641),\n",
       "  5: (36.027, 16.966, 24.821),\n",
       "  6: (32.987, 18.004, 24.751),\n",
       "  7: (32.055, 17.552, 24.411),\n",
       "  8: (32.546, 18.788, 25.366),\n",
       "  9: (33.786, 18.73, 23.677),\n",
       "  10: (34.364, 19.413, 24.3),\n",
       "  11: (34.173, 18.01, 22.956),\n",
       "  12: (32.981, 19.799, 22.948),\n",
       "  13: (31.76, 19.933, 23.192),\n",
       "  14: (33.581, 20.503, 22.115)},\n",
       " 'GLU55A': {0: (36.994, 15.475, 27.662),\n",
       "  1: (36.322, 15.532, 28.427),\n",
       "  2: (37.746, 14.22, 27.585),\n",
       "  3: (38.675, 14.448, 28.108),\n",
       "  4: (38.012, 13.837, 26.148),\n",
       "  5: (39.079, 13.34, 25.826),\n",
       "  6: (37.01, 13.087, 28.297),\n",
       "  7: (36.059, 12.806, 27.845),\n",
       "  8: (37.646, 12.214, 28.15),\n",
       "  9: (37.103, 13.202, 29.79),\n",
       "  10: (38.164, 13.145, 30.034),\n",
       "  11: (36.71, 13.94, 30.489),\n",
       "  12: (36.389, 12.097, 30.533),\n",
       "  13: (35.696, 11.258, 29.925),\n",
       "  14: (36.532, 12.069, 31.759)},\n",
       " 'LYS57A': {0: (38.336, 15.886, 23.59),\n",
       "  1: (37.963, 16.252, 24.466),\n",
       "  2: (39.365, 16.791, 23.073),\n",
       "  3: (39.391, 16.726, 21.985),\n",
       "  4: (40.766, 16.438, 23.543),\n",
       "  5: (41.737, 16.677, 22.833),\n",
       "  6: (39.077, 18.234, 23.471),\n",
       "  7: (38.944, 18.427, 24.536),\n",
       "  8: (40.033, 18.748, 23.366),\n",
       "  9: (38.087, 18.905, 22.593),\n",
       "  10: (38.643, 19.13, 21.683),\n",
       "  11: (37.136, 18.395, 22.437),\n",
       "  12: (37.781, 20.283, 23.136),\n",
       "  13: (38.661, 20.92, 23.05),\n",
       "  14: (37.6, 20.341, 24.209),\n",
       "  15: (36.611, 20.912, 22.432),\n",
       "  16: (35.66, 20.381, 22.464),\n",
       "  17: (37.005, 21.11, 21.435),\n",
       "  18: (36.385, 22.282, 22.953),\n",
       "  19: (36.655, 23.156, 22.459),\n",
       "  20: (36.756, 22.483, 23.904),\n",
       "  21: (35.403, 22.594, 23.092)},\n",
       " 'GLU60A': {0: (44.396, 11.667, 22.742),\n",
       "  1: (44.878, 12.147, 23.502),\n",
       "  2: (45.291, 11.68, 21.59),\n",
       "  3: (45.226, 12.673, 21.145),\n",
       "  4: (44.883, 10.591, 20.595),\n",
       "  5: (45.022, 10.759, 19.385),\n",
       "  6: (46.746, 11.495, 22.033),\n",
       "  7: (47.066, 11.924, 22.982),\n",
       "  8: (47.141, 10.496, 22.216),\n",
       "  9: (47.78, 12.101, 21.073),\n",
       "  10: (48.743, 11.895, 21.54),\n",
       "  11: (47.923, 11.747, 20.052),\n",
       "  12: (47.813, 13.633, 21.092),\n",
       "  13: (47.155, 14.253, 21.956),\n",
       "  14: (48.515, 14.222, 20.239)},\n",
       " 'ASP61A': {0: (44.347, 9.492, 21.117),\n",
       "  1: (45.086, 9.108, 21.706),\n",
       "  2: (43.9, 8.38, 20.287),\n",
       "  3: (44.577, 7.852, 19.616),\n",
       "  4: (42.769, 8.79, 19.365),\n",
       "  5: (42.754, 8.411, 18.199),\n",
       "  6: (43.441, 7.209, 21.155),\n",
       "  7: (43.204, 7.411, 22.2),\n",
       "  8: (42.736, 6.554, 20.644),\n",
       "  9: (44.562, 6.236, 21.471),\n",
       "  10: (45.756, 6.607, 21.384),\n",
       "  11: (44.238, 5.084, 21.812)},\n",
       " 'LYS63A': {0: (42.1, 11.843, 18.319),\n",
       "  1: (42.346, 12.092, 19.277),\n",
       "  2: (42.651, 12.756, 17.331),\n",
       "  3: (41.756, 13.233, 16.932),\n",
       "  4: (43.382, 12.01, 16.209),\n",
       "  5: (43.235, 12.358, 15.03),\n",
       "  6: (43.598, 13.767, 17.985),\n",
       "  7: (44.478, 13.165, 18.211),\n",
       "  8: (43.315, 14.264, 18.913),\n",
       "  9: (44.201, 14.751, 17.005),\n",
       "  10: (44.946, 14.331, 16.329),\n",
       "  11: (43.463, 15.375, 16.5),\n",
       "  12: (44.922, 15.861, 17.733),\n",
       "  13: (45.39, 16.605, 17.088),\n",
       "  14: (44.356, 16.506, 18.405),\n",
       "  15: (46.108, 15.329, 18.503),\n",
       "  16: (45.942, 14.626, 19.319),\n",
       "  17: (46.854, 14.922, 17.82),\n",
       "  18: (46.751, 16.396, 19.326),\n",
       "  19: (47.036, 16.021, 20.253),\n",
       "  20: (46.327, 17.322, 19.537),\n",
       "  21: (47.676, 16.792, 19.064)},\n",
       " 'LYS64A': {0: (44.135, 10.97, 16.562),\n",
       "  1: (44.132, 10.543, 17.488),\n",
       "  2: (44.854, 10.212, 15.549),\n",
       "  3: (45.47, 10.909, 14.981),\n",
       "  4: (43.851, 9.533, 14.62),\n",
       "  5: (44.018, 9.571, 13.395),\n",
       "  6: (45.786, 9.157, 16.185),\n",
       "  7: (46.031, 8.411, 15.429),\n",
       "  8: (45.416, 8.732, 17.118),\n",
       "  9: (46.98, 9.785, 16.895),\n",
       "  10: (47.067, 10.499, 17.714),\n",
       "  11: (47.437, 10.462, 16.173),\n",
       "  12: (48.172, 8.845, 17.018),\n",
       "  13: (48.665, 8.496, 16.111),\n",
       "  14: (49.049, 9.328, 17.449),\n",
       "  15: (47.808, 7.558, 17.747),\n",
       "  16: (47.419, 6.795, 17.073),\n",
       "  17: (48.741, 7.002, 17.842),\n",
       "  18: (47.33, 7.795, 19.137),\n",
       "  19: (48.043, 8.494, 19.429),\n",
       "  20: (46.866, 7.211, 19.861),\n",
       "  21: (46.529, 8.453, 19.056)},\n",
       " 'HIS65A': {0: (42.781, 8.979, 15.197),\n",
       "  1: (42.423, 9.037, 16.15),\n",
       "  2: (41.78, 8.297, 14.386),\n",
       "  3: (42.297, 7.525, 13.816),\n",
       "  4: (41.068, 9.251, 13.451),\n",
       "  5: (40.73, 8.864, 12.337),\n",
       "  6: (40.755, 7.548, 15.219),\n",
       "  7: (40.007, 8.209, 15.656),\n",
       "  8: (41.218, 6.812, 15.876),\n",
       "  9: (39.951, 6.565, 14.42),\n",
       "  10: (40.554, 5.496, 13.799),\n",
       "  11: (41.243, 4.766, 13.98),\n",
       "  12: (38.638, 6.612, 14.062),\n",
       "  13: (37.815, 7.023, 14.628),\n",
       "  14: (39.605, 4.92, 13.078),\n",
       "  15: (39.855, 4.137, 12.377),\n",
       "  16: (38.428, 5.558, 13.205)},\n",
       " 'LYS78A': {0: (39.118, 12.907, -3.892),\n",
       "  1: (39.544, 12.807, -2.97),\n",
       "  2: (40.062, 13.264, -4.961),\n",
       "  3: (39.896, 14.235, -5.428),\n",
       "  4: (39.984, 12.405, -6.238),\n",
       "  5: (40.494, 12.801, -7.293),\n",
       "  6: (41.48, 13.319, -4.429),\n",
       "  7: (42.175, 13.281, -5.268),\n",
       "  8: (41.801, 12.4, -3.939),\n",
       "  9: (41.744, 14.553, -3.585),\n",
       "  10: (40.968, 14.851, -2.88),\n",
       "  11: (41.88, 15.365, -4.3),\n",
       "  12: (43.054, 14.429, -2.857),\n",
       "  13: (43.15, 13.487, -2.317),\n",
       "  14: (43.854, 14.427, -3.597),\n",
       "  15: (43.239, 15.572, -1.899),\n",
       "  16: (43.196, 16.608, -2.234),\n",
       "  17: (42.435, 15.525, -1.165),\n",
       "  18: (44.49, 15.414, -1.106),\n",
       "  19: (44.772, 14.426, -1.268),\n",
       "  20: (44.371, 15.84, -0.165),\n",
       "  21: (45.451, 15.764, -1.292)},\n",
       " 'LYS79A': {0: (39.361, 11.232, -6.132),\n",
       "  1: (39.54, 10.858, -5.2),\n",
       "  2: (39.161, 10.349, -7.285),\n",
       "  3: (39.95, 10.564, -8.006),\n",
       "  4: (37.999, 10.848, -8.152),\n",
       "  5: (37.769, 10.307, -9.239),\n",
       "  6: (38.862, 8.911, -6.842),\n",
       "  7: (37.939, 8.897, -6.262),\n",
       "  8: (38.756, 8.447, -7.822),\n",
       "  9: (39.99, 8.237, -6.059),\n",
       "  10: (39.652, 7.223, -5.846),\n",
       "  11: (40.239, 8.481, -5.026),\n",
       "  12: (41.324, 8.373, -6.762),\n",
       "  13: (41.684, 9.392, -6.903),\n",
       "  14: (42.137, 7.986, -6.148),\n",
       "  15: (41.368, 7.623, -8.076),\n",
       "  16: (40.957, 8.141, -8.943),\n",
       "  17: (42.421, 7.774, -8.312),\n",
       "  18: (41.24, 6.146, -7.88),\n",
       "  19: (40.31, 5.948, -7.459),\n",
       "  20: (40.984, 5.567, -8.705),\n",
       "  21: (41.914, 5.967, -7.108)},\n",
       " 'LYS80A': {0: (37.239, 11.832, -7.642),\n",
       "  1: (37.496, 12.43, -6.857),\n",
       "  2: (36.104, 12.418, -8.37),\n",
       "  3: (35.469, 12.908, -7.632),\n",
       "  4: (35.122, 11.403, -8.977),\n",
       "  5: (34.765, 11.491, -10.156),\n",
       "  6: (36.631, 13.36, -9.448),\n",
       "  7: (35.884, 13.714, -10.158),\n",
       "  8: (37.293, 12.784, -10.094),\n",
       "  9: (37.301, 14.605, -8.851),\n",
       "  10: (36.582, 15.275, -8.379),\n",
       "  11: (38.057, 14.339, -8.112),\n",
       "  12: (38.272, 15.263, -9.792),\n",
       "  13: (38.807, 16.091, -9.326),\n",
       "  14: (39.062, 14.612, -10.166),\n",
       "  15: (37.628, 15.813, -11.035),\n",
       "  16: (37.513, 15.012, -11.766),\n",
       "  17: (36.669, 16.329, -10.977),\n",
       "  18: (38.601, 16.748, -11.755),\n",
       "  19: (39.428, 16.995, -11.174),\n",
       "  20: (38.318, 17.73, -11.947),\n",
       "  21: (38.887, 16.448, -12.709)},\n",
       " 'HIS82A': {0: (35.662, 8.03, -9.353),\n",
       "  1: (36.393, 8.723, -9.196),\n",
       "  2: (36.315, 6.756, -9.743),\n",
       "  3: (35.632, 6.144, -10.332),\n",
       "  4: (37.013, 6.182, -8.514),\n",
       "  5: (38.229, 5.984, -8.511),\n",
       "  6: (37.354, 7.028, -10.834),\n",
       "  7: (38.241, 7.644, -10.683),\n",
       "  8: (37.679, 6.073, -11.247),\n",
       "  9: (36.804, 7.747, -12.025),\n",
       "  10: (36.766, 9.12, -12.151),\n",
       "  11: (36.27, 7.257, -13.173),\n",
       "  12: (35.632, 6.476, -13.56),\n",
       "  13: (36.224, 9.412, -13.336),\n",
       "  14: (36.089, 10.222, -14.037),\n",
       "  15: (35.905, 8.32, -14.001),\n",
       "  16: (35.48, 8.263, -14.927)},\n",
       " 'HIS83A': {0: (36.215, 5.867, -7.495),\n",
       "  1: (35.316, 6.342, -7.418),\n",
       "  2: (36.729, 5.426, -6.205),\n",
       "  3: (37.782, 5.605, -5.988),\n",
       "  4: (36.486, 3.962, -5.84),\n",
       "  5: (36.614, 3.58, -4.67),\n",
       "  6: (36.12, 6.341, -5.125),\n",
       "  7: (36.224, 5.99, -4.098),\n",
       "  8: (36.455, 7.372, -5.239),\n",
       "  9: (34.623, 6.399, -5.164),\n",
       "  10: (33.891, 7.548, -4.981),\n",
       "  11: (33.709, 5.414, -5.39),\n",
       "  12: (33.821, 4.34, -5.412),\n",
       "  13: (32.587, 7.236, -5.101),\n",
       "  14: (31.776, 7.636, -4.511),\n",
       "  15: (32.43, 5.951, -5.35),\n",
       "  16: (31.55, 5.454, -5.485)},\n",
       " 'GLU84A': {0: (36.107, 3.159, -6.827),\n",
       "  1: (36.041, 3.634, -7.727),\n",
       "  2: (35.833, 1.737, -6.606),\n",
       "  3: (34.992, 1.806, -5.916),\n",
       "  4: (36.925, 1.001, -5.803),\n",
       "  5: (36.626, 0.325, -4.825),\n",
       "  6: (35.589, 1.033, -7.957),\n",
       "  7: (35.092, 0.153, -7.548),\n",
       "  8: (34.946, 1.469, -8.722),\n",
       "  9: (36.673, 1.293, -9.037),\n",
       "  10: (36.374, 0.275, -9.288),\n",
       "  11: (37.666, 1.366, -9.48),\n",
       "  12: (36.633, 2.71, -9.627),\n",
       "  13: (35.52, 3.259, -9.788),\n",
       "  14: (37.714, 3.269, -9.933)},\n",
       " 'GLU86A': {0: (39.221, 2.163, -3.715),\n",
       "  1: (38.96, 2.713, -4.533),\n",
       "  2: (39.282, 2.682, -2.347),\n",
       "  3: (40.199, 2.258, -1.936),\n",
       "  4: (38.1, 2.241, -1.502),\n",
       "  5: (38.24, 2.015, -0.299),\n",
       "  6: (39.33, 4.217, -2.346),\n",
       "  7: (39.305, 4.441, -1.279),\n",
       "  8: (38.598, 4.805, -2.899),\n",
       "  9: (40.646, 4.843, -2.837),\n",
       "  10: (41.538, 4.318, -2.495),\n",
       "  11: (40.765, 5.906, -2.628),\n",
       "  12: (40.841, 4.789, -4.348),\n",
       "  13: (39.877, 4.516, -5.103),\n",
       "  14: (41.969, 5.052, -4.799)},\n",
       " 'LYS88A': {0: (36.275, -0.591, -1.714),\n",
       "  1: (36.66, -0.367, -2.632),\n",
       "  2: (36.279, -2.013, -1.381),\n",
       "  3: (35.217, -2.206, -1.533),\n",
       "  4: (36.738, -2.381, 0.027),\n",
       "  5: (35.993, -3.007, 0.759),\n",
       "  6: (37.055, -2.816, -2.424),\n",
       "  7: (37.253, -3.759, -1.914),\n",
       "  8: (38.031, -2.472, -2.765),\n",
       "  9: (36.232, -3.236, -3.634),\n",
       "  10: (35.531, -4.027, -3.369),\n",
       "  11: (35.624, -2.466, -4.109),\n",
       "  12: (37.164, -3.894, -4.662),\n",
       "  13: (37.707, -4.709, -4.183),\n",
       "  14: (37.902, -3.264, -5.158),\n",
       "  15: (36.406, -4.511, -5.817),\n",
       "  16: (35.825, -3.85, -6.46),\n",
       "  17: (35.723, -5.297, -5.495),\n",
       "  18: (37.334, -5.199, -6.774),\n",
       "  19: (37.183, -5.115, -7.8),\n",
       "  20: (38.253, -4.72, -6.689),\n",
       "  21: (37.513, -6.212, -6.62)},\n",
       " 'HIS94A': {0: (33.869, -1.972, 7.452),\n",
       "  1: (33.888, -1.892, 6.435),\n",
       "  2: (32.718, -1.369, 8.122),\n",
       "  3: (33.081, -1.182, 9.133),\n",
       "  4: (31.457, -2.224, 8.103),\n",
       "  5: (30.61, -2.106, 8.988),\n",
       "  6: (32.467, 0.039, 7.601),\n",
       "  7: (32.311, 0.185, 6.532),\n",
       "  8: (31.584, 0.557, 7.975),\n",
       "  9: (33.551, 0.986, 7.987),\n",
       "  10: (34.872, 0.759, 7.67),\n",
       "  11: (35.316, 0.637, 6.76),\n",
       "  12: (33.533, 2.093, 8.758),\n",
       "  13: (32.756, 2.798, 8.502),\n",
       "  14: (35.622, 1.688, 8.229),\n",
       "  15: (36.69, 1.845, 8.274),\n",
       "  16: (34.834, 2.509, 8.895)},\n",
       " 'LYS97A': {0: (33.395, -6.133, 9.995),\n",
       "  1: (34.073, -6.182, 9.234),\n",
       "  2: (34.063, -6.063, 11.277),\n",
       "  3: (34.089, -7.012, 11.812),\n",
       "  4: (33.374, -5.109, 12.257),\n",
       "  5: (32.948, -5.521, 13.334),\n",
       "  6: (35.525, -5.669, 11.082),\n",
       "  7: (35.787, -4.708, 10.638),\n",
       "  8: (35.938, -6.431, 10.421),\n",
       "  9: (36.41, -5.906, 12.3),\n",
       "  10: (35.956, -5.916, 13.291),\n",
       "  11: (36.711, -6.954, 12.301),\n",
       "  12: (37.63, -5.026, 12.206),\n",
       "  13: (38.029, -4.611, 11.281),\n",
       "  14: (37.349, -4.238, 12.905),\n",
       "  15: (38.792, -5.515, 13.047),\n",
       "  16: (38.545, -6.192, 13.865),\n",
       "  17: (39.396, -4.815, 13.624),\n",
       "  18: (39.789, -6.215, 12.181),\n",
       "  19: (39.718, -7.135, 11.701),\n",
       "  20: (39.821, -5.678, 11.291),\n",
       "  21: (40.79, -6.158, 12.458)},\n",
       " 'HIS98A': {0: (33.207, -3.853, 11.848),\n",
       "  1: (33.746, -3.512, 11.052),\n",
       "  2: (32.625, -2.825, 12.702),\n",
       "  3: (32.897, -3.073, 13.728),\n",
       "  4: (31.112, -2.841, 12.764),\n",
       "  5: (30.559, -2.429, 13.774),\n",
       "  6: (33.092, -1.443, 12.241),\n",
       "  7: (32.808, -0.707, 12.993),\n",
       "  8: (32.608, -1.337, 11.27),\n",
       "  9: (34.56, -1.366, 11.955),\n",
       "  10: (35.53, -1.983, 12.715),\n",
       "  11: (35.221, -0.759, 10.937),\n",
       "  12: (34.915, 0.232, 10.636),\n",
       "  13: (36.727, -1.749, 12.153),\n",
       "  14: (37.728, -1.893, 12.531),\n",
       "  15: (36.598, -1.009, 11.069),\n",
       "  16: (37.344, -0.685, 10.453)},\n",
       " 'LYS99A': {0: (30.451, -3.266, 11.677),\n",
       "  1: (30.849, -3.902, 10.986),\n",
       "  2: (28.986, -3.334, 11.594),\n",
       "  3: (28.596, -3.682, 10.638),\n",
       "  4: (28.36, -1.947, 11.737),\n",
       "  5: (27.644, -1.664, 12.698),\n",
       "  6: (28.436, -4.287, 12.66),\n",
       "  7: (27.475, -3.936, 13.036),\n",
       "  8: (29.06, -4.557, 13.512),\n",
       "  9: (28.108, -5.664, 12.116),\n",
       "  10: (28.226, -5.96, 11.074),\n",
       "  11: (27.038, -5.838, 12.232),\n",
       "  12: (28.691, -6.787, 12.951),\n",
       "  13: (28.32, -6.814, 13.976),\n",
       "  14: (28.185, -7.714, 12.682),\n",
       "  15: (30.187, -6.846, 12.792),\n",
       "  16: (30.457, -6.694, 11.747),\n",
       "  17: (30.619, -6.256, 13.6),\n",
       "  18: (30.74, -8.154, 13.225),\n",
       "  19: (30.089, -8.965, 13.209),\n",
       "  20: (31.51, -8.533, 12.638),\n",
       "  21: (31.135, -8.346, 14.168)},\n",
       " 'LYS103A': {0: (22.668, 4.437, 8.202),\n",
       "  1: (22.466, 3.505, 8.564),\n",
       "  2: (22.106, 5.526, 8.997),\n",
       "  3: (21.452, 6.025, 8.282),\n",
       "  4: (23.246, 6.296, 9.687),\n",
       "  5: (23.219, 7.526, 9.754),\n",
       "  6: (21.143, 4.923, 10.033),\n",
       "  7: (20.264, 4.459, 9.586),\n",
       "  8: (21.508, 4.141, 10.699),\n",
       "  9: (20.505, 5.877, 11.022),\n",
       "  10: (19.972, 5.19, 11.679),\n",
       "  11: (21.216, 6.423, 11.642),\n",
       "  12: (19.484, 6.815, 10.382),\n",
       "  13: (20.011, 7.697, 10.019),\n",
       "  14: (18.872, 6.284, 9.653),\n",
       "  15: (18.416, 7.235, 11.402),\n",
       "  16: (17.702, 6.421, 11.53),\n",
       "  17: (19.015, 7.8, 12.116),\n",
       "  18: (17.593, 8.416, 10.981),\n",
       "  19: (17.396, 8.979, 11.833),\n",
       "  20: (16.69, 8.423, 10.465),\n",
       "  21: (18.034, 9.224, 10.497)},\n",
       " 'TYR104A': {0: (24.254, 5.571, 10.177),\n",
       "  1: (24.019, 4.631, 10.497),\n",
       "  2: (25.401, 6.219, 10.853),\n",
       "  3: (25.05, 6.972, 11.559),\n",
       "  4: (26.293, 6.997, 9.89),\n",
       "  5: (26.923, 7.976, 10.286),\n",
       "  6: (26.255, 5.214, 11.648),\n",
       "  7: (26.493, 4.417, 10.943),\n",
       "  8: (27.057, 5.804, 12.093),\n",
       "  9: (25.62, 4.683, 12.914),\n",
       "  10: (24.273, 4.928, 13.204),\n",
       "  11: (23.473, 5.584, 12.893),\n",
       "  12: (26.353, 3.9, 13.798),\n",
       "  13: (27.143, 3.251, 13.45),\n",
       "  14: (23.667, 4.388, 14.346),\n",
       "  15: (22.684, 3.942, 14.307),\n",
       "  16: (25.773, 3.36, 14.947),\n",
       "  17: (26.129, 2.725, 15.745),\n",
       "  18: (24.428, 3.6, 15.211),\n",
       "  19: (23.816, 3.023, 16.307),\n",
       "  20: (23.516, 3.675, 16.974)},\n",
       " 'GLU106A': {0: (25.069, 8.495, 7.431),\n",
       "  1: (24.484, 7.679, 7.61),\n",
       "  2: (24.293, 9.724, 7.218),\n",
       "  3: (24.53, 10.35, 6.357),\n",
       "  4: (24.609, 10.73, 8.336),\n",
       "  5: (24.804, 11.921, 8.063),\n",
       "  6: (22.793, 9.428, 7.16),\n",
       "  7: (22.349, 10.42, 7.242),\n",
       "  8: (22.196, 9.047, 7.989),\n",
       "  9: (22.4, 8.776, 5.855),\n",
       "  10: (22.89, 7.813, 5.708),\n",
       "  11: (22.003, 9.633, 5.312),\n",
       "  12: (20.989, 8.253, 5.864),\n",
       "  13: (20.304, 8.4, 6.893),\n",
       "  14: (20.581, 7.666, 4.844)},\n",
       " 'GLU110A': {0: (26.378, 14.581, 7.999),\n",
       "  1: (25.641, 13.876, 8.007),\n",
       "  2: (25.755, 15.751, 8.618),\n",
       "  3: (25.362, 16.399, 7.834),\n",
       "  4: (26.8, 16.498, 9.459),\n",
       "  5: (26.865, 17.73, 9.429),\n",
       "  6: (24.599, 15.276, 9.502),\n",
       "  7: (24.754, 14.514, 10.266),\n",
       "  8: (23.811, 15.027, 8.791),\n",
       "  9: (23.928, 16.333, 10.367),\n",
       "  10: (24.292, 17.194, 10.928),\n",
       "  11: (23.306, 16.903, 9.677),\n",
       "  12: (22.742, 15.772, 11.173),\n",
       "  13: (22.573, 14.529, 11.248),\n",
       "  14: (21.974, 16.581, 11.733)},\n",
       " 'HIS114A': {0: (28.267, 20.318, 8.923),\n",
       "  1: (28.021, 19.371, 9.21),\n",
       "  2: (28.149, 21.286, 10.016),\n",
       "  3: (27.384, 22.008, 9.728),\n",
       "  4: (29.494, 21.952, 10.325),\n",
       "  5: (29.568, 23.171, 10.551),\n",
       "  6: (27.66, 20.591, 11.288),\n",
       "  7: (26.601, 20.397, 11.12),\n",
       "  8: (28.184, 19.768, 11.775),\n",
       "  9: (27.616, 21.495, 12.483),\n",
       "  10: (26.492, 22.177, 12.894),\n",
       "  11: (28.601, 21.858, 13.344),\n",
       "  12: (29.536, 21.518, 13.765),\n",
       "  13: (26.819, 22.92, 13.959),\n",
       "  14: (26.25, 23.725, 14.399),\n",
       "  15: (28.092, 22.759, 14.27),\n",
       "  16: (28.598, 23.205, 15.035)},\n",
       " 'HIS117A': {0: (30.614, 24.196, 7.466),\n",
       "  1: (30.116, 23.373, 7.805),\n",
       "  2: (29.777, 25.385, 7.491),\n",
       "  3: (29.804, 25.97, 6.572),\n",
       "  4: (30.287, 26.368, 8.537),\n",
       "  5: (30.346, 27.576, 8.277),\n",
       "  6: (28.31, 25.006, 7.768),\n",
       "  7: (27.905, 24.622, 8.704),\n",
       "  8: (27.849, 24.342, 7.036),\n",
       "  9: (27.375, 26.182, 7.824),\n",
       "  10: (26.961, 26.899, 6.721),\n",
       "  11: (26.778, 26.775, 8.89),\n",
       "  12: (26.183, 26.738, 9.791),\n",
       "  13: (26.142, 27.883, 7.144),\n",
       "  14: (25.514, 28.276, 6.358),\n",
       "  15: (25.996, 27.854, 8.454),\n",
       "  16: (25.434, 28.483, 9.028)},\n",
       " 'HIS120A': {0: (34.381, 27.377, 7.453),\n",
       "  1: (33.385, 27.256, 7.638),\n",
       "  2: (34.666, 27.902, 6.117),\n",
       "  3: (35.411, 28.696, 6.167),\n",
       "  4: (33.453, 28.552, 5.445),\n",
       "  5: (33.154, 28.277, 4.271),\n",
       "  6: (35.184, 26.753, 5.242),\n",
       "  7: (35.609, 27.043, 4.281),\n",
       "  8: (34.388, 26.015, 5.143),\n",
       "  9: (36.368, 26.047, 5.82),\n",
       "  10: (37.643, 26.565, 5.824),\n",
       "  11: (38.271, 27.368, 5.79),\n",
       "  12: (36.463, 24.835, 6.422),\n",
       "  13: (35.92, 24.026, 6.887),\n",
       "  14: (38.463, 25.676, 6.406),\n",
       "  15: (39.492, 25.767, 6.722),\n",
       "  16: (37.806, 24.603, 6.794)},\n",
       " 'ASP127A': {0: (25.806, 26.82, -2.947),\n",
       "  1: (25.436, 27.719, -3.256),\n",
       "  2: (26.24, 25.977, -4.056),\n",
       "  3: (25.363, 25.432, -4.405),\n",
       "  4: (27.308, 24.991, -3.629),\n",
       "  5: (27.255, 23.821, -3.994),\n",
       "  6: (26.739, 26.823, -5.229),\n",
       "  7: (26.937, 26.165, -6.075),\n",
       "  8: (27.482, 27.62, -5.202),\n",
       "  9: (25.615, 27.66, -5.877),\n",
       "  10: (24.437, 27.269, -5.776),\n",
       "  11: (25.913, 28.716, -6.47)},\n",
       " 'LYS134A': {0: (26.105, 16.285, -2.431),\n",
       "  1: (26.125, 17.291, -2.599),\n",
       "  2: (26.004, 15.383, -3.57),\n",
       "  3: (25.015, 14.965, -3.382),\n",
       "  4: (27.177, 14.387, -3.574),\n",
       "  5: (26.988, 13.188, -3.878),\n",
       "  6: (25.993, 16.212, -4.851),\n",
       "  7: (25.194, 16.934, -4.684),\n",
       "  8: (26.914, 16.74, -5.098),\n",
       "  9: (25.458, 15.508, -6.063),\n",
       "  10: (26.052, 14.603, -6.187),\n",
       "  11: (24.429, 15.233, -5.83),\n",
       "  12: (25.222, 16.474, -7.205),\n",
       "  13: (24.302, 17.033, -7.036),\n",
       "  14: (25.919, 17.309, -7.281),\n",
       "  15: (24.944, 15.701, -8.483),\n",
       "  16: (25.304, 14.736, -8.841),\n",
       "  17: (23.911, 15.406, -8.299),\n",
       "  18: (24.89, 16.549, -9.709),\n",
       "  19: (24.871, 17.587, -9.641),\n",
       "  20: (24.456, 16.129, -10.556),\n",
       "  21: (25.849, 16.511, -10.11)},\n",
       " 'GLU137A': {0: (26.519, 11.932, -0.792),\n",
       "  1: (26.647, 12.897, -1.097),\n",
       "  2: (25.395, 11.051, -1.13),\n",
       "  3: (25.059, 10.601, -0.196),\n",
       "  4: (25.903, 9.953, -2.065),\n",
       "  5: (25.492, 8.798, -1.947),\n",
       "  6: (24.258, 11.81, -1.805),\n",
       "  7: (24.425, 12.598, -2.539),\n",
       "  8: (23.704, 11.063, -2.374),\n",
       "  9: (23.477, 12.651, -0.821),\n",
       "  10: (22.921, 11.969, -0.177),\n",
       "  11: (23.895, 13.463, -0.226),\n",
       "  12: (22.23, 13.26, -1.419),\n",
       "  13: (22.049, 13.183, -2.652),\n",
       "  14: (21.432, 13.82, -0.644)},\n",
       " 'LYS141A': {0: (25.749, 6.002, -1.459),\n",
       "  1: (26.177, 6.852, -1.825),\n",
       "  2: (25.059, 5.141, -2.421),\n",
       "  3: (24.146, 4.777, -1.95),\n",
       "  4: (25.968, 3.989, -2.841),\n",
       "  5: (25.549, 2.837, -2.878),\n",
       "  6: (24.664, 5.925, -3.668),\n",
       "  7: (25.416, 6.624, -4.035),\n",
       "  8: (23.884, 6.595, -3.307),\n",
       "  9: (23.881, 5.086, -4.684),\n",
       "  10: (24.324, 4.129, -4.96),\n",
       "  11: (22.869, 4.889, -4.329),\n",
       "  12: (23.514, 5.86, -5.945),\n",
       "  13: (22.942, 6.753, -5.693),\n",
       "  14: (22.67, 5.411, -6.468),\n",
       "  15: (24.571, 5.71, -7.033),\n",
       "  16: (23.951, 5.962, -7.893),\n",
       "  17: (24.794, 4.688, -7.339),\n",
       "  18: (25.879, 6.301, -6.667),\n",
       "  19: (25.649, 7.294, -6.459),\n",
       "  20: (26.17, 5.667, -5.895),\n",
       "  21: (26.479, 6.37, -7.514)},\n",
       " 'ASP142A': {0: (27.222, 4.305, -3.138),\n",
       "  1: (27.72, 5.159, -2.887),\n",
       "  2: (28.174, 3.283, -3.553),\n",
       "  3: (27.621, 2.737, -4.317),\n",
       "  4: (28.555, 2.36, -2.417),\n",
       "  5: (28.732, 1.158, -2.629),\n",
       "  6: (29.398, 3.911, -4.232),\n",
       "  7: (29.869, 4.799, -3.81),\n",
       "  8: (30.175, 3.236, -4.592),\n",
       "  9: (29.072, 4.439, -5.625),\n",
       "  10: (27.92, 4.245, -6.076),\n",
       "  11: (29.941, 5.06, -6.264)},\n",
       " 'LYS146A': {0: (27.843, -1.8, -2.101),\n",
       "  1: (27.987, -0.883, -1.677),\n",
       "  2: (28.875, -2.772, -1.753),\n",
       "  3: (29.003, -3.489, -2.564),\n",
       "  4: (28.438, -3.571, -0.523),\n",
       "  5: (28.688, -4.766, -0.44),\n",
       "  6: (30.232, -2.077, -1.521),\n",
       "  7: (30.358, -1.251, -0.821),\n",
       "  8: (30.799, -2.957, -1.216),\n",
       "  9: (30.917, -1.63, -2.817),\n",
       "  10: (31.738, -0.958, -2.567),\n",
       "  11: (31.516, -2.454, -3.204),\n",
       "  12: (29.874, -1.207, -3.87),\n",
       "  13: (30.01, -2.054, -4.543),\n",
       "  14: (28.787, -1.142, -3.915),\n",
       "  15: (30.264, 0.03, -4.647),\n",
       "  16: (30.662, 0.748, -3.93),\n",
       "  17: (30.978, -0.385, -5.358),\n",
       "  18: (29.186, 0.483, -5.535),\n",
       "  19: (29.096, 1.516, -5.456),\n",
       "  20: (28.267, 0.398, -5.055),\n",
       "  21: (28.795, 0.463, -6.498)},\n",
       " 'TYR147A': {0: (27.796, -2.904, 0.432),\n",
       "  1: (28.519, -2.255, 0.741),\n",
       "  2: (27.297, -3.57, 1.638),\n",
       "  3: (28.053, -4.051, 2.259),\n",
       "  4: (26.419, -4.765, 1.212),\n",
       "  5: (26.554, -5.896, 1.714),\n",
       "  6: (26.432, -2.59, 2.446),\n",
       "  7: (25.87, -3.159, 3.187),\n",
       "  8: (25.983, -1.768, 1.888),\n",
       "  9: (27.151, -1.764, 3.511),\n",
       "  10: (28.551, -1.68, 3.572),\n",
       "  11: (29.38, -2.055, 2.991),\n",
       "  12: (26.411, -1.131, 4.518),\n",
       "  13: (25.334, -1.085, 4.446),\n",
       "  14: (29.184, -1.005, 4.618),\n",
       "  15: (30.232, -0.783, 4.758),\n",
       "  16: (27.038, -0.449, 5.561),\n",
       "  17: (26.565, 0.309, 6.167),\n",
       "  18: (28.425, -0.398, 5.613),\n",
       "  19: (29.012, 0.223, 6.703),\n",
       "  20: (28.567, 0.046, 7.558)},\n",
       " 'LYS148A': {0: (25.539, -4.494, 0.258),\n",
       "  1: (25.611, -3.626, -0.272),\n",
       "  2: (24.611, -5.475, -0.275),\n",
       "  3: (24.164, -6.048, 0.538),\n",
       "  4: (25.362, -6.665, -0.842),\n",
       "  5: (25.062, -7.82, -0.5),\n",
       "  6: (23.744, -4.818, -1.355),\n",
       "  7: (24.271, -4.849, -2.308),\n",
       "  8: (23.453, -3.768, -1.311),\n",
       "  9: (22.567, -5.649, -1.8),\n",
       "  10: (22.651, -6.426, -2.56),\n",
       "  11: (22.324, -6.285, -0.949),\n",
       "  12: (21.398, -4.763, -2.213),\n",
       "  13: (20.923, -4.017, -1.577),\n",
       "  14: (21.647, -4.384, -3.204),\n",
       "  15: (20.213, -5.613, -2.627),\n",
       "  16: (20.381, -6.104, -3.585),\n",
       "  17: (19.311, -5.064, -2.9),\n",
       "  18: (19.945, -6.686, -1.621),\n",
       "  19: (20.621, -7.475, -1.582),\n",
       "  20: (20.197, -6.238, -0.717),\n",
       "  21: (18.997, -6.997, -1.327)},\n",
       " 'GLU149A': {0: (26.363, -6.387, -1.672),\n",
       "  1: (26.607, -5.519, -2.149),\n",
       "  2: (27.164, -7.451, -2.276),\n",
       "  3: (26.487, -8.161, -2.751),\n",
       "  4: (27.898, -8.287, -1.234),\n",
       "  5: (28.111, -9.474, -1.432),\n",
       "  6: (28.189, -6.87, -3.269),\n",
       "  7: (28.87, -6.183, -2.766),\n",
       "  8: (28.788, -7.713, -3.614),\n",
       "  9: (27.558, -6.334, -4.535),\n",
       "  10: (27.725, -7.293, -5.025),\n",
       "  11: (26.645, -6.097, -5.081),\n",
       "  12: (28.509, -5.486, -5.345),\n",
       "  13: (29.692, -5.841, -5.448),\n",
       "  14: (28.073, -4.448, -5.874)},\n",
       " 'TYR152A': {0: (25.593, -7.608, 3.922),\n",
       "  1: (26.411, -7.344, 3.372),\n",
       "  2: (25.182, -6.853, 5.091),\n",
       "  3: (24.607, -7.502, 5.752),\n",
       "  4: (24.062, -5.843, 4.802),\n",
       "  5: (23.019, -5.856, 5.464),\n",
       "  6: (26.4, -6.159, 5.707),\n",
       "  7: (27.214, -5.87, 5.042),\n",
       "  8: (26.866, -7.04, 6.148),\n",
       "  9: (26.097, -5.204, 6.852),\n",
       "  10: (25.879, -5.67, 8.148),\n",
       "  11: (25.786, -6.692, 8.485),\n",
       "  12: (26.091, -3.83, 6.647),\n",
       "  13: (26.669, -3.38, 5.853),\n",
       "  14: (25.67, -4.784, 9.208),\n",
       "  15: (24.975, -5.146, 9.952),\n",
       "  16: (25.882, -2.942, 7.696),\n",
       "  17: (26.379, -1.983, 7.713),\n",
       "  18: (25.675, -3.425, 8.971),\n",
       "  19: (25.502, -2.536, 9.999),\n",
       "  20: (25.344, -1.619, 9.692)}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"109m\"][\"pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'GLU5A'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-22087c23eafe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GLU5A\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'GLU5A'"
     ]
    }
   ],
   "source": [
    "T[\"GLU5A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
