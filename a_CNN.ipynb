{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here this works!!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "num_atoms=5\n",
    "embedding_size=100#pretending max Z is sulfur\n",
    "embedding_dim=100 #of filters/channels #rn its # of energy values\n",
    "#embedding = nn.Embedding(embedding_size, embedding_dim)\n",
    "\n",
    "output_channels = 20 #number of rows\n",
    "kernel_size = 2\n",
    "\n",
    "\n",
    "numpy_array = np.array([[ -4575.5059, -11106.7861],\n",
    "[ -2831.1064, -22243.9648],\n",
    "[ -3284.0540, -14817.7686],\n",
    "[ -7160.0083, -44779.5859],\n",
    "[ -7257.9487, -29798.2441]])\n",
    "E = torch.from_numpy(numpy_array).float()\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_channels, kernel_size, num_atoms):\n",
    "        super(net, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.E = nn.Embedding(embedding_size, embedding_dim)\n",
    "        self.conv1d = nn.Conv1d(num_atoms, num_atoms, kernel_size)\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.modelE = nn.Sequential(nn.Conv1d(num_atoms, num_atoms, kernel_size=3, stride=1, padding=1), #can replace num atoms 2 3 and 4 w output_channels to make less output\n",
    "        nn.Conv1d(num_atoms, num_atoms, kernel_size=kernel_size, stride=1, padding=1),\n",
    "        nn.MaxPool1d(2))\n",
    "\n",
    "        \n",
    "    def forward(self, z, S, E):\n",
    "\n",
    "\n",
    "    # Embedding layer\n",
    "        z_embedded = self.E(z)\n",
    "        #print(z_embedded.shape)\n",
    "        #B_embedded = self.E(B)\n",
    "        S_embedded = self.E(S)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        #print(self.conv1d)\n",
    "        z_conv = self.conv1d(z_embedded) \n",
    "        #print(z_conv)\n",
    "        #B_conv = self.conv1d(B_embedded)\n",
    "        S_conv = self.conv1d(S_embedded)\n",
    "\n",
    "        E_out = self.modelE(E)\n",
    "\n",
    "        #print(z_conv, E_conv)\n",
    "        # Concatenate along the channel dimension\n",
    "        combined = torch.cat((z_conv, S_conv, E_out), dim=1)\n",
    "\n",
    "        nn.Conv2d(num_atoms, output_channels, kernel_size=kernel_size, stride=1, padding=1)(combined.unsqueeze(2))\n",
    "        \n",
    "        # Global average pooling\n",
    "        #pooled = torch.mean(combined, dim=-1)  # Pool across the spatial dimension\n",
    "        # Linear layer\n",
    "        #output = self.linear(pooled.view(-1, 1))  # Reshape the pooled tensor to match weight matrix shape\n",
    "        return nn.Conv1d(num_atoms, output_channels, kernel_size=len(combined), stride=1)(combined)\n",
    "\n",
    "\n",
    "\n",
    "model = net(embedding_dim, output_channels, kernel_size, num_atoms)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "#summary(model, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessihoernschemeyer/miniconda3/envs/cfcnn/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.0012696488993242383, Val Loss: 0.015637433156371117\n",
      "Epoch 2/200, Loss: 0.05303289368748665, Val Loss: 0.2574244439601898\n",
      "Epoch 3/200, Loss: 0.013296712189912796, Val Loss: 0.08890756964683533\n",
      "Epoch 4/200, Loss: 0.08092978596687317, Val Loss: 0.07945426553487778\n",
      "Epoch 5/200, Loss: 0.012741118669509888, Val Loss: 0.0261014886200428\n",
      "Epoch 6/200, Loss: 0.028023848310112953, Val Loss: 0.0008075364166870713\n",
      "Epoch 7/200, Loss: 0.03975783661007881, Val Loss: 0.016571804881095886\n",
      "Epoch 8/200, Loss: 0.17302164435386658, Val Loss: 0.008718704804778099\n",
      "Epoch 9/200, Loss: 0.06454870849847794, Val Loss: 0.0010721286525949836\n",
      "Epoch 10/200, Loss: 0.006824870593845844, Val Loss: 0.02632097154855728\n",
      "Epoch 11/200, Loss: 0.05165804922580719, Val Loss: 0.09243788570165634\n",
      "Epoch 12/200, Loss: 0.0017382100922986865, Val Loss: 0.0033044840674847364\n",
      "Epoch 13/200, Loss: 0.009691105224192142, Val Loss: 0.21587711572647095\n",
      "Epoch 14/200, Loss: 0.021638209000229836, Val Loss: 0.009881658479571342\n",
      "Epoch 15/200, Loss: 0.07525918632745743, Val Loss: 0.022052641957998276\n",
      "Epoch 16/200, Loss: 0.01107375044375658, Val Loss: 0.24252067506313324\n",
      "Epoch 17/200, Loss: 0.0052954968996346, Val Loss: 0.029662158340215683\n",
      "Epoch 18/200, Loss: 0.08449586480855942, Val Loss: 0.18409809470176697\n",
      "Epoch 19/200, Loss: 0.0012029645731672645, Val Loss: 0.21309107542037964\n",
      "Epoch 20/200, Loss: 0.07791902124881744, Val Loss: 0.0027118660509586334\n",
      "Epoch 21/200, Loss: 0.24553462862968445, Val Loss: 0.06636188179254532\n",
      "Epoch 22/200, Loss: 0.14591166377067566, Val Loss: 0.021255187690258026\n",
      "Epoch 23/200, Loss: 0.001037608366459608, Val Loss: 0.00018209224799647927\n",
      "Epoch 24/200, Loss: 0.0006123564671725035, Val Loss: 0.030134720727801323\n",
      "Epoch 25/200, Loss: 0.0007420391775667667, Val Loss: 0.0058870501816272736\n",
      "Epoch 26/200, Loss: 0.003502811072394252, Val Loss: 0.13758039474487305\n",
      "Epoch 27/200, Loss: 6.575003226316767e-06, Val Loss: 0.062443602830171585\n",
      "Epoch 28/200, Loss: 0.15612784028053284, Val Loss: 0.19371089339256287\n",
      "Epoch 29/200, Loss: 0.4454665184020996, Val Loss: 0.013447138480842113\n",
      "Epoch 30/200, Loss: 0.04899044707417488, Val Loss: 0.027095401659607887\n",
      "Epoch 31/200, Loss: 0.0013985434779897332, Val Loss: 0.05773964151740074\n",
      "Epoch 32/200, Loss: 0.06563518196344376, Val Loss: 0.05341034755110741\n",
      "Epoch 33/200, Loss: 0.08723858743906021, Val Loss: 0.00038790941471233964\n",
      "Epoch 34/200, Loss: 0.0004341154417488724, Val Loss: 0.0058892411179840565\n",
      "Epoch 35/200, Loss: 0.21827028691768646, Val Loss: 0.00047226634342223406\n",
      "Epoch 36/200, Loss: 0.010731996037065983, Val Loss: 0.1746203601360321\n",
      "Epoch 37/200, Loss: 0.013316065073013306, Val Loss: 0.003603733144700527\n",
      "Epoch 38/200, Loss: 0.056706495583057404, Val Loss: 0.2272089123725891\n",
      "Epoch 39/200, Loss: 0.025046510621905327, Val Loss: 0.018903518095612526\n",
      "Epoch 40/200, Loss: 0.03832946717739105, Val Loss: 0.2515881657600403\n",
      "Epoch 41/200, Loss: 0.10990356653928757, Val Loss: 0.07743887603282928\n",
      "Epoch 42/200, Loss: 0.04201941564679146, Val Loss: 0.005868816748261452\n",
      "Epoch 43/200, Loss: 0.16973648965358734, Val Loss: 0.33653703331947327\n",
      "Epoch 44/200, Loss: 0.1680024266242981, Val Loss: 0.015912828966975212\n",
      "Epoch 45/200, Loss: 0.002095060423016548, Val Loss: 0.056438229978084564\n",
      "Epoch 46/200, Loss: 2.33272003242746e-05, Val Loss: 0.04440019279718399\n",
      "Epoch 47/200, Loss: 0.09174925088882446, Val Loss: 0.03817976266145706\n",
      "Epoch 48/200, Loss: 0.0006230910657905042, Val Loss: 0.007839171215891838\n",
      "Epoch 49/200, Loss: 0.018740534782409668, Val Loss: 0.0018905092729255557\n",
      "Epoch 50/200, Loss: 0.01713341660797596, Val Loss: 0.014280905947089195\n",
      "Epoch 51/200, Loss: 0.0012479913420975208, Val Loss: 0.01688038557767868\n",
      "Epoch 52/200, Loss: 0.019702205434441566, Val Loss: 0.08758445829153061\n",
      "Epoch 53/200, Loss: 0.012601745314896107, Val Loss: 0.020567184314131737\n",
      "Epoch 54/200, Loss: 0.01166450884193182, Val Loss: 4.6430873226199765e-06\n",
      "Epoch 55/200, Loss: 0.0021083508618175983, Val Loss: 0.020838554948568344\n",
      "Epoch 56/200, Loss: 0.28238123655319214, Val Loss: 0.02703932486474514\n",
      "Epoch 57/200, Loss: 6.612644938286394e-05, Val Loss: 0.0004187626182101667\n",
      "Epoch 58/200, Loss: 0.05305355042219162, Val Loss: 0.008789819665253162\n",
      "Epoch 59/200, Loss: 2.082632136080065e-06, Val Loss: 0.0004365364438854158\n",
      "Epoch 60/200, Loss: 0.004484319128096104, Val Loss: 0.0898289605975151\n",
      "Epoch 61/200, Loss: 0.168040931224823, Val Loss: 4.6626199036836624e-05\n",
      "Epoch 62/200, Loss: 0.06345885992050171, Val Loss: 0.014719000086188316\n",
      "Epoch 63/200, Loss: 0.03217974677681923, Val Loss: 0.0032921084202826023\n",
      "Epoch 64/200, Loss: 0.019745931029319763, Val Loss: 3.64398438250646e-05\n",
      "Epoch 65/200, Loss: 0.07034434378147125, Val Loss: 0.0010755729163065553\n",
      "Epoch 66/200, Loss: 0.011164252646267414, Val Loss: 0.12126857787370682\n",
      "Epoch 67/200, Loss: 0.004942012019455433, Val Loss: 0.33938419818878174\n",
      "Epoch 68/200, Loss: 0.08383993059396744, Val Loss: 0.19324249029159546\n",
      "Epoch 69/200, Loss: 0.0008527193567715585, Val Loss: 0.056725893169641495\n",
      "Epoch 70/200, Loss: 0.010618253611028194, Val Loss: 0.06332945078611374\n",
      "Epoch 71/200, Loss: 0.0014103876892477274, Val Loss: 0.009468174539506435\n",
      "Epoch 72/200, Loss: 0.04567083343863487, Val Loss: 0.04336774721741676\n",
      "Epoch 73/200, Loss: 0.1326233595609665, Val Loss: 0.010417790152132511\n",
      "Epoch 74/200, Loss: 0.021698493510484695, Val Loss: 0.043726589530706406\n",
      "Epoch 75/200, Loss: 0.010370652191340923, Val Loss: 0.02258165553212166\n",
      "Epoch 76/200, Loss: 0.06028180196881294, Val Loss: 0.407955139875412\n",
      "Epoch 77/200, Loss: 0.1742008924484253, Val Loss: 3.382634895388037e-05\n",
      "Epoch 78/200, Loss: 0.12610268592834473, Val Loss: 0.035022083669900894\n",
      "Epoch 79/200, Loss: 0.23561038076877594, Val Loss: 0.03699255362153053\n",
      "Epoch 80/200, Loss: 0.12101535499095917, Val Loss: 0.020602980628609657\n",
      "Epoch 81/200, Loss: 0.0399148054420948, Val Loss: 0.14477777481079102\n",
      "Epoch 82/200, Loss: 0.046221259981393814, Val Loss: 0.10909685492515564\n",
      "Epoch 83/200, Loss: 2.74439462373266e-05, Val Loss: 0.0022526110988110304\n",
      "Epoch 84/200, Loss: 0.012991633266210556, Val Loss: 0.10725472122430801\n",
      "Epoch 85/200, Loss: 0.04897428676486015, Val Loss: 0.035512764006853104\n",
      "Epoch 86/200, Loss: 0.01816163770854473, Val Loss: 0.014326031319797039\n",
      "Epoch 87/200, Loss: 0.024636005982756615, Val Loss: 0.012199455872178078\n",
      "Epoch 88/200, Loss: 0.005854418035596609, Val Loss: 0.014559466391801834\n",
      "Epoch 89/200, Loss: 0.09173812717199326, Val Loss: 0.1243179589509964\n",
      "Epoch 90/200, Loss: 0.07031691819429398, Val Loss: 0.001564138219691813\n",
      "Epoch 91/200, Loss: 0.18921905755996704, Val Loss: 0.33821940422058105\n",
      "Epoch 92/200, Loss: 0.01866201125085354, Val Loss: 0.4079788029193878\n",
      "Epoch 93/200, Loss: 0.016226639971137047, Val Loss: 0.1897859275341034\n",
      "Epoch 94/200, Loss: 0.00044162411359138787, Val Loss: 0.00028855245909653604\n",
      "Epoch 95/200, Loss: 0.01609359309077263, Val Loss: 0.10429832339286804\n",
      "Epoch 96/200, Loss: 0.0006520310416817665, Val Loss: 0.03233615309000015\n",
      "Epoch 97/200, Loss: 0.01826229691505432, Val Loss: 0.001436205580830574\n",
      "Epoch 98/200, Loss: 0.1494530290365219, Val Loss: 0.019315162673592567\n",
      "Epoch 99/200, Loss: 0.02808232419192791, Val Loss: 0.0758616030216217\n",
      "Epoch 100/200, Loss: 0.005266616120934486, Val Loss: 0.010761698707938194\n",
      "Epoch 101/200, Loss: 0.00040697871008887887, Val Loss: 0.002111139940097928\n",
      "Epoch 102/200, Loss: 0.002559550339356065, Val Loss: 0.2742931544780731\n",
      "Epoch 103/200, Loss: 0.04346752539277077, Val Loss: 0.08651941269636154\n",
      "Epoch 104/200, Loss: 0.48656195402145386, Val Loss: 0.04328020662069321\n",
      "Epoch 105/200, Loss: 0.0031639202497899532, Val Loss: 0.004806974437087774\n",
      "Epoch 106/200, Loss: 0.1391899436712265, Val Loss: 0.12262012809515\n",
      "Epoch 107/200, Loss: 0.003928691614419222, Val Loss: 0.000983620178885758\n",
      "Epoch 108/200, Loss: 0.12405487895011902, Val Loss: 0.0007668191101402044\n",
      "Epoch 109/200, Loss: 0.05467311292886734, Val Loss: 0.015932666137814522\n",
      "Epoch 110/200, Loss: 0.15791642665863037, Val Loss: 0.08696163445711136\n",
      "Epoch 111/200, Loss: 0.015672940760850906, Val Loss: 0.00913972593843937\n",
      "Epoch 112/200, Loss: 0.05376750975847244, Val Loss: 0.06134495884180069\n",
      "Epoch 113/200, Loss: 0.004848212003707886, Val Loss: 0.1350909173488617\n",
      "Epoch 114/200, Loss: 0.03399566933512688, Val Loss: 8.228411752497777e-05\n",
      "Epoch 115/200, Loss: 0.03409359231591225, Val Loss: 0.1630881428718567\n",
      "Epoch 116/200, Loss: 0.02727554179728031, Val Loss: 0.04912935197353363\n",
      "Epoch 117/200, Loss: 0.04229768365621567, Val Loss: 0.3387157618999481\n",
      "Epoch 118/200, Loss: 0.11926335096359253, Val Loss: 0.03262431547045708\n",
      "Epoch 119/200, Loss: 0.009571988135576248, Val Loss: 0.08275950700044632\n",
      "Epoch 120/200, Loss: 0.07022413611412048, Val Loss: 0.0012096470454707742\n",
      "Epoch 121/200, Loss: 0.012785474769771099, Val Loss: 0.01687011867761612\n",
      "Epoch 122/200, Loss: 0.002424439415335655, Val Loss: 0.05780046433210373\n",
      "Epoch 123/200, Loss: 0.037851374596357346, Val Loss: 0.053901463747024536\n",
      "Epoch 124/200, Loss: 0.07984725385904312, Val Loss: 0.034279871731996536\n",
      "Epoch 125/200, Loss: 0.27040109038352966, Val Loss: 0.0020678516011685133\n",
      "Epoch 126/200, Loss: 0.0036439155228435993, Val Loss: 2.9144173367967596e-06\n",
      "Epoch 127/200, Loss: 0.00245116651058197, Val Loss: 0.043479036539793015\n",
      "Epoch 128/200, Loss: 0.007146831601858139, Val Loss: 0.03899513930082321\n",
      "Epoch 129/200, Loss: 0.11762327700853348, Val Loss: 0.05042501538991928\n",
      "Epoch 130/200, Loss: 0.037371087819337845, Val Loss: 5.664374475600198e-05\n",
      "Epoch 131/200, Loss: 0.003031285246834159, Val Loss: 0.06283745914697647\n",
      "Epoch 132/200, Loss: 0.000548692827578634, Val Loss: 0.01152905821800232\n",
      "Epoch 133/200, Loss: 0.010482297278940678, Val Loss: 0.02115657739341259\n",
      "Epoch 134/200, Loss: 0.07637154310941696, Val Loss: 0.0010882935021072626\n",
      "Epoch 135/200, Loss: 0.07539201527833939, Val Loss: 0.018572937697172165\n",
      "Epoch 136/200, Loss: 0.0003364626900292933, Val Loss: 0.001966371899470687\n",
      "Epoch 137/200, Loss: 0.00014508211461361498, Val Loss: 0.013780294917523861\n",
      "Epoch 138/200, Loss: 0.00017379928613081574, Val Loss: 0.007116337306797504\n",
      "Epoch 139/200, Loss: 0.0061987112276256084, Val Loss: 0.010172710753977299\n",
      "Epoch 140/200, Loss: 0.0020368769764900208, Val Loss: 0.04413067549467087\n",
      "Epoch 141/200, Loss: 0.004959683399647474, Val Loss: 0.08307228982448578\n",
      "Epoch 142/200, Loss: 0.16882851719856262, Val Loss: 0.06886374205350876\n",
      "Epoch 143/200, Loss: 0.04118603095412254, Val Loss: 0.032318681478500366\n",
      "Epoch 144/200, Loss: 0.04182858392596245, Val Loss: 0.005628236103802919\n",
      "Epoch 145/200, Loss: 3.304323035990819e-05, Val Loss: 0.012495419010519981\n",
      "Epoch 146/200, Loss: 0.016359703615307808, Val Loss: 0.02332257106900215\n",
      "Epoch 147/200, Loss: 0.004159971605986357, Val Loss: 0.0026269974187016487\n",
      "Epoch 148/200, Loss: 0.5655258297920227, Val Loss: 0.005258521996438503\n",
      "Epoch 149/200, Loss: 0.00012809100735466927, Val Loss: 0.004473944194614887\n",
      "Epoch 150/200, Loss: 0.008397330529987812, Val Loss: 0.0067011215724051\n",
      "Epoch 151/200, Loss: 0.012738797813653946, Val Loss: 0.01684705540537834\n",
      "Epoch 152/200, Loss: 0.00020682405738625675, Val Loss: 0.0005894950591027737\n",
      "Epoch 153/200, Loss: 0.0006331519107334316, Val Loss: 0.008623670786619186\n",
      "Epoch 154/200, Loss: 0.010577207431197166, Val Loss: 0.04632103070616722\n",
      "Epoch 155/200, Loss: 0.06272392719984055, Val Loss: 0.005484845023602247\n",
      "Epoch 156/200, Loss: 0.025311509147286415, Val Loss: 0.00745340995490551\n",
      "Epoch 157/200, Loss: 2.38050506595755e-06, Val Loss: 0.00394296832382679\n",
      "Epoch 158/200, Loss: 0.017949048429727554, Val Loss: 0.1326075941324234\n",
      "Epoch 159/200, Loss: 0.0009531596442684531, Val Loss: 0.08096691220998764\n",
      "Epoch 160/200, Loss: 0.017651420086622238, Val Loss: 0.05394832789897919\n",
      "Epoch 161/200, Loss: 0.002751789754256606, Val Loss: 0.018045978620648384\n",
      "Epoch 162/200, Loss: 0.011098085902631283, Val Loss: 0.04347771778702736\n",
      "Epoch 163/200, Loss: 0.1822943538427353, Val Loss: 0.054530560970306396\n",
      "Epoch 164/200, Loss: 0.00523297069594264, Val Loss: 0.006350899115204811\n",
      "Epoch 165/200, Loss: 0.004267791751772165, Val Loss: 0.00836579967290163\n",
      "Epoch 166/200, Loss: 0.01269491296261549, Val Loss: 0.01644628308713436\n",
      "Epoch 167/200, Loss: 0.1433364748954773, Val Loss: 0.0027935896068811417\n",
      "Epoch 168/200, Loss: 0.0001966061390703544, Val Loss: 0.0017486049327999353\n",
      "Epoch 169/200, Loss: 0.126839280128479, Val Loss: 0.03960902616381645\n",
      "Epoch 170/200, Loss: 0.17367368936538696, Val Loss: 0.1238466203212738\n",
      "Epoch 171/200, Loss: 0.09442592412233353, Val Loss: 0.00847409013658762\n",
      "Epoch 172/200, Loss: 0.0011393555905669928, Val Loss: 0.03915539011359215\n",
      "Epoch 173/200, Loss: 0.014132892712950706, Val Loss: 0.017008455470204353\n",
      "Epoch 174/200, Loss: 0.06299731135368347, Val Loss: 0.0022702848073095083\n",
      "Epoch 175/200, Loss: 0.04744064062833786, Val Loss: 0.0019995567854493856\n",
      "Epoch 176/200, Loss: 0.021709421649575233, Val Loss: 0.07967306673526764\n",
      "Epoch 177/200, Loss: 0.010546067729592323, Val Loss: 0.011511166580021381\n",
      "Epoch 178/200, Loss: 0.05723119527101517, Val Loss: 0.03692329302430153\n",
      "Epoch 179/200, Loss: 0.019193710759282112, Val Loss: 0.07737736403942108\n",
      "Epoch 180/200, Loss: 0.01185851264744997, Val Loss: 0.1330595463514328\n",
      "Epoch 181/200, Loss: 3.8741130993003026e-05, Val Loss: 0.021724674850702286\n",
      "Epoch 182/200, Loss: 0.09874548763036728, Val Loss: 0.05154940485954285\n",
      "Epoch 183/200, Loss: 0.004301377106457949, Val Loss: 0.009384634904563427\n",
      "Epoch 184/200, Loss: 0.004473389126360416, Val Loss: 0.007586353924125433\n",
      "Epoch 185/200, Loss: 0.1090773195028305, Val Loss: 0.23378276824951172\n",
      "Epoch 186/200, Loss: 0.019778454676270485, Val Loss: 0.005759975872933865\n",
      "Epoch 187/200, Loss: 0.07356948405504227, Val Loss: 0.06753090769052505\n",
      "Epoch 188/200, Loss: 0.09474942088127136, Val Loss: 0.0074008465744555\n",
      "Epoch 189/200, Loss: 0.014013375155627728, Val Loss: 0.216387540102005\n",
      "Epoch 190/200, Loss: 0.07946524769067764, Val Loss: 0.001452203607186675\n",
      "Epoch 191/200, Loss: 0.003295046044513583, Val Loss: 0.01672113686800003\n",
      "Epoch 192/200, Loss: 0.018871473148465157, Val Loss: 0.0228708665817976\n",
      "Epoch 193/200, Loss: 0.037372130900621414, Val Loss: 0.10290621966123581\n",
      "Epoch 194/200, Loss: 0.024710826575756073, Val Loss: 0.05338001251220703\n",
      "Epoch 195/200, Loss: 0.00751494849100709, Val Loss: 0.11209902912378311\n",
      "Epoch 196/200, Loss: 0.04327743500471115, Val Loss: 0.0001443523506168276\n",
      "Epoch 197/200, Loss: 0.018149830400943756, Val Loss: 0.03772088885307312\n",
      "Epoch 198/200, Loss: 0.0001978254149435088, Val Loss: 0.000708028266672045\n",
      "Epoch 199/200, Loss: 0.01356600597500801, Val Loss: 0.0013550932053476572\n",
      "Epoch 200/200, Loss: 0.006675119511783123, Val Loss: 0.0032977829687297344\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numpy_array = np.array([[ -4575.5059, -11106.7861],\n",
    "[ -2831.1064, -22243.9648],\n",
    "[ -3284.0540, -14817.7686],\n",
    "[ -7160.0083, -44779.5859],\n",
    "[ -7257.9487, -29798.2441]])\n",
    "E = torch.from_numpy(numpy_array).float()\n",
    "z=torch.tensor([7,6,8,6,6]) \n",
    "S=torch.tensor([97,91,93,90,73])\n",
    "S=torch.tensor([97,97,97,97,97]) #winner\n",
    "B=torch.tensor([10,2,3,5,1])\n",
    "\n",
    "# Normalize input features\n",
    "scaler_z = MinMaxScaler()\n",
    "scaler_S = MinMaxScaler()\n",
    "scaler_E = MinMaxScaler()\n",
    "\n",
    "z = torch.from_numpy(scaler_z.fit_transform(z.view(-1, 1))).float().squeeze()\n",
    "S = torch.from_numpy(scaler_S.fit_transform(S.view(-1, 1))).float().squeeze()\n",
    "E = torch.from_numpy(scaler_E.fit_transform(E)).float()\n",
    "\n",
    "# Normalize target values\n",
    "labels = torch.tensor([13.37]).float()\n",
    "scaler_labels = MinMaxScaler()\n",
    "labels = torch.from_numpy(scaler_labels.fit_transform(labels.view(-1, 1))).float().squeeze()\n",
    "\n",
    "\n",
    "#here this works!!\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "num_atoms=5\n",
    "embedding_size=100#pretending max Z is sulfur\n",
    "embedding_dim=7 #of filters/channels #rn its # of energy values\n",
    "#embedding = nn.Embedding(embedding_size, embedding_dim)\n",
    "\n",
    "output_channels = 4*num_atoms #number of rows\n",
    "kernel_size = 2\n",
    "\n",
    "\n",
    "numpy_array = np.array([[ -4575.5059, -11106.7861],\n",
    "[ -2831.1064, -22243.9648],\n",
    "[ -3284.0540, -14817.7686],\n",
    "[ -7160.0083, -44779.5859],\n",
    "[ -7257.9487, -29798.2441]])\n",
    "E = torch.from_numpy(numpy_array).float()\n",
    "z=torch.tensor([7,6,8,6,6]) \n",
    "#S=torch.tensor([97,91,93,90,73])\n",
    "S=torch.tensor([97,97,97,97,97]) #winner\n",
    "B=torch.tensor([10,2,3,5,1])\n",
    "\n",
    "\n",
    "# Normalize input features\n",
    "scaler_z = MinMaxScaler()\n",
    "scaler_S = MinMaxScaler()\n",
    "scaler_E = MinMaxScaler()\n",
    "\n",
    "z = torch.from_numpy(scaler_z.fit_transform(z.view(-1, 1))).float().squeeze().long()\n",
    "S = torch.from_numpy(scaler_S.fit_transform(S.view(-1, 1))).float().squeeze().long()\n",
    "E = torch.from_numpy(scaler_E.fit_transform(E)).float()\n",
    "\n",
    "# Normalize target values\n",
    "labels = torch.tensor([-0.1]).float()\n",
    "scaler_labels = MinMaxScaler()\n",
    "labels = torch.from_numpy(scaler_labels.fit_transform(labels.view(-1, 1))).float().squeeze()\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_channels, kernel_size, num_atoms):\n",
    "        super(net, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.E = nn.Embedding(embedding_size, embedding_dim)\n",
    "        self.conv1d = nn.Conv1d(num_atoms, num_atoms, kernel_size)\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.modelE = nn.Sequential(nn.Conv1d(num_atoms, num_atoms, kernel_size=3, stride=1, padding=1), #can replace num atoms 2 3 and 4 w output_channels to make less output\n",
    "        nn.Conv1d(num_atoms, num_atoms, kernel_size=kernel_size, stride=1, padding=1),\n",
    "        nn.MaxPool1d(2))\n",
    "\n",
    "        \n",
    "    def forward(self, z, B, S, E):\n",
    "\n",
    "\n",
    "    # Embedding layer\n",
    "        z_embedded = self.E(z)\n",
    "        #print(z_embedded)\n",
    "        B_embedded = self.E(B)\n",
    "        S_embedded = self.E(S)\n",
    "\n",
    "        # Convolutional layer\n",
    "        z_conv = self.conv1d(z_embedded) \n",
    "        #print(z_conv)\n",
    "        B_conv = self.conv1d(B_embedded)\n",
    "        S_conv = self.conv1d(S_embedded)\n",
    "\n",
    "        E_out = self.modelE(E)\n",
    "\n",
    "        #print(z_conv, E_conv)\n",
    "        # Concatenate along the channel dimension\n",
    "        combined = torch.cat((z_conv, B_conv, S_conv, E_out), dim=1)\n",
    "        x = nn.Conv1d(num_atoms, output_channels, kernel_size=10, stride=1)(combined)\n",
    "\n",
    "        y=nn.Conv1d(num_atoms, 1, kernel_size=10, stride=1)(combined)\n",
    "        #y = nn.Linear(output_channels, output_channels)\n",
    "        #nn.Conv2d(num_atoms, output_channels, kernel_size=kernel_size, stride=1, padding=1)(combined.unsqueeze(2))\n",
    "        \n",
    "        # Global average pooling\n",
    "        #pooled = torch.mean(combined, dim=-1)  # Pool across the spatial dimension\n",
    "        # Linear layer\n",
    "        #output = self.linear(pooled.view(-1, 1))  # Reshape the pooled tensor to match weight matrix shape\n",
    "        return (y)\n",
    "\n",
    "\n",
    "\n",
    "model = net(num_atoms, 3*num_atoms, 3, num_atoms)\n",
    "#output = model(z, B, S, E)\n",
    "\n",
    "#nn.Linear(20 * (maxstates // 2 // 2), 20) \n",
    "\n",
    "#E = torch.from_numpy(numpy_array).float()\n",
    "\n",
    "#print(output.unsqueeze(1))\n",
    "#print(output)\n",
    "# Labels for regression\n",
    "#labels = torch.tensor([9.003])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Combine scalar inputs\n",
    "    #scalar_inputs = torch.cat((pol, quarks, protons))\n",
    "    #inputs = torch.cat(())\n",
    "    \n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(z, B, S, E)\n",
    "    loss = criterion(outputs, labels)\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "\n",
    "# Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(z, B, S, E)\n",
    "        val_loss = criterion(val_outputs, labels)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessihoernschemeyer/miniconda3/envs/cfcnn/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 46.0576286315918/\n",
      "Epoch 2/200, Loss: 48.12401580810547/\n",
      "Epoch 3/200, Loss: 46.69378662109375/\n",
      "Epoch 4/200, Loss: 46.7383918762207/\n",
      "Epoch 5/200, Loss: 50.94949722290039/\n",
      "Epoch 6/200, Loss: 47.13343811035156/\n",
      "Epoch 7/200, Loss: 46.56906509399414/\n",
      "Epoch 8/200, Loss: 49.743431091308594/\n",
      "Epoch 9/200, Loss: 48.181480407714844/\n",
      "Epoch 10/200, Loss: 52.01771545410156/\n",
      "Epoch 11/200, Loss: 46.57760238647461/\n",
      "Epoch 12/200, Loss: 48.17796325683594/\n",
      "Epoch 13/200, Loss: 48.739994049072266/\n",
      "Epoch 14/200, Loss: 50.649288177490234/\n",
      "Epoch 15/200, Loss: 45.83479690551758/\n",
      "Epoch 16/200, Loss: 49.5280876159668/\n",
      "Epoch 17/200, Loss: 48.806270599365234/\n",
      "Epoch 18/200, Loss: 47.51215744018555/\n",
      "Epoch 19/200, Loss: 50.88379669189453/\n",
      "Epoch 20/200, Loss: 49.910667419433594/\n",
      "Epoch 21/200, Loss: 46.45388412475586/\n",
      "Epoch 22/200, Loss: 46.27385330200195/\n",
      "Epoch 23/200, Loss: 46.57467269897461/\n",
      "Epoch 24/200, Loss: 45.48044967651367/\n",
      "Epoch 25/200, Loss: 51.20581817626953/\n",
      "Epoch 26/200, Loss: 45.55106735229492/\n",
      "Epoch 27/200, Loss: 48.330055236816406/\n",
      "Epoch 28/200, Loss: 46.05422592163086/\n",
      "Epoch 29/200, Loss: 50.7138557434082/\n",
      "Epoch 30/200, Loss: 46.105751037597656/\n",
      "Epoch 31/200, Loss: 50.5245361328125/\n",
      "Epoch 32/200, Loss: 44.492958068847656/\n",
      "Epoch 33/200, Loss: 47.301673889160156/\n",
      "Epoch 34/200, Loss: 50.085880279541016/\n",
      "Epoch 35/200, Loss: 49.287845611572266/\n",
      "Epoch 36/200, Loss: 52.582496643066406/\n",
      "Epoch 37/200, Loss: 48.89754104614258/\n",
      "Epoch 38/200, Loss: 50.73116683959961/\n",
      "Epoch 39/200, Loss: 47.117000579833984/\n",
      "Epoch 40/200, Loss: 49.53772735595703/\n",
      "Epoch 41/200, Loss: 49.48416519165039/\n",
      "Epoch 42/200, Loss: 49.303680419921875/\n",
      "Epoch 43/200, Loss: 51.303829193115234/\n",
      "Epoch 44/200, Loss: 51.82157516479492/\n",
      "Epoch 45/200, Loss: 51.00629425048828/\n",
      "Epoch 46/200, Loss: 48.408287048339844/\n",
      "Epoch 47/200, Loss: 53.39055252075195/\n",
      "Epoch 48/200, Loss: 48.00990295410156/\n",
      "Epoch 49/200, Loss: 51.317779541015625/\n",
      "Epoch 50/200, Loss: 50.26045608520508/\n",
      "Epoch 51/200, Loss: 51.07065963745117/\n",
      "Epoch 52/200, Loss: 54.51215744018555/\n",
      "Epoch 53/200, Loss: 49.13871765136719/\n",
      "Epoch 54/200, Loss: 48.91115951538086/\n",
      "Epoch 55/200, Loss: 51.39679718017578/\n",
      "Epoch 56/200, Loss: 51.194026947021484/\n",
      "Epoch 57/200, Loss: 49.63887405395508/\n",
      "Epoch 58/200, Loss: 46.57270812988281/\n",
      "Epoch 59/200, Loss: 48.43596649169922/\n",
      "Epoch 60/200, Loss: 45.78572082519531/\n",
      "Epoch 61/200, Loss: 48.098323822021484/\n",
      "Epoch 62/200, Loss: 47.97783279418945/\n",
      "Epoch 63/200, Loss: 50.403167724609375/\n",
      "Epoch 64/200, Loss: 49.3929443359375/\n",
      "Epoch 65/200, Loss: 46.90153884887695/\n",
      "Epoch 66/200, Loss: 52.546390533447266/\n",
      "Epoch 67/200, Loss: 48.26542663574219/\n",
      "Epoch 68/200, Loss: 51.06409454345703/\n",
      "Epoch 69/200, Loss: 49.079566955566406/\n",
      "Epoch 70/200, Loss: 45.08087158203125/\n",
      "Epoch 71/200, Loss: 48.23112106323242/\n",
      "Epoch 72/200, Loss: 52.544857025146484/\n",
      "Epoch 73/200, Loss: 49.763851165771484/\n",
      "Epoch 74/200, Loss: 48.621826171875/\n",
      "Epoch 75/200, Loss: 54.33180618286133/\n",
      "Epoch 76/200, Loss: 48.0126838684082/\n",
      "Epoch 77/200, Loss: 46.0799560546875/\n",
      "Epoch 78/200, Loss: 48.25150680541992/\n",
      "Epoch 79/200, Loss: 43.93086242675781/\n",
      "Epoch 80/200, Loss: 46.311119079589844/\n",
      "Epoch 81/200, Loss: 53.90940475463867/\n",
      "Epoch 82/200, Loss: 48.47712326049805/\n",
      "Epoch 83/200, Loss: 49.0693244934082/\n",
      "Epoch 84/200, Loss: 51.26124954223633/\n",
      "Epoch 85/200, Loss: 48.51621627807617/\n",
      "Epoch 86/200, Loss: 48.358009338378906/\n",
      "Epoch 87/200, Loss: 50.26200866699219/\n",
      "Epoch 88/200, Loss: 49.56444549560547/\n",
      "Epoch 89/200, Loss: 48.84036636352539/\n",
      "Epoch 90/200, Loss: 49.077423095703125/\n",
      "Epoch 91/200, Loss: 46.448822021484375/\n",
      "Epoch 92/200, Loss: 49.83670425415039/\n",
      "Epoch 93/200, Loss: 53.25224685668945/\n",
      "Epoch 94/200, Loss: 49.150272369384766/\n",
      "Epoch 95/200, Loss: 50.32381820678711/\n",
      "Epoch 96/200, Loss: 50.51165771484375/\n",
      "Epoch 97/200, Loss: 46.947723388671875/\n",
      "Epoch 98/200, Loss: 48.690975189208984/\n",
      "Epoch 99/200, Loss: 49.074134826660156/\n",
      "Epoch 100/200, Loss: 50.33511734008789/\n",
      "Epoch 101/200, Loss: 47.21567916870117/\n",
      "Epoch 102/200, Loss: 51.597694396972656/\n",
      "Epoch 103/200, Loss: 51.31168746948242/\n",
      "Epoch 104/200, Loss: 45.98782730102539/\n",
      "Epoch 105/200, Loss: 48.58245849609375/\n",
      "Epoch 106/200, Loss: 49.45761489868164/\n",
      "Epoch 107/200, Loss: 53.37684631347656/\n",
      "Epoch 108/200, Loss: 51.04545211791992/\n",
      "Epoch 109/200, Loss: 49.78466796875/\n",
      "Epoch 110/200, Loss: 52.19766616821289/\n",
      "Epoch 111/200, Loss: 47.1706428527832/\n",
      "Epoch 112/200, Loss: 46.03910827636719/\n",
      "Epoch 113/200, Loss: 49.081390380859375/\n",
      "Epoch 114/200, Loss: 47.123565673828125/\n",
      "Epoch 115/200, Loss: 51.86239242553711/\n",
      "Epoch 116/200, Loss: 51.04362487792969/\n",
      "Epoch 117/200, Loss: 50.5515251159668/\n",
      "Epoch 118/200, Loss: 45.53680419921875/\n",
      "Epoch 119/200, Loss: 47.89675521850586/\n",
      "Epoch 120/200, Loss: 48.547706604003906/\n",
      "Epoch 121/200, Loss: 47.33673858642578/\n",
      "Epoch 122/200, Loss: 49.683040618896484/\n",
      "Epoch 123/200, Loss: 48.0295295715332/\n",
      "Epoch 124/200, Loss: 43.92662811279297/\n",
      "Epoch 125/200, Loss: 52.73085021972656/\n",
      "Epoch 126/200, Loss: 48.198036193847656/\n",
      "Epoch 127/200, Loss: 48.63631057739258/\n",
      "Epoch 128/200, Loss: 47.45659255981445/\n",
      "Epoch 129/200, Loss: 45.95533752441406/\n",
      "Epoch 130/200, Loss: 47.853797912597656/\n",
      "Epoch 131/200, Loss: 54.10835647583008/\n",
      "Epoch 132/200, Loss: 46.900062561035156/\n",
      "Epoch 133/200, Loss: 53.040409088134766/\n",
      "Epoch 134/200, Loss: 47.11283874511719/\n",
      "Epoch 135/200, Loss: 47.14527130126953/\n",
      "Epoch 136/200, Loss: 52.67646408081055/\n",
      "Epoch 137/200, Loss: 47.54177474975586/\n",
      "Epoch 138/200, Loss: 51.7849006652832/\n",
      "Epoch 139/200, Loss: 47.789451599121094/\n",
      "Epoch 140/200, Loss: 49.1142692565918/\n",
      "Epoch 141/200, Loss: 53.47652053833008/\n",
      "Epoch 142/200, Loss: 50.025508880615234/\n",
      "Epoch 143/200, Loss: 46.99407196044922/\n",
      "Epoch 144/200, Loss: 47.68082046508789/\n",
      "Epoch 145/200, Loss: 48.961063385009766/\n",
      "Epoch 146/200, Loss: 46.426483154296875/\n",
      "Epoch 147/200, Loss: 48.09774398803711/\n",
      "Epoch 148/200, Loss: 51.67405700683594/\n",
      "Epoch 149/200, Loss: 49.37924575805664/\n",
      "Epoch 150/200, Loss: 51.22683334350586/\n",
      "Epoch 151/200, Loss: 48.70600128173828/\n",
      "Epoch 152/200, Loss: 43.76719284057617/\n",
      "Epoch 153/200, Loss: 46.33110427856445/\n",
      "Epoch 154/200, Loss: 46.170658111572266/\n",
      "Epoch 155/200, Loss: 47.97360610961914/\n",
      "Epoch 156/200, Loss: 47.14232635498047/\n",
      "Epoch 157/200, Loss: 42.881954193115234/\n",
      "Epoch 158/200, Loss: 51.676719665527344/\n",
      "Epoch 159/200, Loss: 49.79935836791992/\n",
      "Epoch 160/200, Loss: 46.48322296142578/\n",
      "Epoch 161/200, Loss: 52.734249114990234/\n",
      "Epoch 162/200, Loss: 49.2887077331543/\n",
      "Epoch 163/200, Loss: 48.216495513916016/\n",
      "Epoch 164/200, Loss: 48.280208587646484/\n",
      "Epoch 165/200, Loss: 49.92554473876953/\n",
      "Epoch 166/200, Loss: 52.81450271606445/\n",
      "Epoch 167/200, Loss: 50.752960205078125/\n",
      "Epoch 168/200, Loss: 47.888851165771484/\n",
      "Epoch 169/200, Loss: 50.342960357666016/\n",
      "Epoch 170/200, Loss: 47.110748291015625/\n",
      "Epoch 171/200, Loss: 49.37907028198242/\n",
      "Epoch 172/200, Loss: 47.692562103271484/\n",
      "Epoch 173/200, Loss: 47.756507873535156/\n",
      "Epoch 174/200, Loss: 52.746826171875/\n",
      "Epoch 175/200, Loss: 45.561893463134766/\n",
      "Epoch 176/200, Loss: 46.79843521118164/\n",
      "Epoch 177/200, Loss: 51.06624984741211/\n",
      "Epoch 178/200, Loss: 49.82258224487305/\n",
      "Epoch 179/200, Loss: 45.315250396728516/\n",
      "Epoch 180/200, Loss: 46.50407028198242/\n",
      "Epoch 181/200, Loss: 49.988948822021484/\n",
      "Epoch 182/200, Loss: 53.29718780517578/\n",
      "Epoch 183/200, Loss: 51.82425308227539/\n",
      "Epoch 184/200, Loss: 48.559993743896484/\n",
      "Epoch 185/200, Loss: 47.02982711791992/\n",
      "Epoch 186/200, Loss: 47.33051681518555/\n",
      "Epoch 187/200, Loss: 52.29955291748047/\n",
      "Epoch 188/200, Loss: 48.34760665893555/\n",
      "Epoch 189/200, Loss: 51.48170852661133/\n",
      "Epoch 190/200, Loss: 50.14213943481445/\n",
      "Epoch 191/200, Loss: 50.86237716674805/\n",
      "Epoch 192/200, Loss: 47.301082611083984/\n",
      "Epoch 193/200, Loss: 45.43141555786133/\n",
      "Epoch 194/200, Loss: 45.52153396606445/\n",
      "Epoch 195/200, Loss: 49.2228889465332/\n",
      "Epoch 196/200, Loss: 49.14567947387695/\n",
      "Epoch 197/200, Loss: 51.57231903076172/\n",
      "Epoch 198/200, Loss: 49.599708557128906/\n",
      "Epoch 199/200, Loss: 52.68947982788086/\n",
      "Epoch 200/200, Loss: 48.33808135986328/\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "labels = torch.tensor([6.98])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "batch_size = 0\n",
    "\n",
    "#dataset = torch.utils.data.TensorDataset(z, S, B, labels)\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#len(dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Combine scalar inputs\n",
    "    #scalar_inputs = torch.cat((pol, quarks, protons))\n",
    "    #inputs = torch.cat(())\n",
    "    \n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(z, S, B, E)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}/')\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 2, 1], expected input[5, 1, 2] to have 2 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     77\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m outputs \u001b[39m=\u001b[39m model(z, S, E)\n\u001b[1;32m     80\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     82\u001b[0m \u001b[39m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cfcnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cfcnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, z, S, E)\u001b[0m\n\u001b[1;32m     45\u001b[0m S_embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mE(S)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39m# Convolutional layer\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m z_conv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1d(z_embedded)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m S_conv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1d(S_embedded)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m E_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodelE(E\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/cfcnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cfcnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cfcnn/lib/python3.9/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/cfcnn/lib/python3.9/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 2, 1], expected input[5, 1, 2] to have 2 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "num_atoms = 5\n",
    "embedding_size = 100  # Assuming max Z is sulfur\n",
    "embedding_dim = 2  # Embedding dimension\n",
    "output_channels = 1  # Number of output channels for final Conv1d\n",
    "kernel_size = 1\n",
    "\n",
    "# Example inputs\n",
    "z = torch.tensor([7, 1, 8, 6, 1])\n",
    "S = torch.tensor([97, 97, 97, 97, 97])\n",
    "\n",
    "# Example energy states (E)\n",
    "numpy_array = np.array([\n",
    "    [-4575.5059, -11106.7861],\n",
    "    [-2831.1064, -22243.9648],\n",
    "    [-3284.0540, -14817.7686],\n",
    "    [-7160.0083, -44779.5859],\n",
    "    [-7257.9487, -29798.2441]\n",
    "])\n",
    "E = torch.from_numpy(numpy_array).float()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_channels, kernel_size, num_atoms):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.E = nn.Embedding(embedding_size, embedding_dim)\n",
    "        self.conv1d = nn.Conv1d(embedding_dim, output_channels, kernel_size)\n",
    "        \n",
    "        self.modelE = nn.Sequential(\n",
    "            nn.Conv1d(num_atoms, num_atoms, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv1d(num_atoms, num_atoms, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.linear = nn.Linear(num_atoms * output_channels, 1)  # Adjusted\n",
    "\n",
    "    def forward(self, z, S, E):\n",
    "        # Embedding layer\n",
    "        z_embedded = self.E(z).unsqueeze(1)\n",
    "        S_embedded = self.E(S).unsqueeze(1)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        z_conv = self.conv1d(z_embedded).view(1, -1)\n",
    "        S_conv = self.conv1d(S_embedded).view(1, -1)\n",
    "        E_out = self.modelE(E.unsqueeze(0)).view(1, -1)\n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        combined = torch.cat((z_conv, S_conv, E_out), dim=1)\n",
    "        \n",
    "        # Linear layer\n",
    "        output = self.linear(combined)\n",
    "        return output\n",
    "\n",
    "model = Net(embedding_dim, output_channels, kernel_size, num_atoms)\n",
    "\n",
    "# Labels for regression\n",
    "labels = torch.tensor([6.98])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(z, S, E)\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}')\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
